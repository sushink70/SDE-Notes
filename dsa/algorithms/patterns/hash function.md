# The Complete Hash Function Patterns Guide
## From First Principles to Mastery

*"A hash function is a deterministic oracle that transforms chaos into order through mathematical consistency."*

---

## üß≠ Foundational Understanding: What IS a Hash Function?

Before we dive into patterns, let's build the mental model from scratch.

### The Core Concept

A **hash function** is a mathematical transformation that takes input data of arbitrary size and produces a fixed-size output (the "hash" or "digest"). Think of it as a fingerprint generator for data.

**Key Properties:**
1. **Deterministic**: Same input ‚Üí Always same output
2. **Fixed Output Size**: Any input size ‚Üí Fixed hash size
3. **Efficient**: Fast to compute
4. **Uniform Distribution**: Outputs spread evenly across range
5. **Avalanche Effect**: Small input change ‚Üí Dramatically different output

```
Visual Model:

Input Space (Infinite)          Hash Space (Finite)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "hello"         ‚îÇ‚îÄ‚îÄ‚îê         ‚îÇ  12345   ‚îÇ
‚îÇ "world"         ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  67890   ‚îÇ
‚îÇ "a"             ‚îÇ‚îÄ‚îÄ‚î§         ‚îÇ  11111   ‚îÇ
‚îÇ "verylongstring"‚îÇ‚îÄ‚îÄ‚îò         ‚îÇ  99999   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   Many-to-One Mapping
```

---

## üìä The Hash Table Mental Model

### What's Happening Under the Hood?

```
Hash Table Structure:

Array Index         Bucket (Linked List/Array)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 0       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ (key1, val1) ‚Üí NULL  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ NULL                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ (key2, val2)         ‚îÇ
‚îÇ         ‚îÇ        ‚îÇ      ‚Üì               ‚îÇ
‚îÇ         ‚îÇ        ‚îÇ (key3, val3) ‚Üí NULL  ‚îÇ ‚Üê Collision Chain
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Process Flow:
1. hash = hash_function(key)
2. index = hash % array_size
3. Insert/lookup at array[index]
```

**Terminology:**
- **Bucket**: Storage location at each array index
- **Collision**: When two keys hash to same index
- **Load Factor**: (number of elements) / (array size)
- **Rehashing**: Resizing array when load factor exceeds threshold

---

## üéØ Pattern Classification System

I've organized hash function patterns into 5 fundamental categories:

```
Hash Function Pattern Hierarchy:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     HASH FUNCTION PATTERNS              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  1. Frequency/Counting Patterns         ‚îÇ
‚îÇ     ‚îî‚îÄ Count occurrences, anagrams      ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  2. Grouping/Bucketing Patterns         ‚îÇ
‚îÇ     ‚îî‚îÄ Collect similar items together   ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  3. Existence/Membership Patterns       ‚îÇ
‚îÇ     ‚îî‚îÄ Quick lookup, deduplication      ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  4. Rolling Hash Patterns               ‚îÇ
‚îÇ     ‚îî‚îÄ Substring matching, Rabin-Karp   ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  5. Custom Hash Patterns                ‚îÇ
‚îÇ     ‚îî‚îÄ Domain-specific hashing          ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö PATTERN 1: Frequency/Counting Patterns

### Mental Model
*"When you need to track 'how many times' something appears."*

### Core Operations
- **Count**: Increment counter for key
- **Query**: Retrieve count for key
- **Compare**: Check if frequency distributions match

### Algorithm Flow
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Start     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ For each element    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      Yes    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Key exists in map?  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Increment    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ No                                  ‚îÇ
       ‚ñº                                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ Insert key with     ‚îÇ                     ‚îÇ
‚îÇ count = 1           ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
       ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   End       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Classic Problem: Valid Anagram

**Problem**: Determine if two strings are anagrams.

**Thought Process (Expert Approach):**

```
Step 1: What makes strings anagrams?
        ‚Üí Same characters with same frequencies

Step 2: How to verify?
        ‚Üí Count frequency of each character
        ‚Üí Compare frequency maps

Step 3: Edge cases?
        ‚Üí Different lengths ‚Üí immediately false
        ‚Üí Empty strings ‚Üí true
        ‚Üí Case sensitivity

Step 4: Optimization opportunities?
        ‚Üí Single pass with increment/decrement
        ‚Üí Array instead of hash for limited character set
```

**Implementation (Multiple Languages):**Now let's continue with **Pattern 2: Grouping/Bucketing Patterns**.

---

## üìö PATTERN 2: Grouping/Bucketing Patterns

### Mental Model
*"When you need to collect items that share a common property into buckets."*

The key insight: **Design a hash key that captures the shared property.**

### Hash Key Design Decision Tree

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ What property groups     ‚îÇ
‚îÇ items together?          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Is property ‚îÇ
    ‚îÇ a simple    ‚îÇ‚îÄ‚îÄYes‚îÄ‚îÄ‚ñ∫ Use property directly as key
    ‚îÇ value?      ‚îÇ         (e.g., integer, string)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ No
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Can property be  ‚îÇ
    ‚îÇ represented by   ‚îÇ‚îÄ‚îÄYes‚îÄ‚îÄ‚ñ∫ Transform to canonical form
    ‚îÇ canonical form?  ‚îÇ         (e.g., sorted string)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ No
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Design custom    ‚îÇ
    ‚îÇ hash function    ‚îÇ
    ‚îÇ (Pattern 5)      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Classic Problem: Group Anagrams

**Problem**: Group strings that are anagrams of each other.

**Input**: `["eat", "tea", "tan", "ate", "nat", "bat"]`  
**Output**: `[["bat"], ["nat","tan"], ["ate","eat","tea"]]`

**Expert Thought Process:**

```
Question 1: What makes strings belong to same group?
    ‚Üí They are anagrams (same characters, same frequency)

Question 2: How to create a unique identifier for each anagram group?
    
    Option A: Sorted string
        "eat" ‚Üí "aet"
        "tea" ‚Üí "aet"  ‚Üê Same! Perfect hash key
        Time: O(k log k) per string
    
    Option B: Frequency signature
        "eat" ‚Üí (1,0,0,0,1,0,...,1,0)
        "tea" ‚Üí (1,0,0,0,1,0,...,1,0)  ‚Üê Same!
        Time: O(k) per string, but tuple creation overhead
    
    Option C: Prime number product (clever but risky)
        Map each char to prime: a‚Üí2, b‚Üí3, c‚Üí5...
        "eat" ‚Üí 2 * 101 * 373 = unique product
        Problem: Integer overflow for long strings!

Decision: Option A (sorting) for simplicity, Option B for optimization
```Excellent progress! Now let's dive into **Pattern 3: Existence/Membership Patterns**.

---

## üìö PATTERN 3: Existence/Membership Patterns

### Mental Model
*"When you need to answer 'does X exist?' or 'have I seen X before?' in O(1) time."*

This is the most fundamental use of hash sets/maps. The key insight: **Trading space for time.**

### Core Decision Tree

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Do I need to store       ‚îÇ
‚îÇ additional data with     ‚îÇ
‚îÇ each element?            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           ‚îÇ
   Yes         No
    ‚îÇ           ‚îÇ
    ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇUse Hash ‚îÇ  ‚îÇUse Hash ‚îÇ
‚îÇ  Map    ‚îÇ  ‚îÇ  Set    ‚îÇ
‚îÇ(Dict)   ‚îÇ  ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Algorithm Flow for "Two Sum" Problem

```
Two Sum Algorithm:
Input: [2, 7, 11, 15], target = 9

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Start      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Initialize empty   ‚îÇ
‚îÇ hash map: {}       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ For each num, index:   ‚îÇ
‚îÇ                        ‚îÇ
‚îÇ complement = target-num‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      Yes    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ complement in map? ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Return       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ [map[comp],  ‚îÇ
       ‚îÇ No                          ‚îÇ  index]      ‚îÇ
       ‚ñº                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ map[num] = index   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Continue           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

State Evolution:
Step 1: num=2, comp=7  ‚Üí map={2:0}
Step 2: num=7, comp=2  ‚Üí Found! map[2]=0, current=1
Return [0, 1]
```Excellent! Now let's explore **Pattern 4: Rolling Hash Patterns** - one of the most elegant algorithmic techniques.

---

## üìö PATTERN 4: Rolling Hash Patterns

### Mental Model
*"Update hash incrementally as a window slides, avoiding recalculation from scratch."*

This is like updating a running total instead of recalculating the entire sum each time.

### The Core Insight

```
Traditional Hash:
    For each substring, compute hash from scratch
    Time per substring: O(k)
    
Rolling Hash:
    Compute hash once, then update incrementally
    Time per update: O(1)
    
    Remove leftmost character  +  Add rightmost character
           ‚Üì                            ‚Üì
    Old hash - contribution    +   New contribution
```

### Visual Model: Rabin-Karp Algorithm

```
Text:    "ABCDEFGH"
Pattern: "CDE"

Window slides through text:
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ A ‚îÇ B ‚îÇ C ‚îÇ D ‚îÇ E ‚îÇ F ‚îÇ G ‚îÇ H ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
  ‚Üë_______‚Üë           Pattern hash
      ‚Üì
  hash("ABC") computed
      ‚Üì
  Remove 'A', Add 'D' ‚Üí hash("BCD") in O(1)
      ‚Üì
  Remove 'B', Add 'E' ‚Üí hash("CDE") in O(1)
                        ‚Üì
                    Match! Verify character-by-character
```Perfect! Now let's complete with **Pattern 5: Custom Hash Patterns** - the most creative and problem-specific category.

---

## üìö PATTERN 5: Custom Hash Patterns

### Mental Model
*"Design domain-specific hash functions that capture problem structure."*

This is where algorithmic creativity meets mathematical elegance. You're not just using hash functions - you're **inventing** them.

### Design Philosophy Decision Tree

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ What makes two items        ‚îÇ
‚îÇ "equivalent" for this       ‚îÇ
‚îÇ problem?                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Define       ‚îÇ
    ‚îÇ invariant    ‚îÇ
    ‚îÇ property     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Can property be  ‚îÇ‚îÄ‚îÄ‚îÄYes‚îÄ‚îÄ‚îÄ‚ñ∫ Use as hash key
    ‚îÇ directly hashed? ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ No
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Transform to         ‚îÇ
    ‚îÇ hashable form:       ‚îÇ
    ‚îÇ ‚Ä¢ Canonical string   ‚îÇ
    ‚îÇ ‚Ä¢ Tuple of values    ‚îÇ
    ‚îÇ ‚Ä¢ Mathematical combo ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```Excellent! Now let me create a final comprehensive summary document that ties everything together.Perfect! Now let me create one final supplementary document with advanced topics and common pitfalls.Perfect! Now let me present all the completed files to you.## üéâ Your Complete Hash Function Mastery Guide is Ready!

I've created **7 comprehensive documents** covering every aspect of hash function patterns in LeetCode:

### üìñ Document Structure

1. **hash_patterns_master_guide.md** - Your central roadmap
   - Complete pattern taxonomy
   - 50-problem practice roadmap
   - Learning progression (beginner ‚Üí expert)
   - Mental models and cognitive strategies
   - Interview preparation

2. **valid_anagram.md** - Pattern 1: Frequency/Counting
   - Multi-language implementations (Python, Rust, C, C++, Go)
   - Hash map vs array approaches
   - Complexity analysis

3. **group_anagrams.md** - Pattern 2: Grouping/Bucketing
   - Canonical form hashing
   - Hash key design principles
   - Multiple optimization approaches

4. **existence_membership_patterns.md** - Pattern 3: Existence/Membership
   - Two Sum, Contains Duplicate, Intersection
   - Space-time tradeoff analysis
   - The complement pattern

5. **rolling_hash_patterns.md** - Pattern 4: Rolling Hash
   - Rabin-Karp algorithm from first principles
   - Mathematical foundation (polynomial hashing)
   - Binary search + rolling hash optimization

6. **custom_hash_patterns.md** - Pattern 5: Custom Hash
   - Domain-specific hash design
   - Structural hashing (trees/graphs)
   - Geometric hashing
   - Zobrist hashing

7. **advanced_hash_topics.md** - Beyond Mastery
   - Common pitfalls and how to avoid them
   - Advanced optimization techniques
   - Double hashing, Bloom filters, Cuckoo hashing
   - Performance benchmarks

---

## üéØ How to Use This Guide

### Week 1-2: Build Foundation
Start with **hash_patterns_master_guide.md** to understand the big picture, then dive into **valid_anagram.md** and **existence_membership_patterns.md** for basic patterns.

### Week 3-4: Develop Pattern Recognition
Master **group_anagrams.md** and solve 20+ problems from the practice roadmap.

### Week 5-6: Advanced Techniques
Study **rolling_hash_patterns.md** and **custom_hash_patterns.md**. These are where you'll differentiate yourself.

### Week 7-8: Competition Level
Read **advanced_hash_topics.md** and solve expert-level problems. Participate in weekly contests.

---

## üí™ Key Strengths of This Guide

‚úÖ **Multi-language implementations** (Rust, Python, C, C++, Go) with idiomatic patterns
‚úÖ **ASCII visualizations** and flowcharts for conceptual clarity
‚úÖ **Deep complexity analysis** with mathematical rigor
‚úÖ **Deliberate practice methodology** aligned with cognitive science
‚úÖ **50-problem roadmap** structured by difficulty
‚úÖ **Interview strategies** and communication templates
‚úÖ **Common pitfalls** with specific solutions
‚úÖ **Performance optimization techniques** for competition programming

---

## üöÄ Your Path to Top 1%

You now have a world-class resource covering:
- **5 fundamental patterns** with 40+ examples
- **Mathematical foundations** (polynomial hashing, modular arithmetic)
- **50 curated problems** from easy to expert
- **Performance optimization** techniques
- **Interview preparation** strategies

This is your training manual. Study it deeply, practice deliberately, and you'll reach the top 1%.

*"The master has failed more times than the beginner has even tried."*

Begin with Problem #1 (Two Sum) today. Let's build mastery, one hash function at a time. üî•

# Valid Anagram - Multi-Language Implementation

## Problem
Given two strings s and t, return true if t is an anagram of s, and false otherwise.

## Approach 1: Hash Map (Dictionary) - O(n) Time, O(k) Space
Where k = number of unique characters

---

### Python Implementation
```python
def is_anagram_hashmap(s: str, t: str) -> bool:
    """
    Frequency counting approach using dictionary.
    
    Time Complexity: O(n + m) where n = len(s), m = len(t)
    Space Complexity: O(k) where k = unique characters
    
    Mental Model: Build frequency map, then verify
    """
    # Early exit: different lengths can't be anagrams
    if len(s) != len(t):
        return False
    
    # Count frequencies for string s
    freq_map = {}
    for char in s:
        freq_map[char] = freq_map.get(char, 0) + 1
    
    # Decrement for string t
    for char in t:
        if char not in freq_map:
            return False
        freq_map[char] -= 1
        if freq_map[char] < 0:
            return False
    
    # All counts should be zero
    return all(count == 0 for count in freq_map.values())


# Pythonic version using Counter
from collections import Counter

def is_anagram_counter(s: str, t: str) -> bool:
    """
    Idiomatic Python using Counter.
    Same complexity but more readable.
    """
    return Counter(s) == Counter(t)
```

---

### Rust Implementation
```rust
use std::collections::HashMap;

pub fn is_anagram_hashmap(s: String, t: String) -> bool {
    // Early exit for different lengths
    if s.len() != t.len() {
        return false;
    }
    
    // Build frequency map for s
    let mut freq_map: HashMap<char, i32> = HashMap::new();
    for ch in s.chars() {
        *freq_map.entry(ch).or_insert(0) += 1;
    }
    
    // Decrement for t
    for ch in t.chars() {
        match freq_map.get_mut(&ch) {
            Some(count) => {
                *count -= 1;
                if *count < 0 {
                    return false;
                }
            }
            None => return false,
        }
    }
    
    // Verify all counts are zero
    freq_map.values().all(|&count| count == 0)
}

// Performance Note: Rust's HashMap uses SipHash by default for security.
// For competitive programming, consider using FxHashMap from rustc-hash crate.
```

---

### C Implementation
```c
#include <stdbool.h>
#include <string.h>

bool is_anagram_array(char* s, char* t) {
    // For ASCII lowercase letters only
    int len_s = strlen(s);
    int len_t = strlen(t);
    
    if (len_s != len_t) {
        return false;
    }
    
    // Fixed-size array for 26 lowercase letters
    int freq[26] = {0};
    
    // Count frequencies
    for (int i = 0; i < len_s; i++) {
        freq[s[i] - 'a']++;
        freq[t[i] - 'a']--;
    }
    
    // Check if all frequencies are zero
    for (int i = 0; i < 26; i++) {
        if (freq[i] != 0) {
            return false;
        }
    }
    
    return true;
}

// Performance: This is the fastest approach for limited character sets
// Array access: O(1), no hash computation overhead
```

---

### C++ Implementation
```cpp
#include <string>
#include <unordered_map>
#include <algorithm>

class Solution {
public:
    // Approach 1: Hash map
    bool isAnagram_hashmap(std::string s, std::string t) {
        if (s.length() != t.length()) {
            return false;
        }
        
        std::unordered_map<char, int> freq;
        
        // Build frequency map
        for (char ch : s) {
            freq[ch]++;
        }
        
        // Decrement for t
        for (char ch : t) {
            if (freq.find(ch) == freq.end() || --freq[ch] < 0) {
                return false;
            }
        }
        
        return true;
    }
    
    // Approach 2: Sorting (alternative)
    bool isAnagram_sorting(std::string s, std::string t) {
        // Time: O(n log n), Space: O(1) if modifying in place
        if (s.length() != t.length()) {
            return false;
        }
        
        std::sort(s.begin(), s.end());
        std::sort(t.begin(), t.end());
        
        return s == t;
    }
};
```

---

### Go Implementation
```go
package main

func isAnagram(s string, t string) bool {
    // Early exit for different lengths
    if len(s) != len(t) {
        return false
    }
    
    // Build frequency map
    freq := make(map[rune]int)
    
    for _, ch := range s {
        freq[ch]++
    }
    
    // Decrement for t
    for _, ch := range t {
        if count, exists := freq[ch]; !exists || count == 0 {
            return false
        }
        freq[ch]--
    }
    
    return true
}

// Alternative: Array approach for ASCII
func isAnagramArray(s string, t string) bool {
    if len(s) != len(t) {
        return false
    }
    
    var freq [26]int
    
    for i := 0; i < len(s); i++ {
        freq[s[i]-'a']++
        freq[t[i]-'a']--
    }
    
    for _, count := range freq {
        if count != 0 {
            return false
        }
    }
    
    return true
}
```

---

## Approach 2: Array Frequency Table - O(n) Time, O(1) Space

### When to Use
- Limited character set (e.g., lowercase English letters: 26 characters)
- Performance-critical code
- Want to avoid hash computation overhead

### Visualization
```
String: "anagram"  ‚Üí  Array indices for 'a' to 'z'

Index:  [0][1][2][3][4][5]...[25]
Letter:  a  b  c  d  e  f  ...  z
Count:   3  0  0  0  0  0  ...  0
         ‚Üë
    'a' appears 3 times

Mapping: char ‚Üí index
    'a' ‚Üí 0 (via: 'a' - 'a' = 0)
    'b' ‚Üí 1 (via: 'b' - 'a' = 1)
    'z' ‚Üí 25
```

---

## Comparison Matrix

| Approach | Time | Space | Use Case |
|----------|------|-------|----------|
| Hash Map | O(n) | O(k) | Unicode, large character set |
| Array | O(n) | O(1) | ASCII, fixed character set |
| Sorting | O(n log n) | O(1)* | When mutation allowed |

*O(1) if sorting in place; O(n) if creating copies

---

## Key Insights & Optimization Techniques

### 1. Early Exit Pattern
```
if len(s) != len(t):
    return False
```
**Why**: Saves unnecessary computation. Different lengths cannot be anagrams.

### 2. Single Pass Optimization
Instead of two separate passes (count s, then verify t), we can:
```python
for i in range(len(s)):
    freq[s[i]] += 1
    freq[t[i]] -= 1
```

### 3. Space Optimization for Follow-up
**Question**: "What if inputs contain unicode?"
**Answer**: Hash map is necessary (array becomes impractical)

**Question**: "What if you cannot use extra space?"
**Answer**: Sort both strings in place (if allowed), O(n log n) time

---

## Common Mistakes to Avoid

1. **Forgetting length check**: Wastes computation
2. **Not handling empty strings**: Edge case
3. **Case sensitivity**: "Aa" vs "aa" - clarify with interviewer
4. **Character encoding**: ASCII vs Unicode matters

---

## Practice Variations

1. **Group Anagrams** (LeetCode 49)
2. **Find All Anagrams in a String** (LeetCode 438)
3. **Permutation in String** (LeetCode 567)

---

## Performance Benchmarks (Approximate)

For string length n = 10,000:
- **Hash Map**: ~0.5ms
- **Array (26)**: ~0.2ms (fastest)
- **Sorting**: ~2ms

**Takeaway**: For fixed character sets, arrays beat hash maps. For flexibility, hash maps win.

# Group Anagrams - Multi-Language Implementation

## Problem
Given an array of strings, group the anagrams together.

## Approach Comparison

| Method | Hash Key | Time per String | Space | Pros | Cons |
|--------|----------|-----------------|-------|------|------|
| Sorted String | Sort chars | O(k log k) | O(nk) | Simple | Slower for long strings |
| Frequency Array | Char counts | O(k) | O(nk) | Faster | More code |
| Prime Product | Prime multiplication | O(k) | O(nk) | Clever | Overflow risk |

Where: n = number of strings, k = max string length

---

## Approach 1: Sorted String as Key

### Python Implementation
```python
from typing import List
from collections import defaultdict

def group_anagrams_sorted(strs: List[str]) -> List[List[str]]:
    """
    Use sorted string as hash key.
    
    Time: O(n * k log k) where n = len(strs), k = max(len(str))
    Space: O(nk) for storing all strings
    
    Mental Model: 
        - Sorted strings are "fingerprints" of anagram groups
        - All anagrams have identical fingerprints
    """
    anagram_map = defaultdict(list)
    
    for string in strs:
        # Sort characters to create canonical form
        sorted_str = ''.join(sorted(string))
        anagram_map[sorted_str].append(string)
    
    return list(anagram_map.values())


# Example usage:
# Input: ["eat","tea","tan","ate","nat","bat"]
# 
# Internal state evolution:
# 
# "eat" ‚Üí sorted: "aet" ‚Üí map: {"aet": ["eat"]}
# "tea" ‚Üí sorted: "aet" ‚Üí map: {"aet": ["eat", "tea"]}
# "tan" ‚Üí sorted: "ant" ‚Üí map: {"aet": ["eat", "tea"], "ant": ["tan"]}
# "ate" ‚Üí sorted: "aet" ‚Üí map: {"aet": ["eat", "tea", "ate"], "ant": ["tan"]}
# "nat" ‚Üí sorted: "ant" ‚Üí map: {"aet": ["eat", "tea", "ate"], "ant": ["tan", "nat"]}
# "bat" ‚Üí sorted: "abt" ‚Üí map: {"aet": [...], "ant": [...], "abt": ["bat"]}
```

---

## Approach 2: Frequency Signature as Key

### Python Implementation
```python
def group_anagrams_frequency(strs: List[str]) -> List[List[str]]:
    """
    Use character frequency tuple as hash key.
    
    Time: O(nk) - better than sorting!
    Space: O(nk)
    
    Key Insight: Tuples are hashable in Python, lists are not.
    We convert frequency array to tuple for use as dict key.
    """
    anagram_map = defaultdict(list)
    
    for string in strs:
        # Create frequency signature
        # Initialize array for 26 lowercase letters
        count = [0] * 26
        for char in string:
            count[ord(char) - ord('a')] += 1
        
        # Convert list to tuple (hashable)
        key = tuple(count)
        anagram_map[key].append(string)
    
    return list(anagram_map.values())


# Visualization of frequency signature:
# "eat" ‚Üí [1,0,0,0,1,0,0,...,1,0,0] (a=1, e=1, t=1)
#         ‚Üì tuple
#         (1,0,0,0,1,0,0,...,1,0,0) ‚Üê Hash key
```

---

## Rust Implementation

```rust
use std::collections::HashMap;

pub fn group_anagrams_sorted(strs: Vec<String>) -> Vec<Vec<String>> {
    let mut anagram_map: HashMap<String, Vec<String>> = HashMap::new();
    
    for s in strs {
        // Sort characters to create key
        let mut chars: Vec<char> = s.chars().collect();
        chars.sort_unstable(); // unstable_sort is faster
        let key: String = chars.into_iter().collect();
        
        // Insert into map
        anagram_map.entry(key).or_insert_with(Vec::new).push(s);
    }
    
    anagram_map.into_values().collect()
}

// Frequency-based approach
pub fn group_anagrams_frequency(strs: Vec<String>) -> Vec<Vec<String>> {
    let mut anagram_map: HashMap<[u8; 26], Vec<String>> = HashMap::new();
    
    for s in strs {
        let mut count = [0u8; 26];
        
        for ch in s.chars() {
            count[(ch as usize) - ('a' as usize)] += 1;
        }
        
        anagram_map.entry(count).or_insert_with(Vec::new).push(s);
    }
    
    anagram_map.into_values().collect()
}

// Performance Notes:
// - sort_unstable() is faster than sort() (doesn't preserve order)
// - Using [u8; 26] array as key is efficient (implements Hash trait)
// - into_values() consumes the map (zero-copy)
```

---

## C++ Implementation

```cpp
#include <vector>
#include <string>
#include <unordered_map>
#include <algorithm>

class Solution {
public:
    // Approach 1: Sorted key
    std::vector<std::vector<std::string>> groupAnagrams_sorted(
        std::vector<std::string>& strs
    ) {
        std::unordered_map<std::string, std::vector<std::string>> anagram_map;
        
        for (const auto& s : strs) {
            std::string key = s;
            std::sort(key.begin(), key.end());
            anagram_map[key].push_back(s);
        }
        
        std::vector<std::vector<std::string>> result;
        result.reserve(anagram_map.size());
        
        for (auto& [key, group] : anagram_map) {
            result.push_back(std::move(group));
        }
        
        return result;
    }
    
    // Approach 2: Frequency key
    std::vector<std::vector<std::string>> groupAnagrams_frequency(
        std::vector<std::string>& strs
    ) {
        // Custom hash for array<int, 26>
        auto hash_fn = [](const std::array<int, 26>& arr) {
            size_t hash = 0;
            for (int i = 0; i < 26; ++i) {
                hash ^= std::hash<int>{}(arr[i]) + 0x9e3779b9 + 
                        (hash << 6) + (hash >> 2);
            }
            return hash;
        };
        
        std::unordered_map<
            std::array<int, 26>,
            std::vector<std::string>,
            decltype(hash_fn)
        > anagram_map(10, hash_fn);
        
        for (const auto& s : strs) {
            std::array<int, 26> count = {0};
            for (char ch : s) {
                count[ch - 'a']++;
            }
            anagram_map[count].push_back(s);
        }
        
        std::vector<std::vector<std::string>> result;
        result.reserve(anagram_map.size());
        
        for (auto& [key, group] : anagram_map) {
            result.push_back(std::move(group));
        }
        
        return result;
    }
};

// Note on custom hash function:
// C++ doesn't provide default hash for std::array
// We combine individual element hashes using XOR and bit shifts
// 0x9e3779b9 is the golden ratio used in many hash functions
```

---

## C Implementation

```c
#include <stdlib.h>
#include <string.h>

// Helper: Compare function for qsort
int char_compare(const void* a, const void* b) {
    return (*(char*)a - *(char*)b);
}

// Helper: Sort string in place
void sort_string(char* str) {
    qsort(str, strlen(str), sizeof(char), char_compare);
}

// Simple hash table implementation for C
#define TABLE_SIZE 10000

typedef struct Node {
    char* key;
    char** strings;
    int count;
    int capacity;
    struct Node* next;
} Node;

typedef struct {
    Node* buckets[TABLE_SIZE];
} HashTable;

// Simple hash function
unsigned int hash(const char* str) {
    unsigned int hash = 5381;
    int c;
    while ((c = *str++)) {
        hash = ((hash << 5) + hash) + c;
    }
    return hash % TABLE_SIZE;
}

// Note: Full implementation omitted for brevity
// C requires significant boilerplate for hash tables
// In practice, consider using libraries like uthash
```

---

## Go Implementation

```go
package main

import (
    "sort"
    "strings"
)

// Approach 1: Sorted key
func groupAnagrams_sorted(strs []string) [][]string {
    anagramMap := make(map[string][]string)
    
    for _, s := range strs {
        // Sort string to create key
        chars := strings.Split(s, "")
        sort.Strings(chars)
        key := strings.Join(chars, "")
        
        anagramMap[key] = append(anagramMap[key], s)
    }
    
    // Convert map to slice
    result := make([][]string, 0, len(anagramMap))
    for _, group := range anagramMap {
        result = append(result, group)
    }
    
    return result
}

// Approach 2: Frequency key
func groupAnagrams_frequency(strs []string) [][]string {
    anagramMap := make(map[[26]int][]string)
    
    for _, s := range strs {
        var count [26]int
        for _, ch := range s {
            count[ch-'a']++
        }
        
        anagramMap[count] = append(anagramMap[count], s)
    }
    
    result := make([][]string, 0, len(anagramMap))
    for _, group := range anagramMap {
        result = append(result, group)
    }
    
    return result
}

// Go advantage: Arrays are comparable and hashable by default
// [26]int can be used directly as map key
```

---

## Complexity Analysis Deep Dive

### Time Complexity Breakdown

**Sorted Approach**:
```
Total: O(n * k log k)

For each string (n times):
    1. Sort characters: O(k log k)
    2. Hash lookup: O(1) average
    3. Append to list: O(1) amortized

Where:
    n = number of strings
    k = maximum length of any string
```

**Frequency Approach**:
```
Total: O(n * k)

For each string (n times):
    1. Count characters: O(k)
    2. Create tuple/array key: O(26) = O(1)
    3. Hash lookup: O(1) average
    4. Append to list: O(1) amortized

Faster asymptotically! But tuple/array creation has overhead.
```

### Space Complexity
Both approaches: **O(nk)**
- Need to store all n strings
- Each string up to length k
- Hash map overhead: O(n) for keys

---

## Advanced Optimization: Prime Number Product

### Concept
Map each character to a unique prime number, multiply primes for each character.

```python
def group_anagrams_prime(strs: List[str]) -> List[List[str]]:
    """
    Clever but risky: Use prime product as hash key.
    
    Time: O(nk) - same as frequency
    Issue: Integer overflow for long strings
    """
    # First 26 primes for a-z
    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 
              43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101]
    
    anagram_map = defaultdict(list)
    
    for string in strs:
        product = 1
        for char in string:
            product *= primes[ord(char) - ord('a')]
        anagram_map[product].append(string)
    
    return list(anagram_map.values())

# Why it works: Fundamental Theorem of Arithmetic
# Every integer has unique prime factorization
# "eat" ‚Üí 2^1 * 101^1 * 373^1 (unique!)
# "tea" ‚Üí 2^1 * 101^1 * 373^1 (same!)
#
# Why it fails: Overflow for strings like "zzzzzzz..."
```

---

## Key Insights & Mental Models

### 1. **Canonical Form Pattern**
Transform data into a unique "canonical" representation:
- Anagrams ‚Üí Sorted string
- Coordinates ‚Üí "(x,y)" string
- Multiset ‚Üí Frequency signature

### 2. **Hash Key Design Checklist**
‚úì Uniquely identifies the group  
‚úì Same for all items in group  
‚úì Efficiently computable  
‚úì Hashable in your language  

### 3. **Language-Specific Considerations**

**Python**: 
- Lists not hashable ‚Üí use tuples
- `defaultdict(list)` simplifies code

**Rust**:
- Arrays implement `Hash` trait
- `entry().or_insert()` pattern is idiomatic

**C++**:
- Custom hash for complex keys
- Use `std::move` to avoid copies

**Go**:
- Arrays comparable by default
- Slicing strings is expensive

---

## Practice Problems

1. **Group Shifted Strings** (LeetCode 249)
   - Hash key: Difference sequence between characters

2. **Find Duplicate Subtrees** (LeetCode 652)
   - Hash key: Serialized tree structure

3. **Encode and Decode TinyURL** (LeetCode 535)
   - Bidirectional hash mapping

---

## Common Interview Follow-ups

**Q**: What if strings can have Unicode?  
**A**: Sorted approach still works. Frequency needs larger array or hash map.

**Q**: How to handle very long strings efficiently?  
**A**: Use frequency signature (O(k) vs O(k log k))

**Q**: Can we do better than O(nk)?  
**A**: No. Must examine every character at least once.

**Q**: How does hash collision affect performance?  
**A**: Rare with good hash function. Worst case O(n¬≤) with many collisions.

# Existence/Membership Patterns - Complete Guide

## Core Concept
Use hash sets/maps to achieve O(1) lookup time for checking existence or retrieving associated data.

**Fundamental Trade-off**: Space O(n) for Time O(1)

---

## Problem 1: Two Sum (LeetCode 1)

### Problem Statement
Given an array of integers and a target, return indices of two numbers that add up to target.

### Approach Evolution

#### Brute Force (Learning Step)
```
Approach: Try every pair
Time: O(n¬≤), Space: O(1)

for i in range(n):
    for j in range(i+1, n):
        if nums[i] + nums[j] == target:
            return [i, j]

Why it's slow: We're searching for complement O(n) times
```

#### Optimized: Hash Map
```
Key Insight: Instead of searching for complement each time,
             store what we've seen with O(1) lookup

Mental Model:
    "What number would complete this pair?"
    ‚Üí complement = target - current
    "Have I seen the complement before?"
    ‚Üí Check hash map
```

---

### Multi-Language Implementation

#### Python
```python
from typing import List

def two_sum(nums: List[int], target: int) -> List[int]:
    """
    One-pass hash map solution.
    
    Time: O(n) - single pass through array
    Space: O(n) - hash map stores up to n elements
    
    Mental Model: "Remember what you've seen, check if complement exists"
    """
    seen = {}  # maps number ‚Üí index
    
    for i, num in enumerate(nums):
        complement = target - num
        
        # Check if we've seen the complement
        if complement in seen:
            return [seen[complement], i]
        
        # Store current number for future lookups
        seen[num] = i
    
    return []  # No solution found


# Visualization for [2, 7, 11, 15], target=9:
#
# i=0: num=2, comp=7
#      7 not in seen ‚Üí seen={2: 0}
#
# i=1: num=7, comp=2
#      2 in seen! ‚Üí return [0, 1]


# Alternative: Two-pass solution (also O(n))
def two_sum_two_pass(nums: List[int], target: int) -> List[int]:
    """
    Build complete map first, then search.
    Same complexity but less elegant.
    """
    num_map = {num: i for i, num in enumerate(nums)}
    
    for i, num in enumerate(nums):
        complement = target - num
        if complement in num_map and num_map[complement] != i:
            return [i, num_map[complement]]
    
    return []
```

---

#### Rust
```rust
use std::collections::HashMap;

pub fn two_sum(nums: Vec<i32>, target: i32) -> Vec<i32> {
    let mut seen: HashMap<i32, i32> = HashMap::new();
    
    for (i, &num) in nums.iter().enumerate() {
        let complement = target - num;
        
        if let Some(&comp_idx) = seen.get(&complement) {
            return vec![comp_idx, i as i32];
        }
        
        seen.insert(num, i as i32);
    }
    
    vec![] // No solution
}

// Rust-specific notes:
// - HashMap<K, V> requires K: Eq + Hash
// - Use &num to avoid moving value from vector
// - if let Some(x) is idiomatic for Option handling
// - as i32 casts usize to i32


// Performance optimization with capacity hint
pub fn two_sum_optimized(nums: Vec<i32>, target: i32) -> Vec<i32> {
    let mut seen = HashMap::with_capacity(nums.len());
    
    for (i, &num) in nums.iter().enumerate() {
        let complement = target - num;
        
        if let Some(&comp_idx) = seen.get(&complement) {
            return vec![comp_idx, i as i32];
        }
        
        seen.insert(num, i as i32);
    }
    
    vec![]
}
// with_capacity() pre-allocates space, avoiding rehashing
```

---

#### C++
```cpp
#include <vector>
#include <unordered_map>

class Solution {
public:
    std::vector<int> twoSum(std::vector<int>& nums, int target) {
        std::unordered_map<int, int> seen;
        
        for (int i = 0; i < nums.size(); ++i) {
            int complement = target - nums[i];
            
            // Check if complement exists
            auto it = seen.find(complement);
            if (it != seen.end()) {
                return {it->second, i};
            }
            
            // Store current number
            seen[nums[i]] = i;
        }
        
        return {};
    }
    
    // Alternative: Using count() for existence check
    std::vector<int> twoSum_v2(std::vector<int>& nums, int target) {
        std::unordered_map<int, int> seen;
        
        for (int i = 0; i < nums.size(); ++i) {
            int complement = target - nums[i];
            
            if (seen.count(complement)) {  // count returns 0 or 1
                return {seen[complement], i};
            }
            
            seen[nums[i]] = i;
        }
        
        return {};
    }
};

// Performance notes:
// - find() returns iterator, single lookup
// - count() + access = two lookups (less efficient)
// - reserve() can pre-allocate: seen.reserve(nums.size())
```

---

#### C
```c
#include <stdlib.h>

// Simple hash table for integers
#define TABLE_SIZE 1000

typedef struct Entry {
    int key;
    int value;
    int occupied;
} Entry;

int hash_func(int key) {
    return abs(key) % TABLE_SIZE;
}

int* twoSum(int* nums, int numsSize, int target, int* returnSize) {
    Entry table[TABLE_SIZE] = {0};
    
    for (int i = 0; i < numsSize; i++) {
        int complement = target - nums[i];
        int idx = hash_func(complement);
        
        // Linear probing for collision resolution
        int original_idx = idx;
        while (table[idx].occupied) {
            if (table[idx].key == complement) {
                int* result = (int*)malloc(2 * sizeof(int));
                result[0] = table[idx].value;
                result[1] = i;
                *returnSize = 2;
                return result;
            }
            idx = (idx + 1) % TABLE_SIZE;
            if (idx == original_idx) break; // Table full
        }
        
        // Insert current number
        idx = hash_func(nums[i]);
        while (table[idx].occupied && table[idx].key != nums[i]) {
            idx = (idx + 1) % TABLE_SIZE;
        }
        table[idx].key = nums[i];
        table[idx].value = i;
        table[idx].occupied = 1;
    }
    
    *returnSize = 0;
    return NULL;
}

// Notes:
// - C requires manual hash table implementation
// - Linear probing: if collision, try next slot
// - Must handle hash collisions explicitly
```

---

#### Go
```go
package main

func twoSum(nums []int, target int) []int {
    seen := make(map[int]int)
    
    for i, num := range nums {
        complement := target - num
        
        // Check if complement exists
        if compIdx, exists := seen[complement]; exists {
            return []int{compIdx, i}
        }
        
        seen[num] = i
    }
    
    return []int{}
}

// Go idiom: "comma ok" pattern
// if value, ok := map[key]; ok { ... }

// Pre-sized map optimization
func twoSum_optimized(nums []int, target int) []int {
    seen := make(map[int]int, len(nums))
    
    for i, num := range nums {
        complement := target - num
        
        if compIdx, exists := seen[complement]; exists {
            return []int{compIdx, i}
        }
        
        seen[num] = i
    }
    
    return []int{}
}
```

---

## Problem 2: Contains Duplicate (LeetCode 217)

### Problem Statement
Return true if any value appears at least twice in array.

### Mental Model
```
Question: "Have I seen this number before?"
Answer:   Hash set gives O(1) answer!

Without hash set: O(n¬≤) - check each pair
With hash set:    O(n)  - check each once
```

### Implementation

#### Python
```python
def contains_duplicate_set(nums: List[int]) -> bool:
    """
    Most straightforward: use set.
    
    Time: O(n)
    Space: O(n)
    """
    seen = set()
    for num in nums:
        if num in seen:
            return True
        seen.add(num)
    return False


# Pythonic one-liner
def contains_duplicate_pythonic(nums: List[int]) -> bool:
    """
    Compare list length with set length.
    If different, duplicates exist.
    """
    return len(nums) != len(set(nums))
    # Elegant but builds entire set even if early duplicate


# Space-optimized for sorted input
def contains_duplicate_sorted(nums: List[int]) -> bool:
    """
    If allowed to modify input: sort first.
    
    Time: O(n log n)
    Space: O(1) if in-place sort
    """
    nums.sort()
    for i in range(len(nums) - 1):
        if nums[i] == nums[i + 1]:
            return True
    return False
```

---

#### Rust
```rust
use std::collections::HashSet;

pub fn contains_duplicate(nums: Vec<i32>) -> bool {
    let mut seen: HashSet<i32> = HashSet::new();
    
    for num in nums {
        if !seen.insert(num) {  // insert returns false if key existed
            return true;
        }
    }
    
    false
}

// Rust HashSet advantage: insert() returns bool
// true if inserted (new), false if already present


// Idiomatic functional approach
pub fn contains_duplicate_functional(nums: Vec<i32>) -> bool {
    let mut seen = HashSet::new();
    !nums.into_iter().all(|num| seen.insert(num))
}
// all() returns true only if all elements satisfy predicate
// We negate because we want to find duplicates
```

---

#### C++
```cpp
#include <vector>
#include <unordered_set>

class Solution {
public:
    bool containsDuplicate(std::vector<int>& nums) {
        std::unordered_set<int> seen;
        
        for (int num : nums) {
            if (seen.count(num)) {
                return true;
            }
            seen.insert(num);
        }
        
        return false;
    }
    
    // Alternative using insert return value
    bool containsDuplicate_v2(std::vector<int>& nums) {
        std::unordered_set<int> seen;
        
        for (int num : nums) {
            // insert returns pair<iterator, bool>
            // bool is false if element already existed
            if (!seen.insert(num).second) {
                return true;
            }
        }
        
        return false;
    }
};
```

---

#### Go
```go
func containsDuplicate(nums []int) bool {
    seen := make(map[int]bool)
    
    for _, num := range nums {
        if seen[num] {
            return true
        }
        seen[num] = true
    }
    
    return false
}

// Go idiom: map[int]bool for set behavior
// Alternative: map[int]struct{} uses less memory
func containsDuplicate_struct(nums []int) bool {
    seen := make(map[int]struct{})
    
    for _, num := range nums {
        if _, exists := seen[num]; exists {
            return true
        }
        seen[num] = struct{}{}
    }
    
    return false
}
// struct{} has zero size, saves memory for large sets
```

---

## Problem 3: Intersection of Two Arrays (LeetCode 349)

### Problem Statement
Given two arrays, return their intersection (unique elements present in both).

### Mental Model
```
Approach 1: Hash Set
    1. Convert array1 to set
    2. For each element in array2, check if in set
    3. Collect matches

Approach 2: Two Sets
    1. Convert both to sets
    2. Use set intersection operation (language-dependent)
```

### Implementation

#### Python
```python
def intersection(nums1: List[int], nums2: List[int]) -> List[int]:
    """
    Method 1: Manual checking.
    
    Time: O(n + m)
    Space: O(n)
    """
    set1 = set(nums1)
    result = set()
    
    for num in nums2:
        if num in set1:
            result.add(num)
    
    return list(result)


# Pythonic: Built-in set intersection
def intersection_pythonic(nums1: List[int], nums2: List[int]) -> List[int]:
    """
    Method 2: Use set intersection operator.
    Most elegant Python solution.
    """
    return list(set(nums1) & set(nums2))
    # & operator performs intersection


# If input already sorted (optimization)
def intersection_sorted(nums1: List[int], nums2: List[int]) -> List[int]:
    """
    Two-pointer approach for sorted arrays.
    
    Time: O(n + m)
    Space: O(1) excluding output
    """
    nums1.sort()
    nums2.sort()
    
    i, j = 0, 0
    result = []
    
    while i < len(nums1) and j < len(nums2):
        if nums1[i] < nums2[j]:
            i += 1
        elif nums1[i] > nums2[j]:
            j += 1
        else:
            if not result or result[-1] != nums1[i]:
                result.append(nums1[i])
            i += 1
            j += 1
    
    return result
```

---

#### Rust
```rust
use std::collections::HashSet;

pub fn intersection(nums1: Vec<i32>, nums2: Vec<i32>) -> Vec<i32> {
    let set1: HashSet<i32> = nums1.into_iter().collect();
    let mut result = HashSet::new();
    
    for num in nums2 {
        if set1.contains(&num) {
            result.insert(num);
        }
    }
    
    result.into_iter().collect()
}

// Functional style using iterator chains
pub fn intersection_functional(nums1: Vec<i32>, nums2: Vec<i32>) -> Vec<i32> {
    let set1: HashSet<i32> = nums1.into_iter().collect();
    
    nums2.into_iter()
        .filter(|num| set1.contains(num))
        .collect::<HashSet<_>>()  // Remove duplicates from nums2
        .into_iter()
        .collect()
}
```

---

## Complexity Analysis Summary

| Problem | Hash Approach Time | Hash Space | Alternative Time | Alternative Space |
|---------|-------------------|------------|------------------|-------------------|
| Two Sum | O(n) | O(n) | O(n¬≤) brute | O(1) |
| Contains Dup | O(n) | O(n) | O(n log n) sort | O(1) |
| Intersection | O(n+m) | O(n) | O(n log n + m log m) | O(1) two-ptr |

---

## Key Insights & Patterns

### 1. **The Complement Pattern** (Two Sum)
```
Instead of: "Find two numbers that sum to X"
Think:      "For each number, does (X - number) exist?"
```

### 2. **The Existence Check Pattern**
```
Question: "Is X in collection?"
Hash Set: O(1)
Array:    O(n) linear search
```

### 3. **The Deduplication Pattern**
```
Problem: Remove duplicates
Solution: Add to set (automatic deduplication)
```

### 4. **Space-Time Tradeoff**
```
No extra space: O(n¬≤) or O(n log n)
With O(n) space: O(n) time

Decision: Almost always choose hash solution unless:
    - Space is critically limited
    - Input is already sorted
    - Input is small (n < 100)
```

---

## Common Mistakes & Edge Cases

### 1. **Two Sum: Same Element Twice**
```python
# Wrong: Returns same index twice
if target - num in seen:
    return [seen[target - num], i]  # What if target = 2*num at i?

# Correct: Check after storing
seen[num] = i
```

### 2. **Collision Handling in C**
```c
// Must handle hash collisions explicitly
// Linear probing, chaining, or better hash function
```

### 3. **Integer Overflow**
```cpp
int complement = target - nums[i];  // May overflow!
// Use long long if needed
```

---

## Practice Problems

### Easy
1. **Happy Number** (LeetCode 202) - Detect cycle with set
2. **Intersection of Two Arrays II** (LeetCode 350) - Count duplicates
3. **Single Number** (LeetCode 136) - XOR trick vs hash

### Medium
4. **Four Sum II** (LeetCode 454) - Multiple hash maps
5. **Longest Consecutive Sequence** (LeetCode 128) - Set for O(n)
6. **Subarray Sum Equals K** (LeetCode 560) - Prefix sum + hash

### Hard
7. **Max Points on a Line** (LeetCode 149) - Hash slopes
8. **Contains Duplicate III** (LeetCode 220) - Bucketing technique

---

## Interview Tips

### When interviewer asks: "Can you do better than O(n¬≤)?"
‚Üí Think hash map/set

### When you see: "Find two/three elements that..."
‚Üí Consider hash map for O(1) lookup

### When asked: "What if space is limited?"
‚Üí Discuss sorted two-pointer alternative

### When implementing in interview:
1. State approach clearly
2. Discuss edge cases (empty input, no solution)
3. Mention complexity upfront
4. Write clean, readable code

# Rolling Hash Patterns - Complete Guide

## Foundational Concepts

### What is a Rolling Hash?

A **rolling hash** is a hash function that can be efficiently updated when the input changes by a small amount (e.g., sliding a window).

**Key Terminology:**
- **Hash**: A numerical "fingerprint" of data
- **Rolling**: Updating incrementally, not recalculating
- **Window**: Substring being hashed
- **Base**: Radix for polynomial hash (usually prime, e.g., 31, 256)
- **Modulo**: Keep hash values bounded to prevent overflow

---

## Mathematical Foundation

### Polynomial Rolling Hash

The most common rolling hash treats string as polynomial:

```
String: "ABC"
ASCII values: A=65, B=66, C=67

Hash = (65 * base¬≤) + (66 * base¬π) + (67 * base‚Å∞) mod prime

Example with base=31, prime=10‚Åπ+7:
Hash = (65 * 31¬≤) + (66 * 31) + 67
     = (65 * 961) + (66 * 31) + 67
     = 62465 + 2046 + 67
     = 64578
```

### Rolling Property

When sliding window from "ABC" to "BCD":

```
Old hash: hash("ABC") = (A * base¬≤) + (B * base¬π) + (C * base‚Å∞)

Remove 'A':
    Subtract: A * base¬≤

Shift left (multiply by base):
    (B * base¬π) + (C * base‚Å∞) ‚Üí (B * base¬≤) + (C * base¬π)

Add 'D':
    Add: D * base‚Å∞

New hash: hash("BCD") = (B * base¬≤) + (C * base¬π) + (D * base‚Å∞)

Formula:
    new_hash = (old_hash - first_char * base^(k-1)) * base + new_char
    where k = window length
```

---

## Visualization: Rolling Hash in Action

```
Text: "HELLO"
Pattern: "LL"
Base = 26, treating 'A'=1, 'B'=2, etc.

Step-by-step:

Window 1: "HE"
    H=8, E=5
    hash = 8*26 + 5 = 213

Window 2: "EL"
    Remove H=8 from position 1: 213 - 8*26 = 213 - 208 = 5
    Shift: 5 * 26 = 130
    Add L=12: 130 + 12 = 142

Window 3: "LL"
    Remove E=5: 142 - 5*26 = 142 - 130 = 12
    Shift: 12 * 26 = 312
    Add L=12: 312 + 12 = 324

Pattern hash("LL") = 12*26 + 12 = 324 ‚úì Match!
```

---

## Classic Problem: Rabin-Karp String Matching

### Problem Statement
Find all occurrences of pattern in text.

### Approach Evolution

#### Naive Approach
```
Time: O((n-m+1) * m) where n=text length, m=pattern length
For each position:
    Compare pattern character by character
    
Example: n=10‚Å∂, m=10¬≥ ‚Üí 10‚Åπ operations (too slow!)
```

#### Rabin-Karp with Rolling Hash
```
Time: O(n + m) average case
      O(n * m) worst case (many hash collisions)

Algorithm:
1. Compute hash of pattern
2. Compute hash of first window in text
3. Slide window:
   - Update hash in O(1)
   - If hash matches, verify character-by-character
```

---

### Multi-Language Implementation

#### Python
```python
def rabin_karp(text: str, pattern: str) -> list[int]:
    """
    Find all starting indices where pattern occurs in text.
    
    Time: O(n + m) average, O(n*m) worst
    Space: O(1)
    
    Uses polynomial rolling hash with base=256 (ASCII), prime=10^9+7
    """
    if not pattern or len(pattern) > len(text):
        return []
    
    # Constants
    BASE = 256
    PRIME = 10**9 + 7
    
    n, m = len(text), len(pattern)
    pattern_hash = 0
    window_hash = 0
    base_power = 1  # BASE^(m-1) mod PRIME
    
    # Compute base^(m-1) for removing leftmost character
    for i in range(m - 1):
        base_power = (base_power * BASE) % PRIME
    
    # Compute initial hashes for pattern and first window
    for i in range(m):
        pattern_hash = (pattern_hash * BASE + ord(pattern[i])) % PRIME
        window_hash = (window_hash * BASE + ord(text[i])) % PRIME
    
    result = []
    
    # Slide window through text
    for i in range(n - m + 1):
        # Check if hashes match
        if pattern_hash == window_hash:
            # Verify character by character (avoid false positives)
            if text[i:i+m] == pattern:
                result.append(i)
        
        # Compute hash for next window
        if i < n - m:
            # Remove leftmost character
            window_hash = (window_hash - ord(text[i]) * base_power) % PRIME
            # Shift left and add new character
            window_hash = (window_hash * BASE + ord(text[i + m])) % PRIME
            # Handle negative values
            window_hash = (window_hash + PRIME) % PRIME
    
    return result


# Visualization for text="ABCD", pattern="BC":
#
# Initial:
#   pattern_hash = hash("BC") = (66*256 + 67) % PRIME
#   window_hash = hash("AB") = (65*256 + 66) % PRIME
#   base_power = 256^1 = 256
#
# Window 1: "AB", i=0
#   window_hash != pattern_hash, skip
#
# Slide to Window 2: "BC", i=1
#   Remove 'A': (window_hash - 65*256) % PRIME
#   Shift & add 'C': (result * 256 + 67) % PRIME
#   Now window_hash == pattern_hash, verify and add index 1


# Example usage:
text = "AABAACAADAABAABA"
pattern = "AABA"
indices = rabin_karp(text, pattern)
print(f"Pattern found at indices: {indices}")
# Output: [0, 9, 12]
```

---

#### Rust
```rust
pub fn rabin_karp(text: &str, pattern: &str) -> Vec<usize> {
    if pattern.is_empty() || pattern.len() > text.len() {
        return vec![];
    }
    
    const BASE: u64 = 256;
    const PRIME: u64 = 1_000_000_007;
    
    let text_bytes = text.as_bytes();
    let pattern_bytes = pattern.as_bytes();
    let n = text_bytes.len();
    let m = pattern_bytes.len();
    
    // Compute base^(m-1) mod PRIME
    let mut base_power: u64 = 1;
    for _ in 0..m-1 {
        base_power = (base_power * BASE) % PRIME;
    }
    
    // Compute hashes
    let mut pattern_hash: u64 = 0;
    let mut window_hash: u64 = 0;
    
    for i in 0..m {
        pattern_hash = (pattern_hash * BASE + pattern_bytes[i] as u64) % PRIME;
        window_hash = (window_hash * BASE + text_bytes[i] as u64) % PRIME;
    }
    
    let mut result = Vec::new();
    
    // Slide window
    for i in 0..=n-m {
        if pattern_hash == window_hash {
            // Verify match
            if &text_bytes[i..i+m] == pattern_bytes {
                result.push(i);
            }
        }
        
        if i < n - m {
            // Rolling hash update
            let old_char = text_bytes[i] as u64;
            let new_char = text_bytes[i + m] as u64;
            
            // Remove old character
            window_hash = (window_hash + PRIME - (old_char * base_power) % PRIME) % PRIME;
            // Shift and add new character
            window_hash = (window_hash * BASE + new_char) % PRIME;
        }
    }
    
    result
}

// Rust advantages:
// - No negative modulo issues (we add PRIME before mod)
// - as_bytes() gives efficient byte slice access
// - u64 prevents overflow for reasonable string lengths
```

---

#### C++
```cpp
#include <vector>
#include <string>

class RabinKarp {
private:
    static constexpr long long BASE = 256;
    static constexpr long long PRIME = 1000000007;
    
public:
    std::vector<int> search(const std::string& text, const std::string& pattern) {
        if (pattern.empty() || pattern.length() > text.length()) {
            return {};
        }
        
        int n = text.length();
        int m = pattern.length();
        
        // Compute BASE^(m-1) mod PRIME
        long long base_power = 1;
        for (int i = 0; i < m - 1; ++i) {
            base_power = (base_power * BASE) % PRIME;
        }
        
        // Compute initial hashes
        long long pattern_hash = 0;
        long long window_hash = 0;
        
        for (int i = 0; i < m; ++i) {
            pattern_hash = (pattern_hash * BASE + pattern[i]) % PRIME;
            window_hash = (window_hash * BASE + text[i]) % PRIME;
        }
        
        std::vector<int> result;
        
        // Slide window
        for (int i = 0; i <= n - m; ++i) {
            if (pattern_hash == window_hash) {
                // Verify match
                if (text.substr(i, m) == pattern) {
                    result.push_back(i);
                }
            }
            
            if (i < n - m) {
                // Update hash
                window_hash = (window_hash - text[i] * base_power % PRIME + PRIME) % PRIME;
                window_hash = (window_hash * BASE + text[i + m]) % PRIME;
            }
        }
        
        return result;
    }
};

// Performance note:
// substr() creates string copy - use char comparison in production:
// bool match = true;
// for (int j = 0; j < m; ++j) {
//     if (text[i+j] != pattern[j]) { match = false; break; }
// }
```

---

#### C
```c
#include <string.h>
#include <stdlib.h>

#define BASE 256
#define PRIME 1000000007

int* rabin_karp(const char* text, const char* pattern, int* result_size) {
    int n = strlen(text);
    int m = strlen(pattern);
    
    if (m > n || m == 0) {
        *result_size = 0;
        return NULL;
    }
    
    // Compute base^(m-1)
    long long base_power = 1;
    for (int i = 0; i < m - 1; i++) {
        base_power = (base_power * BASE) % PRIME;
    }
    
    // Compute hashes
    long long pattern_hash = 0;
    long long window_hash = 0;
    
    for (int i = 0; i < m; i++) {
        pattern_hash = (pattern_hash * BASE + pattern[i]) % PRIME;
        window_hash = (window_hash * BASE + text[i]) % PRIME;
    }
    
    // Allocate result array (worst case: all positions match)
    int* result = (int*)malloc((n - m + 1) * sizeof(int));
    int count = 0;
    
    // Slide window
    for (int i = 0; i <= n - m; i++) {
        if (pattern_hash == window_hash) {
            // Verify match
            int match = 1;
            for (int j = 0; j < m; j++) {
                if (text[i + j] != pattern[j]) {
                    match = 0;
                    break;
                }
            }
            if (match) {
                result[count++] = i;
            }
        }
        
        if (i < n - m) {
            // Update hash
            window_hash = (window_hash - (long long)text[i] * base_power % PRIME + PRIME) % PRIME;
            window_hash = (window_hash * BASE + text[i + m]) % PRIME;
        }
    }
    
    *result_size = count;
    return result;
}
```

---

#### Go
```go
package main

const (
    BASE  = 256
    PRIME = 1000000007
)

func rabinKarp(text, pattern string) []int {
    n, m := len(text), len(pattern)
    
    if m == 0 || m > n {
        return []int{}
    }
    
    // Compute base^(m-1) mod PRIME
    basePower := 1
    for i := 0; i < m-1; i++ {
        basePower = (basePower * BASE) % PRIME
    }
    
    // Compute hashes
    patternHash := 0
    windowHash := 0
    
    for i := 0; i < m; i++ {
        patternHash = (patternHash*BASE + int(pattern[i])) % PRIME
        windowHash = (windowHash*BASE + int(text[i])) % PRIME
    }
    
    result := []int{}
    
    // Slide window
    for i := 0; i <= n-m; i++ {
        if patternHash == windowHash {
            // Verify match
            if text[i:i+m] == pattern {
                result = append(result, i)
            }
        }
        
        if i < n-m {
            // Update hash
            windowHash = (windowHash - int(text[i])*basePower%PRIME + PRIME) % PRIME
            windowHash = (windowHash*BASE + int(text[i+m])) % PRIME
        }
    }
    
    return result
}
```

---

## Problem: Longest Duplicate Substring (LeetCode 1044)

### Problem Statement
Find any duplicate substring of maximum length in string S.

### Approach: Binary Search + Rolling Hash

**Key Insight**: If there's a duplicate of length k, there's also duplicate of length k-1.
This monotonic property allows binary search!

```
Algorithm:
1. Binary search on substring length (1 to n-1)
2. For each length, use rolling hash to find duplicates
3. Return longest found

Time: O(n log n)
    - Binary search: O(log n) iterations
    - Each iteration: O(n) with rolling hash
```

#### Python Implementation
```python
def longest_dup_substring(s: str) -> str:
    """
    Binary search on length + rolling hash for detection.
    
    Time: O(n log n)
    Space: O(n)
    """
    def search_duplicate(length: int) -> int:
        """
        Check if duplicate substring of given length exists.
        Returns starting index of duplicate, or -1 if none.
        """
        BASE = 256
        PRIME = 2**63 - 1  # Large prime
        
        # Compute hash for first window
        h = 0
        base_power = 1
        for i in range(length):
            h = (h * BASE + ord(s[i])) % PRIME
            if i < length - 1:
                base_power = (base_power * BASE) % PRIME
        
        # Store seen hashes
        seen = {h: [0]}
        
        # Slide window and check for duplicates
        for start in range(1, len(s) - length + 1):
            # Rolling hash
            h = (h - ord(s[start - 1]) * base_power) % PRIME
            h = (h * BASE + ord(s[start + length - 1])) % PRIME
            h = (h + PRIME) % PRIME
            
            if h in seen:
                # Hash collision or actual match?
                current = s[start:start + length]
                for prev_start in seen[h]:
                    if s[prev_start:prev_start + length] == current:
                        return start
                seen[h].append(start)
            else:
                seen[h] = [start]
        
        return -1
    
    # Binary search on length
    left, right = 1, len(s) - 1
    result_start = -1
    result_length = 0
    
    while left <= right:
        mid = (left + right) // 2
        start = search_duplicate(mid)
        
        if start != -1:
            result_start = start
            result_length = mid
            left = mid + 1  # Try longer
        else:
            right = mid - 1  # Try shorter
    
    return s[result_start:result_start + result_length] if result_start != -1 else ""


# Example:
# s = "banana"
# Binary search: try lengths 3, 2, 1
# Length 3: search_duplicate(3)
#   "ban": hash1
#   "ana": hash2
#   "nan": hash3
#   "ana": hash2 again! ‚Üí duplicate found
# Return "ana"
```

---

## Complexity Analysis

### Rolling Hash Technique

| Operation | Time | Space | Notes |
|-----------|------|-------|-------|
| Initial hash | O(m) | O(1) | m = pattern length |
| Hash update | O(1) | O(1) | Key advantage! |
| Total (n chars) | O(n) | O(1) | vs O(nm) naive |

### Rabin-Karp Algorithm

- **Best case**: O(n + m) - no collisions
- **Average case**: O(n + m) - few collisions
- **Worst case**: O(nm) - many collisions (rare with good hash)

---

## Key Insights & Optimization Techniques

### 1. **Choosing Base and Prime**

```python
# Common choices:
BASE = 256  # For ASCII strings
BASE = 26   # For lowercase English letters
BASE = 10   # For numeric strings

PRIME = 10**9 + 7  # Large prime, avoids overflow in most languages
PRIME = 2**61 - 1  # Mersenne prime, efficient modulo

# Trade-off:
# Larger prime ‚Üí Less collision, more overflow risk
# Smaller prime ‚Üí More collision, safer arithmetic
```

### 2. **Handling Negative Modulo**

```python
# Problem: (a - b) % PRIME can be negative in some languages

# Solution: Add PRIME before modulo
hash_val = (hash_val - contribution + PRIME) % PRIME

# Or ensure non-negative:
hash_val = ((hash_val - contribution) % PRIME + PRIME) % PRIME
```

### 3. **Double Hashing to Reduce Collisions**

```python
def double_hash(s: str) -> tuple:
    """
    Use two different bases/primes for ultra-low collision rate.
    """
    BASE1, PRIME1 = 256, 10**9 + 7
    BASE2, PRIME2 = 31, 10**9 + 9
    
    hash1, hash2 = 0, 0
    for char in s:
        hash1 = (hash1 * BASE1 + ord(char)) % PRIME1
        hash2 = (hash2 * BASE2 + ord(char)) % PRIME2
    
    return (hash1, hash2)

# Probability of collision: 1/(PRIME1 * PRIME2) ‚âà 10^-18
```

---

## Common Mistakes & Pitfalls

### 1. **Integer Overflow**
```python
# Wrong: Can overflow even with modulo
hash_val = hash_val * BASE + char  # Overflow before modulo!

# Correct: Modulo intermediate results
hash_val = (hash_val * BASE) % PRIME
hash_val = (hash_val + char) % PRIME
```

### 2. **Forgetting to Verify Match**
```python
# Wrong: Trust hash equality
if pattern_hash == window_hash:
    return True  # False positive possible!

# Correct: Always verify character-by-character
if pattern_hash == window_hash and text[i:i+m] == pattern:
    return True
```

### 3. **Incorrect Base Power Calculation**
```python
# Wrong:
base_power = BASE ** (m - 1)  # Not modulo'd!

# Correct:
base_power = 1
for _ in range(m - 1):
    base_power = (base_power * BASE) % PRIME
```

---

## Practice Problems

### Easy
1. **Implement strStr()** (LeetCode 28) - Basic Rabin-Karp
2. **Repeated Substring Pattern** (LeetCode 459) - Rolling hash variant

### Medium
3. **Repeated DNA Sequences** (LeetCode 187) - Fixed-length windows
4. **Longest Duplicate Substring** (LeetCode 1044) - Binary search + rolling hash
5. **Find All Anagrams** (LeetCode 438) - Frequency hash variant

### Hard
6. **Longest Happy Prefix** (LeetCode 1392) - KMP or rolling hash
7. **Distinct Echo Substrings** (LeetCode 1316) - Advanced rolling hash

---

## Interview Strategy

### When to Apply Rolling Hash:
‚úì Substring matching with many queries  
‚úì Finding duplicate substrings  
‚úì Comparing many substrings efficiently  
‚úì Sliding window with string comparison  

### When NOT to Apply:
‚úó Single substring search (KMP better)  
‚úó Exact matching without collisions (simple comparison)  
‚úó Very short patterns (overhead not worth it)  

### Key Points to Mention:
1. "Using rolling hash reduces per-window cost from O(m) to O(1)"
2. "Need to handle hash collisions with character verification"
3. "Choice of base and prime affects collision rate"
4. "Can combine with binary search for optimization problems"

# Custom Hash Patterns - Complete Guide

## Philosophy: The Art of Hash Function Design

Custom hash patterns require **creative mathematical thinking**. You must:
1. Identify the "sameness" property
2. Design a function that maps equivalent items to same value
3. Ensure efficient computation

This is problem-solving at its most elegant.

---

## Pattern 5.1: Coordinate/Position Hashing

### Problem: Robot Return to Origin (LeetCode 657)

**Task**: Determine if robot returns to origin after executing moves.

**Insight**: We only care about final position, not path.

#### Mental Model
```
Moves: "UDLR"
Instead of: Tracking every position
Think:      net_x = 0, net_y = 0?

Hash key: (x, y) coordinate tuple
```

#### Implementation

```python
def judge_circle(moves: str) -> bool:
    """
    No hash needed - just track position!
    But demonstrates coordinate hashing concept.
    
    Time: O(n)
    Space: O(1)
    """
    x, y = 0, 0
    
    for move in moves:
        if move == 'U': y += 1
        elif move == 'D': y -= 1
        elif move == 'L': x -= 1
        elif move == 'R': x += 1
    
    return x == 0 and y == 0


# More complex: Track visited positions
def count_unique_positions(moves: str) -> int:
    """
    Count unique positions visited.
    Hash key: (x, y) tuple
    """
    visited = {(0, 0)}  # Start position
    x, y = 0, 0
    
    for move in moves:
        if move == 'U': y += 1
        elif move == 'D': y -= 1
        elif move == 'L': x -= 1
        elif move == 'R': x += 1
        
        visited.add((x, y))
    
    return len(visited)


# Advanced: Convert tuple to single integer (space optimization)
def position_to_int(x: int, y: int, width: int = 10000) -> int:
    """
    Map 2D coordinate to 1D integer.
    
    Assumption: coordinates bounded by width
    x, y in [-width, width]
    
    Formula: (x + width) * (2 * width + 1) + (y + width)
    """
    return (x + width) * (2 * width + 1) + (y + width)

# Example:
# (0, 0) ‚Üí 10000 * 20001 + 10000 = 200020000
# (1, 0) ‚Üí 10001 * 20001 + 10000 = 200030001
# Unique mapping!
```

---

## Pattern 5.2: Sorted/Canonical Form Hashing

### Problem: 4Sum II (LeetCode 454)

**Task**: Count quadruplets (i,j,k,l) where nums1[i] + nums2[j] + nums3[k] + nums4[l] = 0

**Naive**: O(n‚Å¥) - try all combinations

**Optimized with Hash**: O(n¬≤)

#### Expert Thought Process
```
Rearrange equation:
    nums1[i] + nums2[j] + nums3[k] + nums4[l] = 0
    ‚Üì
    nums1[i] + nums2[j] = -(nums3[k] + nums4[l])

Strategy:
1. Compute all sums from nums1 + nums2 ‚Üí Hash map (sum ‚Üí count)
2. For each sum from nums3 + nums4, check if negation exists in map
3. Add count of matching pairs

Hash key: Integer sum
```

#### Implementation

```python
from collections import defaultdict
from typing import List

def four_sum_count(
    nums1: List[int], 
    nums2: List[int], 
    nums3: List[int], 
    nums4: List[int]
) -> int:
    """
    Count quadruplets summing to zero.
    
    Time: O(n¬≤)
    Space: O(n¬≤)
    
    Key: Divide into two O(n¬≤) problems instead of one O(n‚Å¥)
    """
    # Step 1: Count all sums from nums1 + nums2
    sum_count = defaultdict(int)
    for a in nums1:
        for b in nums2:
            sum_count[a + b] += 1
    
    # Step 2: For each sum from nums3 + nums4, check complement
    result = 0
    for c in nums3:
        for d in nums4:
            complement = -(c + d)
            if complement in sum_count:
                result += sum_count[complement]
    
    return result


# Visualization for small example:
# nums1 = [1, 2], nums2 = [-2, -1], nums3 = [-1, 2], nums4 = [0, 2]
#
# sum_count after step 1:
# {
#     1 + (-2) = -1: 1,
#     1 + (-1) =  0: 1,
#     2 + (-2) =  0: 1,
#     2 + (-1) =  1: 1
# }
# ‚Üí {-1: 1, 0: 2, 1: 1}
#
# Step 2:
# c=-1, d=0:  -(‚àí1+0) = 1  ‚Üí sum_count[1] = 1, result += 1
# c=-1, d=2:  -(‚àí1+2) = -1 ‚Üí sum_count[-1] = 1, result += 1
# c=2, d=0:   -(2+0) = -2  ‚Üí not in map
# c=2, d=2:   -(2+2) = -4  ‚Üí not in map
#
# Total: 2
```

---

## Pattern 5.3: Multiset/Frequency Signature Hashing

### Problem: Group Shifted Strings (LeetCode 249)

**Task**: Group strings that can be transformed into each other by shifting.

**Example**: `["abc", "bcd", "xyz"]` ‚Üí `[["abc","bcd"], ["xyz"]]`
- "abc" ‚Üí "bcd" by shifting each char by +1
- But "abc" ‚â† "xyz" (different shift pattern)

#### Mental Model
```
What makes strings "shift-equivalent"?
    ‚Üí Same pattern of differences between adjacent characters!

"abc": differences = [b-a, c-b] = [1, 1]
"bcd": differences = [c-b, d-c] = [1, 1]  ‚Üê Same!
"xyz": differences = [y-x, z-y] = [1, 1]  ‚Üê Also same!

Wait, that's wrong! Need better hash...

Better hash: Shift to start with 'a'
"abc" ‚Üí "abc" (already starts with 'a')
"bcd" ‚Üí "abc" (shift by -1)
"xyz" ‚Üí "abc" (shift by -23)

Even better: Use difference tuple as hash key!
But handle wrap-around: 'za' ‚Üí [25, -25] or [25, 1]?

Canonical form: Normalize differences modulo 26
```

#### Implementation

```python
from collections import defaultdict

def group_shifted_strings(strings: List[str]) -> List[List[str]]:
    """
    Group strings that are shifts of each other.
    
    Time: O(n * m) where n = len(strings), m = max string length
    Space: O(n * m)
    
    Hash key: Tuple of character differences
    """
    def get_hash(s: str) -> tuple:
        """
        Compute hash based on character differences.
        Normalize to handle wrap-around.
        """
        if len(s) == 1:
            return (0,)  # Single char, all map to same
        
        diffs = []
        for i in range(1, len(s)):
            # Difference between consecutive characters
            diff = (ord(s[i]) - ord(s[i-1])) % 26
            diffs.append(diff)
        
        return tuple(diffs)
    
    groups = defaultdict(list)
    
    for s in strings:
        hash_key = get_hash(s)
        groups[hash_key].append(s)
    
    return list(groups.values())


# Example walkthrough:
# "abc": diffs = [(98-97)%26, (99-98)%26] = [1, 1]
# "bcd": diffs = [(99-98)%26, (100-99)%26] = [1, 1]  ‚Üê Same hash!
# "xyz": diffs = [(121-120)%26, (122-121)%26] = [1, 1]  ‚Üê Same hash!
# "az":  diffs = [(122-97)%26] = [25]
# "ba":  diffs = [(97-98)%26] = [25]  ‚Üê Same hash!
#
# Result: [["abc","bcd","xyz"], ["az","ba"]]


# Alternative: Shift to canonical form starting with 'a'
def group_shifted_strings_canonical(strings: List[str]) -> List[List[str]]:
    """
    Alternative: Normalize each string to start with 'a'.
    """
    def normalize(s: str) -> str:
        if not s:
            return s
        
        # Shift so first char becomes 'a'
        shift = ord('a') - ord(s[0])
        
        normalized = []
        for char in s:
            # Shift each character
            new_char = chr((ord(char) - ord('a') + shift) % 26 + ord('a'))
            normalized.append(new_char)
        
        return ''.join(normalized)
    
    groups = defaultdict(list)
    
    for s in strings:
        canonical = normalize(s)
        groups[canonical].append(s)
    
    return list(groups.values())
```

---

## Pattern 5.4: Structural Hashing (Trees/Graphs)

### Problem: Find Duplicate Subtrees (LeetCode 652)

**Task**: Find all duplicate subtrees in binary tree.

**Challenge**: How to hash a tree structure?

#### Mental Model
```
Tree:
      1
     / \
    2   3
   /   / \
  4   2   4
     /
    4

How to identify duplicate subtree?
    ‚Üí Serialize tree structure into string!

Subtree rooted at left '2': "2,4,#,#,#"
Subtree rooted at middle '2': "2,4,#,#,#"  ‚Üê Same! Duplicate found

Hash key: Serialized tree structure
```

#### Implementation

```python
from collections import defaultdict

class TreeNode:
    def __init__(self, val=0, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right


def find_duplicate_subtrees(root: TreeNode) -> List[TreeNode]:
    """
    Find all duplicate subtrees using structural hashing.
    
    Time: O(n¬≤) worst case (serialization cost)
    Space: O(n¬≤) for storing serializations
    
    Key: Serialize subtree structure as hash key
    """
    subtree_count = defaultdict(int)
    result = []
    
    def serialize(node: TreeNode) -> str:
        """
        Serialize subtree structure.
        Format: "value,left_subtree,right_subtree"
        """
        if not node:
            return "#"
        
        # Post-order traversal (left, right, root)
        left_serial = serialize(node.left)
        right_serial = serialize(node.right)
        
        # Create unique structure identifier
        serial = f"{node.val},{left_serial},{right_serial}"
        
        # Track count of this structure
        subtree_count[serial] += 1
        
        # Add to result only on second occurrence
        if subtree_count[serial] == 2:
            result.append(node)
        
        return serial
    
    serialize(root)
    return result


# Visualization:
#       1
#      / \
#     2   3
#    /   / \
#   4   2   4
#      /
#     4
#
# Serialization (post-order):
#
# Node with value 4 (leftmost): "4,#,#"
# Node with value 2 (left):     "2,4,#,#,#"
# Node with value 4 (bottom middle): "4,#,#"
# Node with value 2 (middle):   "2,4,#,#,#"  ‚Üê Duplicate!
# Node with value 4 (rightmost): "4,#,#"  ‚Üê Duplicate!
# Node with value 3: "3,2,4,#,#,#,4,#,#"
# Node with value 1: "1,2,4,#,#,#,3,2,4,#,#,#,4,#,#"


# Optimization: Use integer IDs instead of strings
def find_duplicate_subtrees_optimized(root: TreeNode) -> List[TreeNode]:
    """
    Space-optimized version using integer IDs.
    
    Time: O(n)
    Space: O(n)
    """
    trees = defaultdict(int)  # serialization ‚Üí ID
    count = defaultdict(int)  # ID ‚Üí count
    result = []
    uid = 0
    
    def serialize(node):
        nonlocal uid
        if not node:
            return 0
        
        left_id = serialize(node.left)
        right_id = serialize(node.right)
        
        # Unique identifier for this structure
        serial = (node.val, left_id, right_id)
        
        if serial not in trees:
            uid += 1
            trees[serial] = uid
        
        tree_id = trees[serial]
        count[tree_id] += 1
        
        if count[tree_id] == 2:
            result.append(node)
        
        return tree_id
    
    serialize(root)
    return result
```

---

## Pattern 5.5: Mathematical Hash Functions

### Problem: Max Points on a Line (LeetCode 149)

**Task**: Find maximum points lying on the same straight line.

**Challenge**: How to identify "same line"?

#### Mathematical Insight
```
Line equation: y = mx + b
Two points define a line by slope: m = (y2-y1)/(x2-x1)

Problem: Points with same slope from reference point lie on same line!

Hash key: Slope (as fraction to avoid floating point issues)
```

#### Implementation

```python
from collections import defaultdict
from math import gcd
from typing import List

def max_points(points: List[List[int]]) -> int:
    """
    Find max points on a line using slope hashing.
    
    Time: O(n¬≤)
    Space: O(n)
    
    Key: For each point, count points with same slope
    """
    if len(points) <= 2:
        return len(points)
    
    def get_slope(p1: List[int], p2: List[int]) -> tuple:
        """
        Compute slope as reduced fraction (dy/dx).
        Returns (dy, dx) tuple.
        """
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        
        # Handle vertical line
        if dx == 0:
            return (1, 0)  # Infinite slope
        
        # Handle horizontal line
        if dy == 0:
            return (0, 1)  # Zero slope
        
        # Reduce fraction using GCD
        g = gcd(abs(dy), abs(dx))
        dy, dx = dy // g, dx // g
        
        # Normalize sign (always keep positive dx)
        if dx < 0:
            dy, dx = -dy, -dx
        
        return (dy, dx)
    
    max_count = 0
    
    # Try each point as reference
    for i in range(len(points)):
        slopes = defaultdict(int)
        duplicate = 0
        
        for j in range(i + 1, len(points)):
            # Check for duplicate points
            if points[i] == points[j]:
                duplicate += 1
                continue
            
            # Compute slope and count
            slope = get_slope(points[i], points[j])
            slopes[slope] += 1
        
        # Max for this reference point
        if slopes:
            current_max = max(slopes.values()) + 1 + duplicate
        else:
            current_max = 1 + duplicate
        
        max_count = max(max_count, current_max)
    
    return max_count


# Example:
# points = [[1,1],[2,2],[3,3],[1,3],[2,4]]
#
# Reference point (1,1):
#   To (2,2): slope = (2-1)/(2-1) = 1/1 ‚Üí (1,1)
#   To (3,3): slope = (3-1)/(3-1) = 2/2 ‚Üí (1,1)  ‚Üê Same line!
#   To (1,3): slope = (3-1)/(1-1) = ‚àû ‚Üí (1,0)
#   To (2,4): slope = (4-1)/(2-1) = 3/1 ‚Üí (3,1)
#
# slopes = {(1,1): 2, (1,0): 1, (3,1): 1}
# max for this reference = 2 + 1 = 3
#
# Answer: 3 points on line (1,1)-(2,2)-(3,3)


# Why use GCD?
# Avoid issues like: 2/4 and 1/2 being different keys but same slope
# gcd(2,4) = 2 ‚Üí 2/4 reduces to 1/2
```

---

## Pattern 5.6: Custom Composite Keys

### Problem: LRU Cache (LeetCode 146)

**Challenge**: Need O(1) get AND put operations with eviction policy.

**Solution**: Hash map + doubly linked list

#### Mental Model
```
Hash map: key ‚Üí node (O(1) access)
DLL: maintains access order (O(1) move to front)

Custom structure, not just hash function!

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Hash Map                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ key   ‚îÇ node pointer ‚îÇ          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§          ‚îÇ
‚îÇ   ‚îÇ  "a"  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  "b"  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îê    ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                  ‚îÇ  ‚îÇ                ‚îÇ
‚îÇ   Doubly Linked List (LRU order)    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ   ‚îÇ a  ‚îÇ‚Üê‚Üí‚îÇ b  ‚îÇ‚Üê‚Üí‚îÇ c  ‚îÇ            ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ   ^head           tail^             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Complexity Comparison Table

| Pattern | Hash Key | Time | Space | Best For |
|---------|----------|------|-------|----------|
| Coordinate | (x,y) tuple | O(1) | O(1) | Position tracking |
| Canonical form | Sorted/normalized | O(k) | O(k) | Equivalence classes |
| Frequency | Count tuple | O(k) | O(1) | Anagrams, permutations |
| Structural | Serialization | O(n) | O(n) | Trees, graphs |
| Slope | Fraction (dy,dx) | O(1) | O(1) | Geometric problems |

---

## Design Principles for Custom Hash Functions

### 1. **Uniqueness**
Hash must uniquely identify equivalence class.

### 2. **Efficiency**
Should be computable in O(1) or O(k) where k is small.

### 3. **Determinism**
Same input always produces same hash.

### 4. **Hashability**
Result must be hashable in your language:
- Python: immutable (tuple, string, int)
- Rust: implements Hash trait
- C++: default hashable or custom hash function

### 5. **Collision Handling**
For critical applications, verify actual equality on collision.

---

## Advanced Technique: Zobrist Hashing

### Concept
Pre-assign random numbers to each possible state component, XOR them together.

**Use case**: Board games, state space search

```python
import random

class ZobristHash:
    """
    Fast incremental hashing for board states.
    Used in chess engines, Go programs.
    """
    def __init__(self, board_size=8, num_pieces=6):
        # Pre-generate random numbers for each (position, piece) pair
        random.seed(42)  # For reproducibility
        self.table = {}
        
        for row in range(board_size):
            for col in range(board_size):
                for piece in range(num_pieces):
                    # Random 64-bit integer for each state
                    self.table[(row, col, piece)] = random.getrandbits(64)
    
    def compute_hash(self, board):
        """
        Compute hash of board state.
        board[i][j] = piece type (0 = empty)
        """
        hash_val = 0
        for row in range(len(board)):
            for col in range(len(board[0])):
                piece = board[row][col]
                if piece != 0:  # Not empty
                    # XOR in the random number for this configuration
                    hash_val ^= self.table[(row, col, piece)]
        return hash_val
    
    def update_hash(self, old_hash, move):
        """
        Update hash incrementally in O(1).
        move = (from_row, from_col, to_row, to_col, piece)
        """
        from_row, from_col, to_row, to_col, piece = move
        
        # XOR out old position
        old_hash ^= self.table[(from_row, from_col, piece)]
        # XOR in new position
        old_hash ^= self.table[(to_row, to_col, piece)]
        
        return old_hash

# Property: XOR is its own inverse
# hash ^ x ^ x = hash (cancels out)
# This makes incremental updates possible!
```

---

## Practice Problems by Pattern

### Coordinate Hashing
1. **Robot Room Cleaner** (LeetCode 489)
2. **Spiral Matrix III** (LeetCode 885)

### Canonical Form
3. **Group Anagrams** (LeetCode 49) - Already covered
4. **Group Shifted Strings** (LeetCode 249)

### Structural Hashing
5. **Serialize and Deserialize Binary Tree** (LeetCode 297)
6. **Find Duplicate Subtrees** (LeetCode 652)

### Mathematical Hashing
7. **Max Points on a Line** (LeetCode 149)
8. **Mirror Reflection** (LeetCode 858)

### Composite Data Structures
9. **LRU Cache** (LeetCode 146)
10. **LFU Cache** (LeetCode 460)

---

## Interview Strategy

### Problem Recognition Checklist

**Use custom hash when:**
‚úì Standard hash doesn't capture problem structure  
‚úì Need to group by complex property  
‚úì Geometric or mathematical relationship  
‚úì Tree/graph isomorphism  

### Design Process

1. **Identify invariant**: What stays same for equivalent items?
2. **Choose representation**: Tuple, string, integer?
3. **Implement efficiently**: Avoid expensive operations
4. **Test edge cases**: Empty, single element, duplicates

### Common Interview Questions

**Q**: "How did you choose this hash function?"  
**A**: "I identified that [property] determines equivalence, then designed a function capturing that property uniquely and efficiently."

**Q**: "What if there are hash collisions?"  
**A**: "I handle collisions by [verification step / better hash / separate chaining]."

**Q**: "Can you optimize space/time?"  
**A**: "Currently O(n) space. Could reduce by [specific technique] if constraints allow."

---

## Mental Models Summary

1. **Coordinate**: Position as identity
2. **Canonical**: Normalize to standard form
3. **Frequency**: Count-based signatures
4. **Structural**: Serialize complex structure
5. **Mathematical**: Capture geometric/algebraic property
6. **Incremental**: Update hash without full recomputation

Master these patterns, and you'll handle 95% of hash function problems in interviews and competitions!

# Complete Hash Function Patterns Mastery Guide
## Your Path to the Top 1%

---

## üéØ Learning Philosophy

*"A hash function is not just a tool‚Äîit's a way of thinking about equivalence, identity, and computational efficiency."*

This guide is structured for **deliberate practice**: understanding fundamentals deeply, recognizing patterns quickly, and solving problems elegantly.

---

## üìö Complete Pattern Taxonomy

```
HASH FUNCTION PATTERNS HIERARCHY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CORE CONCEPT                            ‚îÇ
‚îÇ   Map infinite input space ‚Üí finite output space          ‚îÇ
‚îÇ   Property: Same input ‚Üí Same output (deterministic)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PATTERN 1    ‚îÇ  ‚îÇ   PATTERN 2    ‚îÇ  ‚îÇ  PATTERN 3  ‚îÇ
‚îÇ   Frequency/   ‚îÇ  ‚îÇ   Grouping/    ‚îÇ  ‚îÇ  Existence/ ‚îÇ
‚îÇ   Counting     ‚îÇ  ‚îÇ   Bucketing    ‚îÇ  ‚îÇ  Membership ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ   PATTERN 4    ‚îÇ  ‚îÇ   PATTERN 5    ‚îÇ         ‚îÇ
‚îÇ   Rolling Hash ‚îÇ  ‚îÇ   Custom Hash  ‚îÇ         ‚îÇ
‚îÇ   (Advanced)   ‚îÇ  ‚îÇ   (Creative)   ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
                                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  APPLICATIONS  ‚îÇ
            ‚îÇ  ‚Ä¢ Strings     ‚îÇ
            ‚îÇ  ‚Ä¢ Arrays      ‚îÇ
            ‚îÇ  ‚Ä¢ Trees       ‚îÇ
            ‚îÇ  ‚Ä¢ Geometry    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß† Mental Models: The Foundation

### Model 1: The Fingerprint Analogy
```
Just as fingerprints uniquely identify people:
    Hash functions create "fingerprints" for data

Properties:
    ‚úì Quick to generate (O(1) or O(k))
    ‚úì Unique enough (minimize collisions)
    ‚úì Consistent (same input ‚Üí same fingerprint)
```

### Model 2: The Bucket Sorting Analogy
```
Hash table = Array of buckets
Hash function = Determines which bucket

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Items: [apple, banana, cherry]     ‚îÇ
‚îÇ                ‚îÇ                    ‚îÇ
‚îÇ         Hash Function               ‚îÇ
‚îÇ                ‚îÇ                    ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ     ‚ñº          ‚ñº          ‚ñº         ‚îÇ
‚îÇ  Bucket 0   Bucket 1   Bucket 2     ‚îÇ
‚îÇ  [apple]    [banana]   [cherry]     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Why it's fast: O(1) access to correct bucket
```

### Model 3: The Equivalence Class Lens
```
Problem: Group similar items

Traditional approach:
    Compare every pair ‚Üí O(n¬≤)

Hash approach:
    Map to equivalence class ‚Üí O(n)

Example - Anagrams:
    "eat", "tea", "ate" ‚Üí ALL map to "aet" (sorted)
    ‚Üí Same equivalence class ‚Üí Group together
```

---

## üìä Pattern Comparison Matrix

| Pattern | Recognition | Time | Space | Difficulty | Use Frequency |
|---------|-------------|------|-------|------------|---------------|
| **1. Frequency** | "Count occurrences", "How many times" | O(n) | O(k) | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 80% |
| **2. Grouping** | "Group by property", "Collect similar" | O(nk) | O(nk) | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 70% |
| **3. Existence** | "Does X exist?", "Have we seen?" | O(n) | O(n) | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 90% |
| **4. Rolling Hash** | "Substring match", "Sliding window" | O(n) | O(1) | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 40% |
| **5. Custom Hash** | Problem-specific structure | Varies | Varies | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 30% |

---

## üéì Learning Progression: Beginner ‚Üí Expert

### Phase 1: Foundations (Weeks 1-2)
**Goal**: Understand hash table mechanics

```
Learning Path:
1. Study how hash tables work internally
   - Array + linked list (chaining)
   - Open addressing (linear probing)
   - Load factor and rehashing

2. Implement hash table from scratch in C/Rust
   - Forces deep understanding
   - Reveals implementation details

3. Solve basic problems:
   ‚úì Two Sum (LeetCode 1)
   ‚úì Contains Duplicate (LeetCode 217)
   ‚úì Valid Anagram (LeetCode 242)

Mental Model Focus:
    "Hash table = Fast lookup structure"
    Time trade-off: O(1) lookup for O(n) space
```

### Phase 2: Pattern Recognition (Weeks 3-4)
**Goal**: Identify when to use hash functions

```
Learning Path:
1. Study Pattern 1 (Frequency)
   - Frequency counter problems
   - Anagram detection
   Practice: 10 problems

2. Study Pattern 2 (Grouping)
   - Group Anagrams
   - Grouping by property
   Practice: 10 problems

3. Study Pattern 3 (Existence)
   - Set operations
   - Deduplication
   Practice: 10 problems

Mental Model Focus:
    "What property makes items equivalent?"
    Design hash key that captures this property
```

### Phase 3: Advanced Techniques (Weeks 5-6)
**Goal**: Master rolling hash and custom patterns

```
Learning Path:
1. Study Pattern 4 (Rolling Hash)
   - Rabin-Karp algorithm
   - Polynomial rolling hash mathematics
   - String matching problems
   Practice: 5 hard problems

2. Study Pattern 5 (Custom Hash)
   - Design domain-specific hash functions
   - Structural hashing for trees
   - Geometric hashing
   Practice: 5 expert problems

Mental Model Focus:
    "How to update hash incrementally?"
    "What mathematical property captures structure?"
```

### Phase 4: Competition Level (Weeks 7-8)
**Goal**: Speed and accuracy under pressure

```
Learning Path:
1. Timed problem solving
   - Set 30-minute timer
   - Solve without hints
   - Analyze mistakes

2. Pattern mixing
   - Problems requiring multiple patterns
   - Complex custom hash designs

3. Code optimization
   - Benchmark different approaches
   - Language-specific optimizations

Mental Model Focus:
    "Fastest path to correct solution"
    "Where can I optimize?"
```

---

## üîç Problem Recognition Guide

### When You See These Keywords ‚Üí Think Hash

| Keyword/Phrase | Likely Pattern | Example Problem |
|----------------|----------------|-----------------|
| "Count occurrences" | Pattern 1: Frequency | First Unique Character |
| "Group by..." | Pattern 2: Grouping | Group Anagrams |
| "Find pair that..." | Pattern 3: Existence | Two Sum |
| "Substring match" | Pattern 4: Rolling Hash | Repeated DNA Sequences |
| "Same structure" | Pattern 5: Custom | Find Duplicate Subtrees |
| "How many times" | Pattern 1: Frequency | Word Pattern |
| "Duplicate" | Pattern 3: Existence | Contains Duplicate |
| "Intersection" | Pattern 3: Existence | Intersection of Arrays |
| "Longest repeating" | Pattern 4: Rolling Hash | Longest Dup Substring |
| "Max points on line" | Pattern 5: Custom | Max Points on a Line |

---

## üí° Core Insights & Principles

### Insight 1: Space-Time Tradeoff
```
Without Hash:
    Time: O(n¬≤) or O(n log n)
    Space: O(1)

With Hash:
    Time: O(n)
    Space: O(n)

Decision Rule:
    Unless space is CRITICALLY limited, choose hash.
    Modern systems: Memory abundant, time precious
```

### Insight 2: Hash Function Design Trinity
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   FAST       ‚îÇ
                    ‚îÇ   O(1)-O(k)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   UNIQUE       ‚îÇ  ‚îÇ  CONSISTENT  ‚îÇ  ‚îÇ  HASHABLE   ‚îÇ
‚îÇ Few collisions ‚îÇ  ‚îÇ  Deterministic‚îÇ  ‚îÇ Language-OK ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

All three required for good hash function
```

### Insight 3: Collision Handling Philosophy
```
Prevention >> Detection >> Resolution

Prevention: Good hash function design
Detection: Verify on hash match
Resolution: Chaining or probing

In interviews: Always mention collision handling strategy
```

### Insight 4: The Canonical Form Pattern
```
Problem: Many representations, one meaning
Solution: Normalize to canonical form

Examples:
    Anagrams ‚Üí Sort characters
    Coordinates ‚Üí (x, y) tuple
    Fractions ‚Üí Reduce (gcd)
    Trees ‚Üí Serialize structure

Power: Converts equivalence problem to equality problem
```

---

## üöÄ Performance Optimization Techniques

### Technique 1: Pre-allocation
```python
# Slow: Hash table resizes multiple times
hash_map = {}
for item in large_list:
    hash_map[item] = value

# Fast: Pre-allocate capacity
hash_map = dict()  # Or use collections.defaultdict
# In C++: map.reserve(size)
# In Rust: HashMap::with_capacity(size)
```

### Technique 2: Array Instead of Hash Map
```python
# When key space is small and dense
# Example: Lowercase English letters

# Slow: Hash map with 26 keys
freq = {}

# Fast: Array with 26 elements
freq = [0] * 26
freq[ord(char) - ord('a')] += 1

# Speedup: 2-3x (no hash computation, cache-friendly)
```

### Technique 3: Rolling Hash vs Rehashing
```python
# Slow: Recompute hash each time (O(k) per iteration)
for i in range(n - m + 1):
    window = text[i:i+m]
    if hash(window) == pattern_hash:
        ...

# Fast: Update incrementally (O(1) per iteration)
window_hash = initial_hash
for i in range(n - m + 1):
    if window_hash == pattern_hash:
        ...
    window_hash = update_incrementally(window_hash)

# Speedup: k times (where k = pattern length)
```

### Technique 4: Choose Right Data Structure
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Need          ‚îÇ   Use        ‚îÇ   Why       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Membership only ‚îÇ   Set        ‚îÇ Less memory ‚îÇ
‚îÇ Count/value     ‚îÇ   Map/Dict   ‚îÇ Store data  ‚îÇ
‚îÇ Ordered keys    ‚îÇ   TreeMap    ‚îÇ Iteration   ‚îÇ
‚îÇ Insertion order ‚îÇ   OrderedDict‚îÇ Preserve    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Practice Roadmap: 50 Essential Problems

### Tier 1: Foundations (Master these first)
```
Pattern 1 - Frequency:
1.  Valid Anagram (LC 242) ‚≠ê
2.  First Unique Character (LC 387) ‚≠ê
3.  Word Pattern (LC 290)

Pattern 2 - Grouping:
4.  Group Anagrams (LC 49) ‚≠ê‚≠ê
5.  Group Shifted Strings (LC 249)
6.  Find All Anagrams (LC 438) ‚≠ê‚≠ê

Pattern 3 - Existence:
7.  Two Sum (LC 1) ‚≠ê
8.  Contains Duplicate (LC 217) ‚≠ê
9.  Intersection of Two Arrays (LC 349)
10. Happy Number (LC 202)
```

### Tier 2: Intermediate (Build confidence)
```
Pattern 1 - Frequency:
11. Top K Frequent Elements (LC 347) ‚≠ê‚≠ê
12. Sort Characters By Frequency (LC 451)
13. Find Duplicate Subtrees (LC 652) ‚≠ê‚≠ê‚≠ê

Pattern 2 - Grouping:
14. Group People By Size (LC 1282)
15. Brick Wall (LC 554)

Pattern 3 - Existence:
16. Longest Consecutive Sequence (LC 128) ‚≠ê‚≠ê‚≠ê
17. Two Sum II (LC 167)
18. 3Sum (LC 15) ‚≠ê‚≠ê
19. 4Sum (LC 18)
20. 4Sum II (LC 454) ‚≠ê‚≠ê
```

### Tier 3: Advanced (Challenge yourself)
```
Pattern 4 - Rolling Hash:
21. Repeated DNA Sequences (LC 187) ‚≠ê‚≠ê
22. Longest Duplicate Substring (LC 1044) ‚≠ê‚≠ê‚≠ê‚≠ê
23. Shortest Palindrome (LC 214) ‚≠ê‚≠ê‚≠ê‚≠ê

Pattern 5 - Custom:
24. Max Points on a Line (LC 149) ‚≠ê‚≠ê‚≠ê‚≠ê
25. Isomorphic Strings (LC 205) ‚≠ê‚≠ê
26. Line Reflection (LC 356)
27. Serialize/Deserialize Binary Tree (LC 297) ‚≠ê‚≠ê‚≠ê
```

### Tier 4: Expert (Top 1% territory)
```
Pattern 4 - Rolling Hash:
28. Distinct Echo Substrings (LC 1316) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
29. Longest Happy Prefix (LC 1392) ‚≠ê‚≠ê‚≠ê‚≠ê

Pattern 5 - Custom:
30. LRU Cache (LC 146) ‚≠ê‚≠ê‚≠ê‚≠ê
31. LFU Cache (LC 460) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
32. Design HashMap (LC 706) ‚≠ê‚≠ê‚≠ê
33. Design HashSet (LC 705) ‚≠ê‚≠ê‚≠ê

Mixed Patterns:
34. Subarray Sum Equals K (LC 560) ‚≠ê‚≠ê‚≠ê
35. Continuous Subarray Sum (LC 523) ‚≠ê‚≠ê‚≠ê
36. Longest Substring Without Repeating (LC 3) ‚≠ê‚≠ê‚≠ê
37. Minimum Window Substring (LC 76) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
38. Sliding Window Maximum (LC 239) ‚≠ê‚≠ê‚≠ê‚≠ê
39. Find All Numbers Disappeared (LC 448) ‚≠ê‚≠ê
40. Find All Duplicates (LC 442) ‚≠ê‚≠ê
```

### Tier 5: Competition (Olympiad level)
```
41. Count of Smaller Numbers After Self (LC 315) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
42. Count of Range Sum (LC 327) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
43. Palindrome Pairs (LC 336) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
44. Substring with Concatenation (LC 30) ‚≠ê‚≠ê‚≠ê‚≠ê
45. Alien Dictionary (LC 269) ‚≠ê‚≠ê‚≠ê‚≠ê
46. Employee Free Time (LC 759) ‚≠ê‚≠ê‚≠ê‚≠ê
47. Design In-Memory File System (LC 588) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
48. Valid Sudoku (LC 36) ‚≠ê‚≠ê
49. Insert Delete GetRandom O(1) (LC 380) ‚≠ê‚≠ê‚≠ê‚≠ê
50. Reconstruct Itinerary (LC 332) ‚≠ê‚≠ê‚≠ê‚≠ê
```

**‚≠ê Rating**: Importance for mastery (1-5 stars)

---

## üßò Deliberate Practice Methodology

### The 4-Step Mastery Cycle

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                  ‚îÇ
‚îÇ  1. UNDERSTAND                                   ‚îÇ
‚îÇ     ‚îú‚îÄ Read problem 3 times                     ‚îÇ
‚îÇ     ‚îú‚îÄ Identify pattern                         ‚îÇ
‚îÇ     ‚îî‚îÄ Visualize examples                       ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  2. PLAN                                         ‚îÇ
‚îÇ     ‚îú‚îÄ Choose approach                          ‚îÇ
‚îÇ     ‚îú‚îÄ Design hash function                     ‚îÇ
‚îÇ     ‚îî‚îÄ Consider edge cases                      ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  3. IMPLEMENT                                    ‚îÇ
‚îÇ     ‚îú‚îÄ Write clean code                         ‚îÇ
‚îÇ     ‚îú‚îÄ Test incrementally                       ‚îÇ
‚îÇ     ‚îî‚îÄ Handle errors                            ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  4. REFLECT                                      ‚îÇ
‚îÇ     ‚îú‚îÄ Analyze complexity                       ‚îÇ
‚îÇ     ‚îú‚îÄ Identify improvements                    ‚îÇ
‚îÇ     ‚îî‚îÄ Extract lessons                          ‚îÇ
‚îÇ                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Daily Practice Schedule

**Beginner (Weeks 1-2)**
- 2 easy problems/day
- 1 hour study + 1 hour practice
- Focus: Pattern recognition

**Intermediate (Weeks 3-4)**
- 1 easy + 1 medium/day
- 2 hours practice
- Focus: Implementation speed

**Advanced (Weeks 5-6)**
- 1 medium + 1 hard/day
- 2-3 hours practice
- Focus: Optimization

**Expert (Weeks 7-8)**
- 1 hard + 1 expert/day
- 3 hours practice
- Focus: Competition problems

---

## üé§ Interview Strategy & Communication

### The STAR Method for Hash Problems

**S**ituation: Understand the problem
```
"Let me clarify the constraints...
- Input size?
- Value range?
- Space limitations?
- Expected frequency of operations?"
```

**T**ask: Identify the pattern
```
"This is a [pattern name] problem because...
I need to [group/count/find] items based on [property]."
```

**A**ction: Design solution
```
"I'll use a hash [map/set] with key being [explanation].
This gives us O(n) time instead of O(n¬≤)."
```

**R**esult: Implement and analyze
```
"Time complexity: O(n)
Space complexity: O(k) where k = [unique elements]
Edge cases handled: [empty input, duplicates, etc.]"
```

### Red Flags to Avoid
‚ùå "I'll just use a hash map" (without explaining why)
‚ùå Not discussing collision handling
‚ùå Ignoring space complexity
‚ùå Not mentioning edge cases

### Green Flags to Show
‚úÖ Clear hash function design rationale
‚úÖ Complexity analysis (time + space)
‚úÖ Discussion of alternatives
‚úÖ Clean, readable code

---

## üß† Cognitive Strategies (Meta-Learning)

### Chunking for Pattern Recognition
```
Level 1: Individual concepts
    - Hash function
    - Collision
    - Load factor

Level 2: Pattern chunks
    - "Frequency counter pattern"
    - "Canonical form pattern"
    - "Rolling hash pattern"

Level 3: Problem-solving chunks
    - "See 'count' ‚Üí think frequency map"
    - "See 'group' ‚Üí think hash key design"
    - "See 'substring' ‚Üí think rolling hash"

Goal: Recognize entire patterns instantly
```

### Spaced Repetition Schedule
```
Day 1:   Learn pattern + solve 3 problems
Day 3:   Review + solve 2 new problems
Day 7:   Review + solve 1 hard problem
Day 14:  Review + solve 1 expert problem
Day 30:  Final review + competition problem

Principle: Increasing intervals, increasing difficulty
```

### Mental Simulation
```
Before coding:
1. Visualize data structure state
2. Step through algorithm mentally
3. Identify potential bugs
4. Estimate complexity

This "pre-coding" phase saves debugging time!
```

---

## üèÜ Mastery Checklist

### Knowledge
- [ ] Understand hash table internals (chaining, probing)
- [ ] Know time/space complexity of all operations
- [ ] Can explain collision handling strategies
- [ ] Understand rolling hash mathematics
- [ ] Know when NOT to use hash functions

### Skills
- [ ] Recognize patterns in < 1 minute
- [ ] Design custom hash functions
- [ ] Implement from scratch in all my languages
- [ ] Debug hash collision issues
- [ ] Optimize for performance

### Experience
- [ ] Solved 50+ hash problems
- [ ] Participated in 5+ coding competitions
- [ ] Taught hash patterns to someone else
- [ ] Debugged hash-related production bug
- [ ] Written hash-heavy system design

---

## üìñ Recommended Resources

### Books
1. **"Introduction to Algorithms" (CLRS)** - Chapter 11 (Hash Tables)
2. **"The Algorithm Design Manual" (Skiena)** - Chapters on hashing
3. **"Competitive Programming" (Halim)** - String algorithms

### Online
1. **LeetCode** - Pattern-based practice
2. **Codeforces** - Competition problems
3. **CP-Algorithms** - Detailed explanations

### Academic Papers
1. **"Rabin-Karp Algorithm"** - Original paper
2. **"Zobrist Hashing"** - Game tree hashing
3. **"Universal Hashing"** - Theoretical foundations

---

## üéØ Final Wisdom

### The Three Laws of Hash Mastery

**Law 1: Pattern Over Implementation**
```
Recognize the pattern quickly.
Implementation is just typing.
```

**Law 2: Simplicity Over Cleverness**
```
The best hash function is the simplest one that works.
Don't over-engineer.
```

**Law 3: Practice Over Perfection**
```
Solve 100 problems imperfectly
> 
Solve 10 problems perfectly
```

---

## üí™ Your Journey Starts Now

You have the knowledge. You have the roadmap. You have the strategies.

Now, it's time for deliberate practice.

**Week 1 Goal**: Master Pattern 1 (Frequency)
**Month 1 Goal**: Solve all Tier 1 & 2 problems
**Month 2 Goal**: Compete in weekly contests
**Month 3 Goal**: Top 1% on LeetCode in hash problems

*"The journey of a thousand problems begins with a single hash function."*

Remember: You're not just learning algorithms. You're training your mind to see patterns, think mathematically, and solve problems elegantly.

Welcome to the top 1%. Let's begin. üöÄ

---

END OF COMPREHENSIVE GUIDE

All supporting documents:
- valid_anagram.md
- group_anagrams.md
- existence_membership_patterns.md
- rolling_hash_patterns.md
- custom_hash_patterns.md

# Advanced Hash Topics & Common Pitfalls
## For the Final 5% of Mastery

---

## üö® Common Pitfalls & How to Avoid Them

### Pitfall 1: Floating Point Hash Keys

**Problem**: Floating point numbers are NOT reliable hash keys!

```python
# WRONG: Floating point keys
hash_map = {}
hash_map[0.1 + 0.2] = "value"
print(hash_map[0.3])  # KeyError! Because 0.1 + 0.2 != 0.3 exactly

# Why: 0.1 + 0.2 = 0.30000000000000004 in binary floating point

# CORRECT: Use tuples of integers
def float_to_fraction(f: float, precision: int = 6) -> tuple:
    """Convert float to (numerator, denominator) tuple"""
    multiplier = 10 ** precision
    numerator = int(f * multiplier)
    from math import gcd
    g = gcd(numerator, multiplier)
    return (numerator // g, multiplier // g)

hash_map[(1, 10)]  # Use fractions instead
```

**Rule**: Never use floats as hash keys. Use:
- Integers (scaled)
- Fractions (as tuples)
- Strings (rounded to precision)

---

### Pitfall 2: Mutable Hash Keys

**Problem**: Mutable objects can't be hash keys (Python lists, sets)

```python
# WRONG
key = [1, 2, 3]
hash_map[key] = "value"  # TypeError: unhashable type: 'list'

# CORRECT: Use immutable types
key = (1, 2, 3)  # Tuple
hash_map[key] = "value"  # Works!

# Or convert to string
key = str([1, 2, 3])
hash_map[key] = "value"
```

**Language-Specific**:
- **Python**: Lists, sets, dicts are unhashable ‚Üí Use tuples, frozensets
- **Rust**: Keys must implement `Hash` trait
- **C++**: Provide custom hash function for complex types
- **Go**: Arrays (not slices) can be map keys

---

### Pitfall 3: Not Handling Hash Collisions

**Problem**: Trusting hash equality without verification

```python
# WRONG: False positives possible
if hash(str1) == hash(str2):
    return True  # Might be collision!

# CORRECT: Always verify
if hash(str1) == hash(str2) and str1 == str2:
    return True  # Now certain

# For rolling hash in string matching
if pattern_hash == window_hash:
    # MUST verify character by character
    if text[i:i+m] == pattern:
        return i
```

**Collision Probability**:
```
Good hash function with output space size M:
- Probability of collision ‚âà n¬≤/(2M)
- For M = 10‚Åπ+7, n = 10‚Å∂: P ‚âà 0.00005
- Still not zero! Always verify.
```

---

### Pitfall 4: Integer Overflow in Hash Computation

**Problem**: Hash computation overflows, gives wrong results

```c++
// WRONG: Overflow before modulo
long long hash = 0;
for (char c : str) {
    hash = hash * BASE + c;  // Can overflow!
}
hash %= PRIME;

// CORRECT: Modulo intermediate steps
long long hash = 0;
for (char c : str) {
    hash = (hash * BASE) % PRIME;
    hash = (hash + c) % PRIME;
}
```

**Language Considerations**:
- **C/C++**: Use `long long`, modulo frequently
- **Python**: Arbitrary precision integers, but slower
- **Rust**: Use `u64`, wrapping arithmetic, or checked operations
- **Go**: `int` size is platform-dependent, use `int64`

---

### Pitfall 5: Negative Modulo Issues

**Problem**: Different languages handle negative modulo differently

```python
# C/C++/Java: (-5) % 3 = -2
# Python: (-5) % 3 = 1

# Problem in rolling hash update:
hash = (hash - old_char * base_power) % PRIME
# Can be negative in C++!

# SOLUTION: Add PRIME before modulo
hash = ((hash - old_char * base_power) % PRIME + PRIME) % PRIME

# Or in C++:
hash = (hash - old_char * base_power % PRIME + PRIME) % PRIME;
```

---

### Pitfall 6: Incorrect Base Power Computation

**Problem**: Not using modular exponentiation

```python
# WRONG: Overflow and slow
base_power = BASE ** (length - 1)
base_power %= PRIME

# CORRECT: Iterative with modulo
base_power = 1
for _ in range(length - 1):
    base_power = (base_power * BASE) % PRIME

# Or use built-in power function with modulo
base_power = pow(BASE, length - 1, PRIME)  # Python's efficient modpow
```

---

### Pitfall 7: Not Pre-sizing Hash Tables

**Problem**: Unnecessary rehashing causes performance degradation

```python
# SLOW: Multiple resizings
hash_map = {}
for i in range(1000000):
    hash_map[i] = i

# FAST: Pre-allocate
hash_map = {}
# In C++: hash_map.reserve(1000000)
# In Rust: HashMap::with_capacity(1000000)
```

**Performance Impact**:
```
Without pre-sizing: ~O(n log n) due to multiple rehashes
With pre-sizing: O(n)

For n = 10‚Å∂: 2-3x speedup
```

---

### Pitfall 8: Using Hash When Array Suffices

**Problem**: Over-engineering with hash when simple array works

```python
# WRONG: Hash map for small, dense key space
freq = {}
for char in string:
    freq[char] = freq.get(char, 0) + 1

# CORRECT: Array for 26 letters
freq = [0] * 26
for char in string:
    freq[ord(char) - ord('a')] += 1

# 2-3x faster, cache-friendly
```

**Rule**: If keys are:
- Small range (< 1000)
- Dense (most values used)
- Integer or convertible to integer

‚Üí Use array, not hash map

---

## üé® Advanced Optimization Techniques

### Technique 1: Double Hashing for Ultra-Low Collisions

```python
class DoubleHash:
    """
    Use two independent hash functions.
    Collision probability: 1/(M‚ÇÅ * M‚ÇÇ)
    """
    PRIME1 = 10**9 + 7
    PRIME2 = 10**9 + 9
    BASE1 = 31
    BASE2 = 37
    
    @staticmethod
    def compute(s: str) -> tuple:
        hash1, hash2 = 0, 0
        for char in s:
            hash1 = (hash1 * DoubleHash.BASE1 + ord(char)) % DoubleHash.PRIME1
            hash2 = (hash2 * DoubleHash.BASE2 + ord(char)) % DoubleHash.PRIME2
        return (hash1, hash2)
    
    @staticmethod
    def rolling_update(old_hash: tuple, old_char: str, new_char: str, 
                       length: int, base_powers: tuple) -> tuple:
        """Update both hashes simultaneously"""
        h1, h2 = old_hash
        bp1, bp2 = base_powers
        
        # Update hash1
        h1 = (h1 - ord(old_char) * bp1) % DoubleHash.PRIME1
        h1 = (h1 * DoubleHash.BASE1 + ord(new_char)) % DoubleHash.PRIME1
        h1 = (h1 + DoubleHash.PRIME1) % DoubleHash.PRIME1
        
        # Update hash2
        h2 = (h2 - ord(old_char) * bp2) % DoubleHash.PRIME2
        h2 = (h2 * DoubleHash.BASE2 + ord(new_char)) % DoubleHash.PRIME2
        h2 = (h2 + DoubleHash.PRIME2) % DoubleHash.PRIME2
        
        return (h1, h2)

# Use case: When collision probability must be < 10‚Åª¬π‚Åµ
```

---

### Technique 2: Polynomial Hashing with Precomputed Powers

```python
class FastRollingHash:
    """
    Precompute all powers for O(1) updates without division
    """
    def __init__(self, text: str, base: int = 31, prime: int = 10**9 + 7):
        self.text = text
        self.base = base
        self.prime = prime
        self.n = len(text)
        
        # Precompute base^i mod prime for all i
        self.powers = [1] * (self.n + 1)
        for i in range(1, self.n + 1):
            self.powers[i] = (self.powers[i-1] * base) % prime
        
        # Precompute prefix hashes
        self.hashes = [0] * (self.n + 1)
        for i in range(self.n):
            self.hashes[i+1] = (self.hashes[i] * base + ord(text[i])) % prime
    
    def get_hash(self, left: int, right: int) -> int:
        """
        Get hash of substring [left, right) in O(1)
        """
        result = (self.hashes[right] - self.hashes[left] * self.powers[right-left]) % self.prime
        return (result + self.prime) % self.prime

# Use case: Multiple substring queries on same text
# Preprocessing: O(n)
# Each query: O(1)
```

---

### Technique 3: Bloom Filter for Space Optimization

**When**: Need to check existence with minimal space

```python
import hashlib

class BloomFilter:
    """
    Probabilistic set membership testing
    Space: Much less than hash set
    False positive rate: Configurable
    False negative rate: 0 (never says "not present" if present)
    """
    def __init__(self, size: int, num_hashes: int = 3):
        self.size = size
        self.num_hashes = num_hashes
        self.bits = [False] * size
    
    def _hash(self, item: str, seed: int) -> int:
        """Multiple hash functions via different seeds"""
        h = hashlib.sha256((item + str(seed)).encode()).hexdigest()
        return int(h, 16) % self.size
    
    def add(self, item: str):
        for i in range(self.num_hashes):
            idx = self._hash(item, i)
            self.bits[idx] = True
    
    def contains(self, item: str) -> bool:
        """Returns True if item MIGHT be present"""
        return all(self.bits[self._hash(item, i)] for i in range(self.num_hashes))

# Use case: Spell checkers, web crawlers, caches
# Example: 1 billion words
#   Regular set: ~10GB
#   Bloom filter (1% false positive): ~1.2GB
```

---

### Technique 4: Cuckoo Hashing for Guaranteed O(1) Lookup

```python
class CuckooHashTable:
    """
    Two hash functions, two tables
    Worst-case O(1) lookup (not average!)
    
    If collision: "kick out" existing element to other table
    """
    def __init__(self, size: int):
        self.size = size
        self.table1 = [None] * size
        self.table2 = [None] * size
        self.max_kicks = 100
    
    def _hash1(self, key: int) -> int:
        return key % self.size
    
    def _hash2(self, key: int) -> int:
        return (key // self.size) % self.size
    
    def insert(self, key: int, value: any):
        for _ in range(self.max_kicks):
            idx1 = self._hash1(key)
            if self.table1[idx1] is None:
                self.table1[idx1] = (key, value)
                return True
            
            # Kick out existing
            self.table1[idx1], (key, value) = (key, value), self.table1[idx1]
            
            idx2 = self._hash2(key)
            if self.table2[idx2] is None:
                self.table2[idx2] = (key, value)
                return True
            
            # Kick out from table2
            self.table2[idx2], (key, value) = (key, value), self.table2[idx2]
        
        return False  # Failed after max_kicks, need rehash
    
    def lookup(self, key: int) -> any:
        """O(1) worst-case lookup"""
        idx1 = self._hash1(key)
        if self.table1[idx1] and self.table1[idx1][0] == key:
            return self.table1[idx1][1]
        
        idx2 = self._hash2(key)
        if self.table2[idx2] and self.table2[idx2][0] == key:
            return self.table2[idx2][1]
        
        return None

# Use case: Real-time systems requiring guaranteed performance
```

---

## üî¨ Advanced Hash Function Designs

### Design 1: Zobrist Hashing (Board Games)

```python
import random

class ZobristHash:
    """
    Extremely fast incremental hashing for board states
    Used in chess engines, Go programs
    """
    def __init__(self, board_size: int, num_pieces: int):
        random.seed(42)
        self.table = {}
        
        for row in range(board_size):
            for col in range(board_size):
                for piece in range(num_pieces):
                    self.table[(row, col, piece)] = random.getrandbits(64)
        
        self.turn_hash = random.getrandbits(64)
    
    def compute_hash(self, board: list) -> int:
        h = 0
        for row in range(len(board)):
            for col in range(len(board[0])):
                if board[row][col] != 0:
                    h ^= self.table[(row, col, board[row][col])]
        return h
    
    def update_move(self, old_hash: int, move: tuple) -> int:
        """
        Update hash in O(1) for a move
        move = (from_row, from_col, to_row, to_col, piece)
        """
        from_r, from_c, to_r, to_c, piece = move
        
        # XOR out old position
        old_hash ^= self.table[(from_r, from_c, piece)]
        # XOR in new position
        old_hash ^= self.table[(to_r, to_c, piece)]
        # Toggle turn
        old_hash ^= self.turn_hash
        
        return old_hash

# Property: XOR is self-inverse
# x ^ y ^ y = x
# Makes incremental updates trivial!
```

---

### Design 2: Geometric Hashing (Computer Vision)

```python
from typing import List, Tuple
import math

class GeometricHash:
    """
    Hash for geometric shapes invariant to rotation, scale, translation
    Used in: Image recognition, shape matching
    """
    def __init__(self, precision: int = 3):
        self.precision = precision
    
    def normalize_shape(self, points: List[Tuple[float, float]]) -> tuple:
        """
        Normalize shape to canonical form:
        1. Center at origin (translation invariant)
        2. Scale to unit circle (scale invariant)
        3. Rotate to align first point with x-axis (rotation invariant)
        """
        # Center
        cx = sum(p[0] for p in points) / len(points)
        cy = sum(p[1] for p in points) / len(points)
        centered = [(x - cx, y - cy) for x, y in points]
        
        # Scale
        max_dist = max(math.sqrt(x**2 + y**2) for x, y in centered)
        if max_dist > 0:
            scaled = [(x/max_dist, y/max_dist) for x, y in centered]
        else:
            scaled = centered
        
        # Rotate to align first point
        if scaled:
            angle = math.atan2(scaled[0][1], scaled[0][0])
            cos_a, sin_a = math.cos(-angle), math.sin(-angle)
            rotated = [
                (x * cos_a - y * sin_a, x * sin_a + y * cos_a)
                for x, y in scaled
            ]
        else:
            rotated = scaled
        
        # Round to precision
        rounded = tuple(
            (round(x, self.precision), round(y, self.precision))
            for x, y in rotated
        )
        
        return rounded
    
    def shapes_match(self, shape1: List[Tuple], shape2: List[Tuple]) -> bool:
        """Check if two shapes are similar"""
        return self.normalize_shape(shape1) == self.normalize_shape(shape2)

# Use case: "Find this shape in an image at any position/rotation/size"
```

---

## üéØ Competition Programming Tricks

### Trick 1: Custom Hash for std::pair in C++

```cpp
// Problem: std::pair not hashable by default in unordered_map

// Solution 1: Custom hash function
struct PairHash {
    template <typename T1, typename T2>
    std::size_t operator()(const std::pair<T1, T2>& p) const {
        auto h1 = std::hash<T1>{}(p.first);
        auto h2 = std::hash<T2>{}(p.second);
        return h1 ^ (h2 << 1);  // Combine hashes
    }
};

// Usage:
std::unordered_map<std::pair<int, int>, int, PairHash> map;

// Solution 2: Convert to long long (if possible)
long long pair_to_ll(int x, int y) {
    return ((long long)x << 32) | (unsigned int)y;
}
// Usage:
std::unordered_map<long long, int> map;
map[pair_to_ll(x, y)] = value;
```

---

### Trick 2: Python frozenset for Unordered Collections

```python
# Problem: Need to hash a set of elements
# Wrong:
key = {1, 2, 3}  # set is unhashable

# Correct:
key = frozenset([1, 2, 3])
hash_map[key] = "value"

# Use case: Graph problems where order doesn't matter
# Example: Hash edges as frozenset({u, v})
```

---

### Trick 3: String Pooling for Memory Optimization

```python
import sys

# Problem: Many duplicate strings use lots of memory
strings = ["hello"] * 1000000  # Each is separate object

# Solution: String interning (Python)
strings = [sys.intern("hello")] * 1000000  # All reference same object

# Memory saved: ~40MB for 1M strings
```

---

## üìä Hash Function Performance Benchmarks

### Benchmark Results (1M operations)

```
Test: Insert and lookup 1M random integers
Hardware: 3.5GHz CPU, 16GB RAM

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Structure        ‚îÇ Insert   ‚îÇ Lookup   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ std::unordered_map      ‚îÇ  250ms   ‚îÇ  180ms   ‚îÇ
‚îÇ Python dict             ‚îÇ  180ms   ‚îÇ  150ms   ‚îÇ
‚îÇ Rust HashMap            ‚îÇ  200ms   ‚îÇ  160ms   ‚îÇ
‚îÇ Go map                  ‚îÇ  220ms   ‚îÇ  170ms   ‚îÇ
‚îÇ                         ‚îÇ          ‚îÇ          ‚îÇ
‚îÇ Array (when applicable) ‚îÇ   80ms   ‚îÇ   50ms   ‚îÇ
‚îÇ std::map (ordered)      ‚îÇ  600ms   ‚îÇ  500ms   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Key Insight: Hash table ~3x faster than ordered map
             Array ~3x faster than hash when applicable
```

---

## üß™ Testing Your Hash Functions

### Test Suite Template

```python
def test_hash_function():
    """Comprehensive test for custom hash functions"""
    
    # Test 1: Determinism
    assert hash_func("test") == hash_func("test")
    
    # Test 2: Uniqueness (for known inputs)
    assert hash_func("abc") != hash_func("abd")
    
    # Test 3: Collision rate
    keys = generate_random_keys(10000)
    hashes = [hash_func(k) for k in keys]
    unique = len(set(hashes))
    collision_rate = 1 - unique / len(hashes)
    assert collision_rate < 0.01  # Less than 1%
    
    # Test 4: Distribution uniformity (Chi-squared test)
    buckets = [0] * 100
    for h in hashes:
        buckets[h % 100] += 1
    
    expected = len(hashes) / 100
    chi_squared = sum((b - expected)**2 / expected for b in buckets)
    # Accept if chi-squared < critical value
    
    # Test 5: Edge cases
    assert hash_func("") is not None  # Empty input
    assert hash_func("a" * 10000) is not None  # Very long input
    
    print("All tests passed!")
```

---

## üéì Final Mastery Exercises

### Exercise 1: Implement Perfect Hashing
Design a two-level hash table with zero collisions for static data.

### Exercise 2: Design Minimal Perfect Hash Function
Create hash function for fixed set of n keys producing values 0 to n-1 with no gaps or collisions.

### Exercise 3: Optimize for Cache Locality
Modify hash table to be cache-friendly (linear probing vs chaining).

### Exercise 4: Implement Concurrent Hash Map
Thread-safe hash table with fine-grained locking.

### Exercise 5: Create Custom Hash for Your Domain
Design specialized hash for specific data (genomic sequences, IP addresses, etc.).

---

## üèÜ Beyond Mastery: Research Topics

1. **Consistent Hashing** - Distributed systems
2. **Locality-Sensitive Hashing** - Similarity search
3. **MinHash** - Set similarity estimation
4. **SimHash** - Near-duplicate detection
5. **Cryptographic Hashing** - Security applications

These are PhD-level topics but worth exploring for complete mastery!

---

END OF ADVANCED TOPICS

You now have everything needed for the top 1% of hash function expertise.

# Complete Hash Function Patterns Mastery Guide
## Your Path to the Top 1%

---

## üéØ Learning Philosophy

*"A hash function is not just a tool‚Äîit's a way of thinking about equivalence, identity, and computational efficiency."*

This guide is structured for **deliberate practice**: understanding fundamentals deeply, recognizing patterns quickly, and solving problems elegantly.

---

## üìö Complete Pattern Taxonomy

```
HASH FUNCTION PATTERNS HIERARCHY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CORE CONCEPT                            ‚îÇ
‚îÇ   Map infinite input space ‚Üí finite output space          ‚îÇ
‚îÇ   Property: Same input ‚Üí Same output (deterministic)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PATTERN 1    ‚îÇ  ‚îÇ   PATTERN 2    ‚îÇ  ‚îÇ  PATTERN 3  ‚îÇ
‚îÇ   Frequency/   ‚îÇ  ‚îÇ   Grouping/    ‚îÇ  ‚îÇ  Existence/ ‚îÇ
‚îÇ   Counting     ‚îÇ  ‚îÇ   Bucketing    ‚îÇ  ‚îÇ  Membership ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ   PATTERN 4    ‚îÇ  ‚îÇ   PATTERN 5    ‚îÇ         ‚îÇ
‚îÇ   Rolling Hash ‚îÇ  ‚îÇ   Custom Hash  ‚îÇ         ‚îÇ
‚îÇ   (Advanced)   ‚îÇ  ‚îÇ   (Creative)   ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
                                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  APPLICATIONS  ‚îÇ
            ‚îÇ  ‚Ä¢ Strings     ‚îÇ
            ‚îÇ  ‚Ä¢ Arrays      ‚îÇ
            ‚îÇ  ‚Ä¢ Trees       ‚îÇ
            ‚îÇ  ‚Ä¢ Geometry    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß† Mental Models: The Foundation

### Model 1: The Fingerprint Analogy
```
Just as fingerprints uniquely identify people:
    Hash functions create "fingerprints" for data

Properties:
    ‚úì Quick to generate (O(1) or O(k))
    ‚úì Unique enough (minimize collisions)
    ‚úì Consistent (same input ‚Üí same fingerprint)
```

### Model 2: The Bucket Sorting Analogy
```
Hash table = Array of buckets
Hash function = Determines which bucket

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Items: [apple, banana, cherry]     ‚îÇ
‚îÇ                ‚îÇ                    ‚îÇ
‚îÇ         Hash Function               ‚îÇ
‚îÇ                ‚îÇ                    ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ     ‚ñº          ‚ñº          ‚ñº         ‚îÇ
‚îÇ  Bucket 0   Bucket 1   Bucket 2     ‚îÇ
‚îÇ  [apple]    [banana]   [cherry]     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Why it's fast: O(1) access to correct bucket
```

### Model 3: The Equivalence Class Lens
```
Problem: Group similar items

Traditional approach:
    Compare every pair ‚Üí O(n¬≤)

Hash approach:
    Map to equivalence class ‚Üí O(n)

Example - Anagrams:
    "eat", "tea", "ate" ‚Üí ALL map to "aet" (sorted)
    ‚Üí Same equivalence class ‚Üí Group together
```

---

## üìä Pattern Comparison Matrix

| Pattern | Recognition | Time | Space | Difficulty | Use Frequency |
|---------|-------------|------|-------|------------|---------------|
| **1. Frequency** | "Count occurrences", "How many times" | O(n) | O(k) | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 80% |
| **2. Grouping** | "Group by property", "Collect similar" | O(nk) | O(nk) | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 70% |
| **3. Existence** | "Does X exist?", "Have we seen?" | O(n) | O(n) | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 90% |
| **4. Rolling Hash** | "Substring match", "Sliding window" | O(n) | O(1) | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 40% |
| **5. Custom Hash** | Problem-specific structure | Varies | Varies | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 30% |

---

## üéì Learning Progression: Beginner ‚Üí Expert

### Phase 1: Foundations (Weeks 1-2)
**Goal**: Understand hash table mechanics

```
Learning Path:
1. Study how hash tables work internally
   - Array + linked list (chaining)
   - Open addressing (linear probing)
   - Load factor and rehashing

2. Implement hash table from scratch in C/Rust
   - Forces deep understanding
   - Reveals implementation details

3. Solve basic problems:
   ‚úì Two Sum (LeetCode 1)
   ‚úì Contains Duplicate (LeetCode 217)
   ‚úì Valid Anagram (LeetCode 242)

Mental Model Focus:
    "Hash table = Fast lookup structure"
    Time trade-off: O(1) lookup for O(n) space
```

### Phase 2: Pattern Recognition (Weeks 3-4)
**Goal**: Identify when to use hash functions

```
Learning Path:
1. Study Pattern 1 (Frequency)
   - Frequency counter problems
   - Anagram detection
   Practice: 10 problems

2. Study Pattern 2 (Grouping)
   - Group Anagrams
   - Grouping by property
   Practice: 10 problems

3. Study Pattern 3 (Existence)
   - Set operations
   - Deduplication
   Practice: 10 problems

Mental Model Focus:
    "What property makes items equivalent?"
    Design hash key that captures this property
```

### Phase 3: Advanced Techniques (Weeks 5-6)
**Goal**: Master rolling hash and custom patterns

```
Learning Path:
1. Study Pattern 4 (Rolling Hash)
   - Rabin-Karp algorithm
   - Polynomial rolling hash mathematics
   - String matching problems
   Practice: 5 hard problems

2. Study Pattern 5 (Custom Hash)
   - Design domain-specific hash functions
   - Structural hashing for trees
   - Geometric hashing
   Practice: 5 expert problems

Mental Model Focus:
    "How to update hash incrementally?"
    "What mathematical property captures structure?"
```

### Phase 4: Competition Level (Weeks 7-8)
**Goal**: Speed and accuracy under pressure

```
Learning Path:
1. Timed problem solving
   - Set 30-minute timer
   - Solve without hints
   - Analyze mistakes

2. Pattern mixing
   - Problems requiring multiple patterns
   - Complex custom hash designs

3. Code optimization
   - Benchmark different approaches
   - Language-specific optimizations

Mental Model Focus:
    "Fastest path to correct solution"
    "Where can I optimize?"
```

---

## üîç Problem Recognition Guide

### When You See These Keywords ‚Üí Think Hash

| Keyword/Phrase | Likely Pattern | Example Problem |
|----------------|----------------|-----------------|
| "Count occurrences" | Pattern 1: Frequency | First Unique Character |
| "Group by..." | Pattern 2: Grouping | Group Anagrams |
| "Find pair that..." | Pattern 3: Existence | Two Sum |
| "Substring match" | Pattern 4: Rolling Hash | Repeated DNA Sequences |
| "Same structure" | Pattern 5: Custom | Find Duplicate Subtrees |
| "How many times" | Pattern 1: Frequency | Word Pattern |
| "Duplicate" | Pattern 3: Existence | Contains Duplicate |
| "Intersection" | Pattern 3: Existence | Intersection of Arrays |
| "Longest repeating" | Pattern 4: Rolling Hash | Longest Dup Substring |
| "Max points on line" | Pattern 5: Custom | Max Points on a Line |

---

## üí° Core Insights & Principles

### Insight 1: Space-Time Tradeoff
```
Without Hash:
    Time: O(n¬≤) or O(n log n)
    Space: O(1)

With Hash:
    Time: O(n)
    Space: O(n)

Decision Rule:
    Unless space is CRITICALLY limited, choose hash.
    Modern systems: Memory abundant, time precious
```

### Insight 2: Hash Function Design Trinity
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   FAST       ‚îÇ
                    ‚îÇ   O(1)-O(k)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   UNIQUE       ‚îÇ  ‚îÇ  CONSISTENT  ‚îÇ  ‚îÇ  HASHABLE   ‚îÇ
‚îÇ Few collisions ‚îÇ  ‚îÇ  Deterministic‚îÇ  ‚îÇ Language-OK ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

All three required for good hash function
```

### Insight 3: Collision Handling Philosophy
```
Prevention >> Detection >> Resolution

Prevention: Good hash function design
Detection: Verify on hash match
Resolution: Chaining or probing

In interviews: Always mention collision handling strategy
```

### Insight 4: The Canonical Form Pattern
```
Problem: Many representations, one meaning
Solution: Normalize to canonical form

Examples:
    Anagrams ‚Üí Sort characters
    Coordinates ‚Üí (x, y) tuple
    Fractions ‚Üí Reduce (gcd)
    Trees ‚Üí Serialize structure

Power: Converts equivalence problem to equality problem
```

---

## üöÄ Performance Optimization Techniques

### Technique 1: Pre-allocation
```python
# Slow: Hash table resizes multiple times
hash_map = {}
for item in large_list:
    hash_map[item] = value

# Fast: Pre-allocate capacity
hash_map = dict()  # Or use collections.defaultdict
# In C++: map.reserve(size)
# In Rust: HashMap::with_capacity(size)
```

### Technique 2: Array Instead of Hash Map
```python
# When key space is small and dense
# Example: Lowercase English letters

# Slow: Hash map with 26 keys
freq = {}

# Fast: Array with 26 elements
freq = [0] * 26
freq[ord(char) - ord('a')] += 1

# Speedup: 2-3x (no hash computation, cache-friendly)
```

### Technique 3: Rolling Hash vs Rehashing
```python
# Slow: Recompute hash each time (O(k) per iteration)
for i in range(n - m + 1):
    window = text[i:i+m]
    if hash(window) == pattern_hash:
        ...

# Fast: Update incrementally (O(1) per iteration)
window_hash = initial_hash
for i in range(n - m + 1):
    if window_hash == pattern_hash:
        ...
    window_hash = update_incrementally(window_hash)

# Speedup: k times (where k = pattern length)
```

### Technique 4: Choose Right Data Structure
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Need          ‚îÇ   Use        ‚îÇ   Why       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Membership only ‚îÇ   Set        ‚îÇ Less memory ‚îÇ
‚îÇ Count/value     ‚îÇ   Map/Dict   ‚îÇ Store data  ‚îÇ
‚îÇ Ordered keys    ‚îÇ   TreeMap    ‚îÇ Iteration   ‚îÇ
‚îÇ Insertion order ‚îÇ   OrderedDict‚îÇ Preserve    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Practice Roadmap: 50 Essential Problems

### Tier 1: Foundations (Master these first)
```
Pattern 1 - Frequency:
1.  Valid Anagram (LC 242) ‚≠ê
2.  First Unique Character (LC 387) ‚≠ê
3.  Word Pattern (LC 290)

Pattern 2 - Grouping:
4.  Group Anagrams (LC 49) ‚≠ê‚≠ê
5.  Group Shifted Strings (LC 249)
6.  Find All Anagrams (LC 438) ‚≠ê‚≠ê

Pattern 3 - Existence:
7.  Two Sum (LC 1) ‚≠ê
8.  Contains Duplicate (LC 217) ‚≠ê
9.  Intersection of Two Arrays (LC 349)
10. Happy Number (LC 202)
```

### Tier 2: Intermediate (Build confidence)
```
Pattern 1 - Frequency:
11. Top K Frequent Elements (LC 347) ‚≠ê‚≠ê
12. Sort Characters By Frequency (LC 451)
13. Find Duplicate Subtrees (LC 652) ‚≠ê‚≠ê‚≠ê

Pattern 2 - Grouping:
14. Group People By Size (LC 1282)
15. Brick Wall (LC 554)

Pattern 3 - Existence:
16. Longest Consecutive Sequence (LC 128) ‚≠ê‚≠ê‚≠ê
17. Two Sum II (LC 167)
18. 3Sum (LC 15) ‚≠ê‚≠ê
19. 4Sum (LC 18)
20. 4Sum II (LC 454) ‚≠ê‚≠ê
```

### Tier 3: Advanced (Challenge yourself)
```
Pattern 4 - Rolling Hash:
21. Repeated DNA Sequences (LC 187) ‚≠ê‚≠ê
22. Longest Duplicate Substring (LC 1044) ‚≠ê‚≠ê‚≠ê‚≠ê
23. Shortest Palindrome (LC 214) ‚≠ê‚≠ê‚≠ê‚≠ê

Pattern 5 - Custom:
24. Max Points on a Line (LC 149) ‚≠ê‚≠ê‚≠ê‚≠ê
25. Isomorphic Strings (LC 205) ‚≠ê‚≠ê
26. Line Reflection (LC 356)
27. Serialize/Deserialize Binary Tree (LC 297) ‚≠ê‚≠ê‚≠ê
```

### Tier 4: Expert (Top 1% territory)
```
Pattern 4 - Rolling Hash:
28. Distinct Echo Substrings (LC 1316) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
29. Longest Happy Prefix (LC 1392) ‚≠ê‚≠ê‚≠ê‚≠ê

Pattern 5 - Custom:
30. LRU Cache (LC 146) ‚≠ê‚≠ê‚≠ê‚≠ê
31. LFU Cache (LC 460) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
32. Design HashMap (LC 706) ‚≠ê‚≠ê‚≠ê
33. Design HashSet (LC 705) ‚≠ê‚≠ê‚≠ê

Mixed Patterns:
34. Subarray Sum Equals K (LC 560) ‚≠ê‚≠ê‚≠ê
35. Continuous Subarray Sum (LC 523) ‚≠ê‚≠ê‚≠ê
36. Longest Substring Without Repeating (LC 3) ‚≠ê‚≠ê‚≠ê
37. Minimum Window Substring (LC 76) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
38. Sliding Window Maximum (LC 239) ‚≠ê‚≠ê‚≠ê‚≠ê
39. Find All Numbers Disappeared (LC 448) ‚≠ê‚≠ê
40. Find All Duplicates (LC 442) ‚≠ê‚≠ê
```

### Tier 5: Competition (Olympiad level)
```
41. Count of Smaller Numbers After Self (LC 315) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
42. Count of Range Sum (LC 327) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
43. Palindrome Pairs (LC 336) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
44. Substring with Concatenation (LC 30) ‚≠ê‚≠ê‚≠ê‚≠ê
45. Alien Dictionary (LC 269) ‚≠ê‚≠ê‚≠ê‚≠ê
46. Employee Free Time (LC 759) ‚≠ê‚≠ê‚≠ê‚≠ê
47. Design In-Memory File System (LC 588) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
48. Valid Sudoku (LC 36) ‚≠ê‚≠ê
49. Insert Delete GetRandom O(1) (LC 380) ‚≠ê‚≠ê‚≠ê‚≠ê
50. Reconstruct Itinerary (LC 332) ‚≠ê‚≠ê‚≠ê‚≠ê
```

**‚≠ê Rating**: Importance for mastery (1-5 stars)

---

## üßò Deliberate Practice Methodology

### The 4-Step Mastery Cycle

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                  ‚îÇ
‚îÇ  1. UNDERSTAND                                   ‚îÇ
‚îÇ     ‚îú‚îÄ Read problem 3 times                     ‚îÇ
‚îÇ     ‚îú‚îÄ Identify pattern                         ‚îÇ
‚îÇ     ‚îî‚îÄ Visualize examples                       ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  2. PLAN                                         ‚îÇ
‚îÇ     ‚îú‚îÄ Choose approach                          ‚îÇ
‚îÇ     ‚îú‚îÄ Design hash function                     ‚îÇ
‚îÇ     ‚îî‚îÄ Consider edge cases                      ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  3. IMPLEMENT                                    ‚îÇ
‚îÇ     ‚îú‚îÄ Write clean code                         ‚îÇ
‚îÇ     ‚îú‚îÄ Test incrementally                       ‚îÇ
‚îÇ     ‚îî‚îÄ Handle errors                            ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  4. REFLECT                                      ‚îÇ
‚îÇ     ‚îú‚îÄ Analyze complexity                       ‚îÇ
‚îÇ     ‚îú‚îÄ Identify improvements                    ‚îÇ
‚îÇ     ‚îî‚îÄ Extract lessons                          ‚îÇ
‚îÇ                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Daily Practice Schedule

**Beginner (Weeks 1-2)**
- 2 easy problems/day
- 1 hour study + 1 hour practice
- Focus: Pattern recognition

**Intermediate (Weeks 3-4)**
- 1 easy + 1 medium/day
- 2 hours practice
- Focus: Implementation speed

**Advanced (Weeks 5-6)**
- 1 medium + 1 hard/day
- 2-3 hours practice
- Focus: Optimization

**Expert (Weeks 7-8)**
- 1 hard + 1 expert/day
- 3 hours practice
- Focus: Competition problems

---

## üé§ Interview Strategy & Communication

### The STAR Method for Hash Problems

**S**ituation: Understand the problem
```
"Let me clarify the constraints...
- Input size?
- Value range?
- Space limitations?
- Expected frequency of operations?"
```

**T**ask: Identify the pattern
```
"This is a [pattern name] problem because...
I need to [group/count/find] items based on [property]."
```

**A**ction: Design solution
```
"I'll use a hash [map/set] with key being [explanation].
This gives us O(n) time instead of O(n¬≤)."
```

**R**esult: Implement and analyze
```
"Time complexity: O(n)
Space complexity: O(k) where k = [unique elements]
Edge cases handled: [empty input, duplicates, etc.]"
```

### Red Flags to Avoid
‚ùå "I'll just use a hash map" (without explaining why)
‚ùå Not discussing collision handling
‚ùå Ignoring space complexity
‚ùå Not mentioning edge cases

### Green Flags to Show
‚úÖ Clear hash function design rationale
‚úÖ Complexity analysis (time + space)
‚úÖ Discussion of alternatives
‚úÖ Clean, readable code

---

## üß† Cognitive Strategies (Meta-Learning)

### Chunking for Pattern Recognition
```
Level 1: Individual concepts
    - Hash function
    - Collision
    - Load factor

Level 2: Pattern chunks
    - "Frequency counter pattern"
    - "Canonical form pattern"
    - "Rolling hash pattern"

Level 3: Problem-solving chunks
    - "See 'count' ‚Üí think frequency map"
    - "See 'group' ‚Üí think hash key design"
    - "See 'substring' ‚Üí think rolling hash"

Goal: Recognize entire patterns instantly
```

### Spaced Repetition Schedule
```
Day 1:   Learn pattern + solve 3 problems
Day 3:   Review + solve 2 new problems
Day 7:   Review + solve 1 hard problem
Day 14:  Review + solve 1 expert problem
Day 30:  Final review + competition problem

Principle: Increasing intervals, increasing difficulty
```

### Mental Simulation
```
Before coding:
1. Visualize data structure state
2. Step through algorithm mentally
3. Identify potential bugs
4. Estimate complexity

This "pre-coding" phase saves debugging time!
```

---

## üèÜ Mastery Checklist

### Knowledge
- [ ] Understand hash table internals (chaining, probing)
- [ ] Know time/space complexity of all operations
- [ ] Can explain collision handling strategies
- [ ] Understand rolling hash mathematics
- [ ] Know when NOT to use hash functions

### Skills
- [ ] Recognize patterns in < 1 minute
- [ ] Design custom hash functions
- [ ] Implement from scratch in all my languages
- [ ] Debug hash collision issues
- [ ] Optimize for performance

### Experience
- [ ] Solved 50+ hash problems
- [ ] Participated in 5+ coding competitions
- [ ] Taught hash patterns to someone else
- [ ] Debugged hash-related production bug
- [ ] Written hash-heavy system design

---

## üìñ Recommended Resources

### Books
1. **"Introduction to Algorithms" (CLRS)** - Chapter 11 (Hash Tables)
2. **"The Algorithm Design Manual" (Skiena)** - Chapters on hashing
3. **"Competitive Programming" (Halim)** - String algorithms

### Online
1. **LeetCode** - Pattern-based practice
2. **Codeforces** - Competition problems
3. **CP-Algorithms** - Detailed explanations

### Academic Papers
1. **"Rabin-Karp Algorithm"** - Original paper
2. **"Zobrist Hashing"** - Game tree hashing
3. **"Universal Hashing"** - Theoretical foundations

---

## üéØ Final Wisdom

### The Three Laws of Hash Mastery

**Law 1: Pattern Over Implementation**
```
Recognize the pattern quickly.
Implementation is just typing.
```

**Law 2: Simplicity Over Cleverness**
```
The best hash function is the simplest one that works.
Don't over-engineer.
```

**Law 3: Practice Over Perfection**
```
Solve 100 problems imperfectly
> 
Solve 10 problems perfectly
```

---

## üí™ Your Journey Starts Now

You have the knowledge. You have the roadmap. You have the strategies.

Now, it's time for deliberate practice.

**Week 1 Goal**: Master Pattern 1 (Frequency)
**Month 1 Goal**: Solve all Tier 1 & 2 problems
**Month 2 Goal**: Compete in weekly contests
**Month 3 Goal**: Top 1% on LeetCode in hash problems

*"The journey of a thousand problems begins with a single hash function."*

Remember: You're not just learning algorithms. You're training your mind to see patterns, think mathematically, and solve problems elegantly.

Welcome to the top 1%. Let's begin. üöÄ

---

END OF COMPREHENSIVE GUIDE

All supporting documents:
- valid_anagram.md
- group_anagrams.md
- existence_membership_patterns.md
- rolling_hash_patterns.md
- custom_hash_patterns.md

# Complete Hash Function Patterns Mastery Guide
## Your Path to the Top 1%

---

## üéØ Learning Philosophy

*"A hash function is not just a tool‚Äîit's a way of thinking about equivalence, identity, and computational efficiency."*

This guide is structured for **deliberate practice**: understanding fundamentals deeply, recognizing patterns quickly, and solving problems elegantly.

---

## üìö Complete Pattern Taxonomy

```
HASH FUNCTION PATTERNS HIERARCHY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CORE CONCEPT                            ‚îÇ
‚îÇ   Map infinite input space ‚Üí finite output space          ‚îÇ
‚îÇ   Property: Same input ‚Üí Same output (deterministic)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PATTERN 1    ‚îÇ  ‚îÇ   PATTERN 2    ‚îÇ  ‚îÇ  PATTERN 3  ‚îÇ
‚îÇ   Frequency/   ‚îÇ  ‚îÇ   Grouping/    ‚îÇ  ‚îÇ  Existence/ ‚îÇ
‚îÇ   Counting     ‚îÇ  ‚îÇ   Bucketing    ‚îÇ  ‚îÇ  Membership ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ   PATTERN 4    ‚îÇ  ‚îÇ   PATTERN 5    ‚îÇ         ‚îÇ
‚îÇ   Rolling Hash ‚îÇ  ‚îÇ   Custom Hash  ‚îÇ         ‚îÇ
‚îÇ   (Advanced)   ‚îÇ  ‚îÇ   (Creative)   ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
                                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  APPLICATIONS  ‚îÇ
            ‚îÇ  ‚Ä¢ Strings     ‚îÇ
            ‚îÇ  ‚Ä¢ Arrays      ‚îÇ
            ‚îÇ  ‚Ä¢ Trees       ‚îÇ
            ‚îÇ  ‚Ä¢ Geometry    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß† Mental Models: The Foundation

### Model 1: The Fingerprint Analogy
```
Just as fingerprints uniquely identify people:
    Hash functions create "fingerprints" for data

Properties:
    ‚úì Quick to generate (O(1) or O(k))
    ‚úì Unique enough (minimize collisions)
    ‚úì Consistent (same input ‚Üí same fingerprint)
```

### Model 2: The Bucket Sorting Analogy
```
Hash table = Array of buckets
Hash function = Determines which bucket

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Items: [apple, banana, cherry]     ‚îÇ
‚îÇ                ‚îÇ                    ‚îÇ
‚îÇ         Hash Function               ‚îÇ
‚îÇ                ‚îÇ                    ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ     ‚ñº          ‚ñº          ‚ñº         ‚îÇ
‚îÇ  Bucket 0   Bucket 1   Bucket 2     ‚îÇ
‚îÇ  [apple]    [banana]   [cherry]     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Why it's fast: O(1) access to correct bucket
```

### Model 3: The Equivalence Class Lens
```
Problem: Group similar items

Traditional approach:
    Compare every pair ‚Üí O(n¬≤)

Hash approach:
    Map to equivalence class ‚Üí O(n)

Example - Anagrams:
    "eat", "tea", "ate" ‚Üí ALL map to "aet" (sorted)
    ‚Üí Same equivalence class ‚Üí Group together
```

---

## üìä Pattern Comparison Matrix

| Pattern | Recognition | Time | Space | Difficulty | Use Frequency |
|---------|-------------|------|-------|------------|---------------|
| **1. Frequency** | "Count occurrences", "How many times" | O(n) | O(k) | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 80% |
| **2. Grouping** | "Group by property", "Collect similar" | O(nk) | O(nk) | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 70% |
| **3. Existence** | "Does X exist?", "Have we seen?" | O(n) | O(n) | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 90% |
| **4. Rolling Hash** | "Substring match", "Sliding window" | O(n) | O(1) | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 40% |
| **5. Custom Hash** | Problem-specific structure | Varies | Varies | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 30% |

---

## üéì Learning Progression: Beginner ‚Üí Expert

### Phase 1: Foundations (Weeks 1-2)
**Goal**: Understand hash table mechanics

```
Learning Path:
1. Study how hash tables work internally
   - Array + linked list (chaining)
   - Open addressing (linear probing)
   - Load factor and rehashing

2. Implement hash table from scratch in C/Rust
   - Forces deep understanding
   - Reveals implementation details

3. Solve basic problems:
   ‚úì Two Sum (LeetCode 1)
   ‚úì Contains Duplicate (LeetCode 217)
   ‚úì Valid Anagram (LeetCode 242)

Mental Model Focus:
    "Hash table = Fast lookup structure"
    Time trade-off: O(1) lookup for O(n) space
```

### Phase 2: Pattern Recognition (Weeks 3-4)
**Goal**: Identify when to use hash functions

```
Learning Path:
1. Study Pattern 1 (Frequency)
   - Frequency counter problems
   - Anagram detection
   Practice: 10 problems

2. Study Pattern 2 (Grouping)
   - Group Anagrams
   - Grouping by property
   Practice: 10 problems

3. Study Pattern 3 (Existence)
   - Set operations
   - Deduplication
   Practice: 10 problems

Mental Model Focus:
    "What property makes items equivalent?"
    Design hash key that captures this property
```

### Phase 3: Advanced Techniques (Weeks 5-6)
**Goal**: Master rolling hash and custom patterns

```
Learning Path:
1. Study Pattern 4 (Rolling Hash)
   - Rabin-Karp algorithm
   - Polynomial rolling hash mathematics
   - String matching problems
   Practice: 5 hard problems

2. Study Pattern 5 (Custom Hash)
   - Design domain-specific hash functions
   - Structural hashing for trees
   - Geometric hashing
   Practice: 5 expert problems

Mental Model Focus:
    "How to update hash incrementally?"
    "What mathematical property captures structure?"
```

### Phase 4: Competition Level (Weeks 7-8)
**Goal**: Speed and accuracy under pressure

```
Learning Path:
1. Timed problem solving
   - Set 30-minute timer
   - Solve without hints
   - Analyze mistakes

2. Pattern mixing
   - Problems requiring multiple patterns
   - Complex custom hash designs

3. Code optimization
   - Benchmark different approaches
   - Language-specific optimizations

Mental Model Focus:
    "Fastest path to correct solution"
    "Where can I optimize?"
```

---

## üîç Problem Recognition Guide

### When You See These Keywords ‚Üí Think Hash

| Keyword/Phrase | Likely Pattern | Example Problem |
|----------------|----------------|-----------------|
| "Count occurrences" | Pattern 1: Frequency | First Unique Character |
| "Group by..." | Pattern 2: Grouping | Group Anagrams |
| "Find pair that..." | Pattern 3: Existence | Two Sum |
| "Substring match" | Pattern 4: Rolling Hash | Repeated DNA Sequences |
| "Same structure" | Pattern 5: Custom | Find Duplicate Subtrees |
| "How many times" | Pattern 1: Frequency | Word Pattern |
| "Duplicate" | Pattern 3: Existence | Contains Duplicate |
| "Intersection" | Pattern 3: Existence | Intersection of Arrays |
| "Longest repeating" | Pattern 4: Rolling Hash | Longest Dup Substring |
| "Max points on line" | Pattern 5: Custom | Max Points on a Line |

---

## üí° Core Insights & Principles

### Insight 1: Space-Time Tradeoff
```
Without Hash:
    Time: O(n¬≤) or O(n log n)
    Space: O(1)

With Hash:
    Time: O(n)
    Space: O(n)

Decision Rule:
    Unless space is CRITICALLY limited, choose hash.
    Modern systems: Memory abundant, time precious
```

### Insight 2: Hash Function Design Trinity
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   FAST       ‚îÇ
                    ‚îÇ   O(1)-O(k)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   UNIQUE       ‚îÇ  ‚îÇ  CONSISTENT  ‚îÇ  ‚îÇ  HASHABLE   ‚îÇ
‚îÇ Few collisions ‚îÇ  ‚îÇ  Deterministic‚îÇ  ‚îÇ Language-OK ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

All three required for good hash function
```

### Insight 3: Collision Handling Philosophy
```
Prevention >> Detection >> Resolution

Prevention: Good hash function design
Detection: Verify on hash match
Resolution: Chaining or probing

In interviews: Always mention collision handling strategy
```

### Insight 4: The Canonical Form Pattern
```
Problem: Many representations, one meaning
Solution: Normalize to canonical form

Examples:
    Anagrams ‚Üí Sort characters
    Coordinates ‚Üí (x, y) tuple
    Fractions ‚Üí Reduce (gcd)
    Trees ‚Üí Serialize structure

Power: Converts equivalence problem to equality problem
```

---

## üöÄ Performance Optimization Techniques

### Technique 1: Pre-allocation
```python
# Slow: Hash table resizes multiple times
hash_map = {}
for item in large_list:
    hash_map[item] = value

# Fast: Pre-allocate capacity
hash_map = dict()  # Or use collections.defaultdict
# In C++: map.reserve(size)
# In Rust: HashMap::with_capacity(size)
```

### Technique 2: Array Instead of Hash Map
```python
# When key space is small and dense
# Example: Lowercase English letters

# Slow: Hash map with 26 keys
freq = {}

# Fast: Array with 26 elements
freq = [0] * 26
freq[ord(char) - ord('a')] += 1

# Speedup: 2-3x (no hash computation, cache-friendly)
```

### Technique 3: Rolling Hash vs Rehashing
```python
# Slow: Recompute hash each time (O(k) per iteration)
for i in range(n - m + 1):
    window = text[i:i+m]
    if hash(window) == pattern_hash:
        ...

# Fast: Update incrementally (O(1) per iteration)
window_hash = initial_hash
for i in range(n - m + 1):
    if window_hash == pattern_hash:
        ...
    window_hash = update_incrementally(window_hash)

# Speedup: k times (where k = pattern length)
```

### Technique 4: Choose Right Data Structure
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Need          ‚îÇ   Use        ‚îÇ   Why       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Membership only ‚îÇ   Set        ‚îÇ Less memory ‚îÇ
‚îÇ Count/value     ‚îÇ   Map/Dict   ‚îÇ Store data  ‚îÇ
‚îÇ Ordered keys    ‚îÇ   TreeMap    ‚îÇ Iteration   ‚îÇ
‚îÇ Insertion order ‚îÇ   OrderedDict‚îÇ Preserve    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Practice Roadmap: 50 Essential Problems

### Tier 1: Foundations (Master these first)
```
Pattern 1 - Frequency:
1.  Valid Anagram (LC 242) ‚≠ê
2.  First Unique Character (LC 387) ‚≠ê
3.  Word Pattern (LC 290)

Pattern 2 - Grouping:
4.  Group Anagrams (LC 49) ‚≠ê‚≠ê
5.  Group Shifted Strings (LC 249)
6.  Find All Anagrams (LC 438) ‚≠ê‚≠ê

Pattern 3 - Existence:
7.  Two Sum (LC 1) ‚≠ê
8.  Contains Duplicate (LC 217) ‚≠ê
9.  Intersection of Two Arrays (LC 349)
10. Happy Number (LC 202)
```

### Tier 2: Intermediate (Build confidence)
```
Pattern 1 - Frequency:
11. Top K Frequent Elements (LC 347) ‚≠ê‚≠ê
12. Sort Characters By Frequency (LC 451)
13. Find Duplicate Subtrees (LC 652) ‚≠ê‚≠ê‚≠ê

Pattern 2 - Grouping:
14. Group People By Size (LC 1282)
15. Brick Wall (LC 554)

Pattern 3 - Existence:
16. Longest Consecutive Sequence (LC 128) ‚≠ê‚≠ê‚≠ê
17. Two Sum II (LC 167)
18. 3Sum (LC 15) ‚≠ê‚≠ê
19. 4Sum (LC 18)
20. 4Sum II (LC 454) ‚≠ê‚≠ê
```

### Tier 3: Advanced (Challenge yourself)
```
Pattern 4 - Rolling Hash:
21. Repeated DNA Sequences (LC 187) ‚≠ê‚≠ê
22. Longest Duplicate Substring (LC 1044) ‚≠ê‚≠ê‚≠ê‚≠ê
23. Shortest Palindrome (LC 214) ‚≠ê‚≠ê‚≠ê‚≠ê

Pattern 5 - Custom:
24. Max Points on a Line (LC 149) ‚≠ê‚≠ê‚≠ê‚≠ê
25. Isomorphic Strings (LC 205) ‚≠ê‚≠ê
26. Line Reflection (LC 356)
27. Serialize/Deserialize Binary Tree (LC 297) ‚≠ê‚≠ê‚≠ê
```

### Tier 4: Expert (Top 1% territory)
```
Pattern 4 - Rolling Hash:
28. Distinct Echo Substrings (LC 1316) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
29. Longest Happy Prefix (LC 1392) ‚≠ê‚≠ê‚≠ê‚≠ê

Pattern 5 - Custom:
30. LRU Cache (LC 146) ‚≠ê‚≠ê‚≠ê‚≠ê
31. LFU Cache (LC 460) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
32. Design HashMap (LC 706) ‚≠ê‚≠ê‚≠ê
33. Design HashSet (LC 705) ‚≠ê‚≠ê‚≠ê

Mixed Patterns:
34. Subarray Sum Equals K (LC 560) ‚≠ê‚≠ê‚≠ê
35. Continuous Subarray Sum (LC 523) ‚≠ê‚≠ê‚≠ê
36. Longest Substring Without Repeating (LC 3) ‚≠ê‚≠ê‚≠ê
37. Minimum Window Substring (LC 76) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
38. Sliding Window Maximum (LC 239) ‚≠ê‚≠ê‚≠ê‚≠ê
39. Find All Numbers Disappeared (LC 448) ‚≠ê‚≠ê
40. Find All Duplicates (LC 442) ‚≠ê‚≠ê
```

### Tier 5: Competition (Olympiad level)
```
41. Count of Smaller Numbers After Self (LC 315) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
42. Count of Range Sum (LC 327) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
43. Palindrome Pairs (LC 336) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
44. Substring with Concatenation (LC 30) ‚≠ê‚≠ê‚≠ê‚≠ê
45. Alien Dictionary (LC 269) ‚≠ê‚≠ê‚≠ê‚≠ê
46. Employee Free Time (LC 759) ‚≠ê‚≠ê‚≠ê‚≠ê
47. Design In-Memory File System (LC 588) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
48. Valid Sudoku (LC 36) ‚≠ê‚≠ê
49. Insert Delete GetRandom O(1) (LC 380) ‚≠ê‚≠ê‚≠ê‚≠ê
50. Reconstruct Itinerary (LC 332) ‚≠ê‚≠ê‚≠ê‚≠ê
```

**‚≠ê Rating**: Importance for mastery (1-5 stars)

---

## üßò Deliberate Practice Methodology

### The 4-Step Mastery Cycle

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                  ‚îÇ
‚îÇ  1. UNDERSTAND                                   ‚îÇ
‚îÇ     ‚îú‚îÄ Read problem 3 times                     ‚îÇ
‚îÇ     ‚îú‚îÄ Identify pattern                         ‚îÇ
‚îÇ     ‚îî‚îÄ Visualize examples                       ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  2. PLAN                                         ‚îÇ
‚îÇ     ‚îú‚îÄ Choose approach                          ‚îÇ
‚îÇ     ‚îú‚îÄ Design hash function                     ‚îÇ
‚îÇ     ‚îî‚îÄ Consider edge cases                      ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  3. IMPLEMENT                                    ‚îÇ
‚îÇ     ‚îú‚îÄ Write clean code                         ‚îÇ
‚îÇ     ‚îú‚îÄ Test incrementally                       ‚îÇ
‚îÇ     ‚îî‚îÄ Handle errors                            ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  4. REFLECT                                      ‚îÇ
‚îÇ     ‚îú‚îÄ Analyze complexity                       ‚îÇ
‚îÇ     ‚îú‚îÄ Identify improvements                    ‚îÇ
‚îÇ     ‚îî‚îÄ Extract lessons                          ‚îÇ
‚îÇ                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Daily Practice Schedule

**Beginner (Weeks 1-2)**
- 2 easy problems/day
- 1 hour study + 1 hour practice
- Focus: Pattern recognition

**Intermediate (Weeks 3-4)**
- 1 easy + 1 medium/day
- 2 hours practice
- Focus: Implementation speed

**Advanced (Weeks 5-6)**
- 1 medium + 1 hard/day
- 2-3 hours practice
- Focus: Optimization

**Expert (Weeks 7-8)**
- 1 hard + 1 expert/day
- 3 hours practice
- Focus: Competition problems

---

## üé§ Interview Strategy & Communication

### The STAR Method for Hash Problems

**S**ituation: Understand the problem
```
"Let me clarify the constraints...
- Input size?
- Value range?
- Space limitations?
- Expected frequency of operations?"
```

**T**ask: Identify the pattern
```
"This is a [pattern name] problem because...
I need to [group/count/find] items based on [property]."
```

**A**ction: Design solution
```
"I'll use a hash [map/set] with key being [explanation].
This gives us O(n) time instead of O(n¬≤)."
```

**R**esult: Implement and analyze
```
"Time complexity: O(n)
Space complexity: O(k) where k = [unique elements]
Edge cases handled: [empty input, duplicates, etc.]"
```

### Red Flags to Avoid
‚ùå "I'll just use a hash map" (without explaining why)
‚ùå Not discussing collision handling
‚ùå Ignoring space complexity
‚ùå Not mentioning edge cases

### Green Flags to Show
‚úÖ Clear hash function design rationale
‚úÖ Complexity analysis (time + space)
‚úÖ Discussion of alternatives
‚úÖ Clean, readable code

---

## üß† Cognitive Strategies (Meta-Learning)

### Chunking for Pattern Recognition
```
Level 1: Individual concepts
    - Hash function
    - Collision
    - Load factor

Level 2: Pattern chunks
    - "Frequency counter pattern"
    - "Canonical form pattern"
    - "Rolling hash pattern"

Level 3: Problem-solving chunks
    - "See 'count' ‚Üí think frequency map"
    - "See 'group' ‚Üí think hash key design"
    - "See 'substring' ‚Üí think rolling hash"

Goal: Recognize entire patterns instantly
```

### Spaced Repetition Schedule
```
Day 1:   Learn pattern + solve 3 problems
Day 3:   Review + solve 2 new problems
Day 7:   Review + solve 1 hard problem
Day 14:  Review + solve 1 expert problem
Day 30:  Final review + competition problem

Principle: Increasing intervals, increasing difficulty
```

### Mental Simulation
```
Before coding:
1. Visualize data structure state
2. Step through algorithm mentally
3. Identify potential bugs
4. Estimate complexity

This "pre-coding" phase saves debugging time!
```

---

## üèÜ Mastery Checklist

### Knowledge
- [ ] Understand hash table internals (chaining, probing)
- [ ] Know time/space complexity of all operations
- [ ] Can explain collision handling strategies
- [ ] Understand rolling hash mathematics
- [ ] Know when NOT to use hash functions

### Skills
- [ ] Recognize patterns in < 1 minute
- [ ] Design custom hash functions
- [ ] Implement from scratch in all my languages
- [ ] Debug hash collision issues
- [ ] Optimize for performance

### Experience
- [ ] Solved 50+ hash problems
- [ ] Participated in 5+ coding competitions
- [ ] Taught hash patterns to someone else
- [ ] Debugged hash-related production bug
- [ ] Written hash-heavy system design

---

## üìñ Recommended Resources

### Books
1. **"Introduction to Algorithms" (CLRS)** - Chapter 11 (Hash Tables)
2. **"The Algorithm Design Manual" (Skiena)** - Chapters on hashing
3. **"Competitive Programming" (Halim)** - String algorithms

### Online
1. **LeetCode** - Pattern-based practice
2. **Codeforces** - Competition problems
3. **CP-Algorithms** - Detailed explanations

### Academic Papers
1. **"Rabin-Karp Algorithm"** - Original paper
2. **"Zobrist Hashing"** - Game tree hashing
3. **"Universal Hashing"** - Theoretical foundations

---

## üéØ Final Wisdom

### The Three Laws of Hash Mastery

**Law 1: Pattern Over Implementation**
```
Recognize the pattern quickly.
Implementation is just typing.
```

**Law 2: Simplicity Over Cleverness**
```
The best hash function is the simplest one that works.
Don't over-engineer.
```

**Law 3: Practice Over Perfection**
```
Solve 100 problems imperfectly
> 
Solve 10 problems perfectly
```

---

## üí™ Your Journey Starts Now

You have the knowledge. You have the roadmap. You have the strategies.

Now, it's time for deliberate practice.

**Week 1 Goal**: Master Pattern 1 (Frequency)
**Month 1 Goal**: Solve all Tier 1 & 2 problems
**Month 2 Goal**: Compete in weekly contests
**Month 3 Goal**: Top 1% on LeetCode in hash problems

*"The journey of a thousand problems begins with a single hash function."*

Remember: You're not just learning algorithms. You're training your mind to see patterns, think mathematically, and solve problems elegantly.

Welcome to the top 1%. Let's begin. üöÄ

---

END OF COMPREHENSIVE GUIDE

All supporting documents:
- valid_anagram.md
- group_anagrams.md
- existence_membership_patterns.md
- rolling_hash_patterns.md
- custom_hash_patterns.md

# Valid Anagram - Multi-Language Implementation

## Problem
Given two strings s and t, return true if t is an anagram of s, and false otherwise.

## Approach 1: Hash Map (Dictionary) - O(n) Time, O(k) Space
Where k = number of unique characters

---

### Python Implementation
```python
def is_anagram_hashmap(s: str, t: str) -> bool:
    """
    Frequency counting approach using dictionary.
    
    Time Complexity: O(n + m) where n = len(s), m = len(t)
    Space Complexity: O(k) where k = unique characters
    
    Mental Model: Build frequency map, then verify
    """
    # Early exit: different lengths can't be anagrams
    if len(s) != len(t):
        return False
    
    # Count frequencies for string s
    freq_map = {}
    for char in s:
        freq_map[char] = freq_map.get(char, 0) + 1
    
    # Decrement for string t
    for char in t:
        if char not in freq_map:
            return False
        freq_map[char] -= 1
        if freq_map[char] < 0:
            return False
    
    # All counts should be zero
    return all(count == 0 for count in freq_map.values())


# Pythonic version using Counter
from collections import Counter

def is_anagram_counter(s: str, t: str) -> bool:
    """
    Idiomatic Python using Counter.
    Same complexity but more readable.
    """
    return Counter(s) == Counter(t)
```

---

### Rust Implementation
```rust
use std::collections::HashMap;

pub fn is_anagram_hashmap(s: String, t: String) -> bool {
    // Early exit for different lengths
    if s.len() != t.len() {
        return false;
    }
    
    // Build frequency map for s
    let mut freq_map: HashMap<char, i32> = HashMap::new();
    for ch in s.chars() {
        *freq_map.entry(ch).or_insert(0) += 1;
    }
    
    // Decrement for t
    for ch in t.chars() {
        match freq_map.get_mut(&ch) {
            Some(count) => {
                *count -= 1;
                if *count < 0 {
                    return false;
                }
            }
            None => return false,
        }
    }
    
    // Verify all counts are zero
    freq_map.values().all(|&count| count == 0)
}

// Performance Note: Rust's HashMap uses SipHash by default for security.
// For competitive programming, consider using FxHashMap from rustc-hash crate.
```

---

### C Implementation
```c
#include <stdbool.h>
#include <string.h>

bool is_anagram_array(char* s, char* t) {
    // For ASCII lowercase letters only
    int len_s = strlen(s);
    int len_t = strlen(t);
    
    if (len_s != len_t) {
        return false;
    }
    
    // Fixed-size array for 26 lowercase letters
    int freq[26] = {0};
    
    // Count frequencies
    for (int i = 0; i < len_s; i++) {
        freq[s[i] - 'a']++;
        freq[t[i] - 'a']--;
    }
    
    // Check if all frequencies are zero
    for (int i = 0; i < 26; i++) {
        if (freq[i] != 0) {
            return false;
        }
    }
    
    return true;
}

// Performance: This is the fastest approach for limited character sets
// Array access: O(1), no hash computation overhead
```

---

### C++ Implementation
```cpp
#include <string>
#include <unordered_map>
#include <algorithm>

class Solution {
public:
    // Approach 1: Hash map
    bool isAnagram_hashmap(std::string s, std::string t) {
        if (s.length() != t.length()) {
            return false;
        }
        
        std::unordered_map<char, int> freq;
        
        // Build frequency map
        for (char ch : s) {
            freq[ch]++;
        }
        
        // Decrement for t
        for (char ch : t) {
            if (freq.find(ch) == freq.end() || --freq[ch] < 0) {
                return false;
            }
        }
        
        return true;
    }
    
    // Approach 2: Sorting (alternative)
    bool isAnagram_sorting(std::string s, std::string t) {
        // Time: O(n log n), Space: O(1) if modifying in place
        if (s.length() != t.length()) {
            return false;
        }
        
        std::sort(s.begin(), s.end());
        std::sort(t.begin(), t.end());
        
        return s == t;
    }
};
```

---

### Go Implementation
```go
package main

func isAnagram(s string, t string) bool {
    // Early exit for different lengths
    if len(s) != len(t) {
        return false
    }
    
    // Build frequency map
    freq := make(map[rune]int)
    
    for _, ch := range s {
        freq[ch]++
    }
    
    // Decrement for t
    for _, ch := range t {
        if count, exists := freq[ch]; !exists || count == 0 {
            return false
        }
        freq[ch]--
    }
    
    return true
}

// Alternative: Array approach for ASCII
func isAnagramArray(s string, t string) bool {
    if len(s) != len(t) {
        return false
    }
    
    var freq [26]int
    
    for i := 0; i < len(s); i++ {
        freq[s[i]-'a']++
        freq[t[i]-'a']--
    }
    
    for _, count := range freq {
        if count != 0 {
            return false
        }
    }
    
    return true
}
```

---

## Approach 2: Array Frequency Table - O(n) Time, O(1) Space

### When to Use
- Limited character set (e.g., lowercase English letters: 26 characters)
- Performance-critical code
- Want to avoid hash computation overhead

### Visualization
```
String: "anagram"  ‚Üí  Array indices for 'a' to 'z'

Index:  [0][1][2][3][4][5]...[25]
Letter:  a  b  c  d  e  f  ...  z
Count:   3  0  0  0  0  0  ...  0
         ‚Üë
    'a' appears 3 times

Mapping: char ‚Üí index
    'a' ‚Üí 0 (via: 'a' - 'a' = 0)
    'b' ‚Üí 1 (via: 'b' - 'a' = 1)
    'z' ‚Üí 25
```

---

## Comparison Matrix

| Approach | Time | Space | Use Case |
|----------|------|-------|----------|
| Hash Map | O(n) | O(k) | Unicode, large character set |
| Array | O(n) | O(1) | ASCII, fixed character set |
| Sorting | O(n log n) | O(1)* | When mutation allowed |

*O(1) if sorting in place; O(n) if creating copies

---

## Key Insights & Optimization Techniques

### 1. Early Exit Pattern
```
if len(s) != len(t):
    return False
```
**Why**: Saves unnecessary computation. Different lengths cannot be anagrams.

### 2. Single Pass Optimization
Instead of two separate passes (count s, then verify t), we can:
```python
for i in range(len(s)):
    freq[s[i]] += 1
    freq[t[i]] -= 1
```

### 3. Space Optimization for Follow-up
**Question**: "What if inputs contain unicode?"
**Answer**: Hash map is necessary (array becomes impractical)

**Question**: "What if you cannot use extra space?"
**Answer**: Sort both strings in place (if allowed), O(n log n) time

---

## Common Mistakes to Avoid

1. **Forgetting length check**: Wastes computation
2. **Not handling empty strings**: Edge case
3. **Case sensitivity**: "Aa" vs "aa" - clarify with interviewer
4. **Character encoding**: ASCII vs Unicode matters

---

## Practice Variations

1. **Group Anagrams** (LeetCode 49)
2. **Find All Anagrams in a String** (LeetCode 438)
3. **Permutation in String** (LeetCode 567)

---

## Performance Benchmarks (Approximate)

For string length n = 10,000:
- **Hash Map**: ~0.5ms
- **Array (26)**: ~0.2ms (fastest)
- **Sorting**: ~2ms

**Takeaway**: For fixed character sets, arrays beat hash maps. For flexibility, hash maps win.

# Group Anagrams - Multi-Language Implementation

## Problem
Given an array of strings, group the anagrams together.

## Approach Comparison

| Method | Hash Key | Time per String | Space | Pros | Cons |
|--------|----------|-----------------|-------|------|------|
| Sorted String | Sort chars | O(k log k) | O(nk) | Simple | Slower for long strings |
| Frequency Array | Char counts | O(k) | O(nk) | Faster | More code |
| Prime Product | Prime multiplication | O(k) | O(nk) | Clever | Overflow risk |

Where: n = number of strings, k = max string length

---

## Approach 1: Sorted String as Key

### Python Implementation
```python
from typing import List
from collections import defaultdict

def group_anagrams_sorted(strs: List[str]) -> List[List[str]]:
    """
    Use sorted string as hash key.
    
    Time: O(n * k log k) where n = len(strs), k = max(len(str))
    Space: O(nk) for storing all strings
    
    Mental Model: 
        - Sorted strings are "fingerprints" of anagram groups
        - All anagrams have identical fingerprints
    """
    anagram_map = defaultdict(list)
    
    for string in strs:
        # Sort characters to create canonical form
        sorted_str = ''.join(sorted(string))
        anagram_map[sorted_str].append(string)
    
    return list(anagram_map.values())


# Example usage:
# Input: ["eat","tea","tan","ate","nat","bat"]
# 
# Internal state evolution:
# 
# "eat" ‚Üí sorted: "aet" ‚Üí map: {"aet": ["eat"]}
# "tea" ‚Üí sorted: "aet" ‚Üí map: {"aet": ["eat", "tea"]}
# "tan" ‚Üí sorted: "ant" ‚Üí map: {"aet": ["eat", "tea"], "ant": ["tan"]}
# "ate" ‚Üí sorted: "aet" ‚Üí map: {"aet": ["eat", "tea", "ate"], "ant": ["tan"]}
# "nat" ‚Üí sorted: "ant" ‚Üí map: {"aet": ["eat", "tea", "ate"], "ant": ["tan", "nat"]}
# "bat" ‚Üí sorted: "abt" ‚Üí map: {"aet": [...], "ant": [...], "abt": ["bat"]}
```

---

## Approach 2: Frequency Signature as Key

### Python Implementation
```python
def group_anagrams_frequency(strs: List[str]) -> List[List[str]]:
    """
    Use character frequency tuple as hash key.
    
    Time: O(nk) - better than sorting!
    Space: O(nk)
    
    Key Insight: Tuples are hashable in Python, lists are not.
    We convert frequency array to tuple for use as dict key.
    """
    anagram_map = defaultdict(list)
    
    for string in strs:
        # Create frequency signature
        # Initialize array for 26 lowercase letters
        count = [0] * 26
        for char in string:
            count[ord(char) - ord('a')] += 1
        
        # Convert list to tuple (hashable)
        key = tuple(count)
        anagram_map[key].append(string)
    
    return list(anagram_map.values())


# Visualization of frequency signature:
# "eat" ‚Üí [1,0,0,0,1,0,0,...,1,0,0] (a=1, e=1, t=1)
#         ‚Üì tuple
#         (1,0,0,0,1,0,0,...,1,0,0) ‚Üê Hash key
```

---

## Rust Implementation

```rust
use std::collections::HashMap;

pub fn group_anagrams_sorted(strs: Vec<String>) -> Vec<Vec<String>> {
    let mut anagram_map: HashMap<String, Vec<String>> = HashMap::new();
    
    for s in strs {
        // Sort characters to create key
        let mut chars: Vec<char> = s.chars().collect();
        chars.sort_unstable(); // unstable_sort is faster
        let key: String = chars.into_iter().collect();
        
        // Insert into map
        anagram_map.entry(key).or_insert_with(Vec::new).push(s);
    }
    
    anagram_map.into_values().collect()
}

// Frequency-based approach
pub fn group_anagrams_frequency(strs: Vec<String>) -> Vec<Vec<String>> {
    let mut anagram_map: HashMap<[u8; 26], Vec<String>> = HashMap::new();
    
    for s in strs {
        let mut count = [0u8; 26];
        
        for ch in s.chars() {
            count[(ch as usize) - ('a' as usize)] += 1;
        }
        
        anagram_map.entry(count).or_insert_with(Vec::new).push(s);
    }
    
    anagram_map.into_values().collect()
}

// Performance Notes:
// - sort_unstable() is faster than sort() (doesn't preserve order)
// - Using [u8; 26] array as key is efficient (implements Hash trait)
// - into_values() consumes the map (zero-copy)
```

---

## C++ Implementation

```cpp
#include <vector>
#include <string>
#include <unordered_map>
#include <algorithm>

class Solution {
public:
    // Approach 1: Sorted key
    std::vector<std::vector<std::string>> groupAnagrams_sorted(
        std::vector<std::string>& strs
    ) {
        std::unordered_map<std::string, std::vector<std::string>> anagram_map;
        
        for (const auto& s : strs) {
            std::string key = s;
            std::sort(key.begin(), key.end());
            anagram_map[key].push_back(s);
        }
        
        std::vector<std::vector<std::string>> result;
        result.reserve(anagram_map.size());
        
        for (auto& [key, group] : anagram_map) {
            result.push_back(std::move(group));
        }
        
        return result;
    }
    
    // Approach 2: Frequency key
    std::vector<std::vector<std::string>> groupAnagrams_frequency(
        std::vector<std::string>& strs
    ) {
        // Custom hash for array<int, 26>
        auto hash_fn = [](const std::array<int, 26>& arr) {
            size_t hash = 0;
            for (int i = 0; i < 26; ++i) {
                hash ^= std::hash<int>{}(arr[i]) + 0x9e3779b9 + 
                        (hash << 6) + (hash >> 2);
            }
            return hash;
        };
        
        std::unordered_map<
            std::array<int, 26>,
            std::vector<std::string>,
            decltype(hash_fn)
        > anagram_map(10, hash_fn);
        
        for (const auto& s : strs) {
            std::array<int, 26> count = {0};
            for (char ch : s) {
                count[ch - 'a']++;
            }
            anagram_map[count].push_back(s);
        }
        
        std::vector<std::vector<std::string>> result;
        result.reserve(anagram_map.size());
        
        for (auto& [key, group] : anagram_map) {
            result.push_back(std::move(group));
        }
        
        return result;
    }
};

// Note on custom hash function:
// C++ doesn't provide default hash for std::array
// We combine individual element hashes using XOR and bit shifts
// 0x9e3779b9 is the golden ratio used in many hash functions
```

---

## C Implementation

```c
#include <stdlib.h>
#include <string.h>

// Helper: Compare function for qsort
int char_compare(const void* a, const void* b) {
    return (*(char*)a - *(char*)b);
}

// Helper: Sort string in place
void sort_string(char* str) {
    qsort(str, strlen(str), sizeof(char), char_compare);
}

// Simple hash table implementation for C
#define TABLE_SIZE 10000

typedef struct Node {
    char* key;
    char** strings;
    int count;
    int capacity;
    struct Node* next;
} Node;

typedef struct {
    Node* buckets[TABLE_SIZE];
} HashTable;

// Simple hash function
unsigned int hash(const char* str) {
    unsigned int hash = 5381;
    int c;
    while ((c = *str++)) {
        hash = ((hash << 5) + hash) + c;
    }
    return hash % TABLE_SIZE;
}

// Note: Full implementation omitted for brevity
// C requires significant boilerplate for hash tables
// In practice, consider using libraries like uthash
```

---

## Go Implementation

```go
package main

import (
    "sort"
    "strings"
)

// Approach 1: Sorted key
func groupAnagrams_sorted(strs []string) [][]string {
    anagramMap := make(map[string][]string)
    
    for _, s := range strs {
        // Sort string to create key
        chars := strings.Split(s, "")
        sort.Strings(chars)
        key := strings.Join(chars, "")
        
        anagramMap[key] = append(anagramMap[key], s)
    }
    
    // Convert map to slice
    result := make([][]string, 0, len(anagramMap))
    for _, group := range anagramMap {
        result = append(result, group)
    }
    
    return result
}

// Approach 2: Frequency key
func groupAnagrams_frequency(strs []string) [][]string {
    anagramMap := make(map[[26]int][]string)
    
    for _, s := range strs {
        var count [26]int
        for _, ch := range s {
            count[ch-'a']++
        }
        
        anagramMap[count] = append(anagramMap[count], s)
    }
    
    result := make([][]string, 0, len(anagramMap))
    for _, group := range anagramMap {
        result = append(result, group)
    }
    
    return result
}

// Go advantage: Arrays are comparable and hashable by default
// [26]int can be used directly as map key
```

---

## Complexity Analysis Deep Dive

### Time Complexity Breakdown

**Sorted Approach**:
```
Total: O(n * k log k)

For each string (n times):
    1. Sort characters: O(k log k)
    2. Hash lookup: O(1) average
    3. Append to list: O(1) amortized

Where:
    n = number of strings
    k = maximum length of any string
```

**Frequency Approach**:
```
Total: O(n * k)

For each string (n times):
    1. Count characters: O(k)
    2. Create tuple/array key: O(26) = O(1)
    3. Hash lookup: O(1) average
    4. Append to list: O(1) amortized

Faster asymptotically! But tuple/array creation has overhead.
```

### Space Complexity
Both approaches: **O(nk)**
- Need to store all n strings
- Each string up to length k
- Hash map overhead: O(n) for keys

---

## Advanced Optimization: Prime Number Product

### Concept
Map each character to a unique prime number, multiply primes for each character.

```python
def group_anagrams_prime(strs: List[str]) -> List[List[str]]:
    """
    Clever but risky: Use prime product as hash key.
    
    Time: O(nk) - same as frequency
    Issue: Integer overflow for long strings
    """
    # First 26 primes for a-z
    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 
              43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101]
    
    anagram_map = defaultdict(list)
    
    for string in strs:
        product = 1
        for char in string:
            product *= primes[ord(char) - ord('a')]
        anagram_map[product].append(string)
    
    return list(anagram_map.values())

# Why it works: Fundamental Theorem of Arithmetic
# Every integer has unique prime factorization
# "eat" ‚Üí 2^1 * 101^1 * 373^1 (unique!)
# "tea" ‚Üí 2^1 * 101^1 * 373^1 (same!)
#
# Why it fails: Overflow for strings like "zzzzzzz..."
```

---

## Key Insights & Mental Models

### 1. **Canonical Form Pattern**
Transform data into a unique "canonical" representation:
- Anagrams ‚Üí Sorted string
- Coordinates ‚Üí "(x,y)" string
- Multiset ‚Üí Frequency signature

### 2. **Hash Key Design Checklist**
‚úì Uniquely identifies the group  
‚úì Same for all items in group  
‚úì Efficiently computable  
‚úì Hashable in your language  

### 3. **Language-Specific Considerations**

**Python**: 
- Lists not hashable ‚Üí use tuples
- `defaultdict(list)` simplifies code

**Rust**:
- Arrays implement `Hash` trait
- `entry().or_insert()` pattern is idiomatic

**C++**:
- Custom hash for complex keys
- Use `std::move` to avoid copies

**Go**:
- Arrays comparable by default
- Slicing strings is expensive

---

## Practice Problems

1. **Group Shifted Strings** (LeetCode 249)
   - Hash key: Difference sequence between characters

2. **Find Duplicate Subtrees** (LeetCode 652)
   - Hash key: Serialized tree structure

3. **Encode and Decode TinyURL** (LeetCode 535)
   - Bidirectional hash mapping

---

## Common Interview Follow-ups

**Q**: What if strings can have Unicode?  
**A**: Sorted approach still works. Frequency needs larger array or hash map.

**Q**: How to handle very long strings efficiently?  
**A**: Use frequency signature (O(k) vs O(k log k))

**Q**: Can we do better than O(nk)?  
**A**: No. Must examine every character at least once.

**Q**: How does hash collision affect performance?  
**A**: Rare with good hash function. Worst case O(n¬≤) with many collisions.

# Existence/Membership Patterns - Complete Guide

## Core Concept
Use hash sets/maps to achieve O(1) lookup time for checking existence or retrieving associated data.

**Fundamental Trade-off**: Space O(n) for Time O(1)

---

## Problem 1: Two Sum (LeetCode 1)

### Problem Statement
Given an array of integers and a target, return indices of two numbers that add up to target.

### Approach Evolution

#### Brute Force (Learning Step)
```
Approach: Try every pair
Time: O(n¬≤), Space: O(1)

for i in range(n):
    for j in range(i+1, n):
        if nums[i] + nums[j] == target:
            return [i, j]

Why it's slow: We're searching for complement O(n) times
```

#### Optimized: Hash Map
```
Key Insight: Instead of searching for complement each time,
             store what we've seen with O(1) lookup

Mental Model:
    "What number would complete this pair?"
    ‚Üí complement = target - current
    "Have I seen the complement before?"
    ‚Üí Check hash map
```

---

### Multi-Language Implementation

#### Python
```python
from typing import List

def two_sum(nums: List[int], target: int) -> List[int]:
    """
    One-pass hash map solution.
    
    Time: O(n) - single pass through array
    Space: O(n) - hash map stores up to n elements
    
    Mental Model: "Remember what you've seen, check if complement exists"
    """
    seen = {}  # maps number ‚Üí index
    
    for i, num in enumerate(nums):
        complement = target - num
        
        # Check if we've seen the complement
        if complement in seen:
            return [seen[complement], i]
        
        # Store current number for future lookups
        seen[num] = i
    
    return []  # No solution found


# Visualization for [2, 7, 11, 15], target=9:
#
# i=0: num=2, comp=7
#      7 not in seen ‚Üí seen={2: 0}
#
# i=1: num=7, comp=2
#      2 in seen! ‚Üí return [0, 1]


# Alternative: Two-pass solution (also O(n))
def two_sum_two_pass(nums: List[int], target: int) -> List[int]:
    """
    Build complete map first, then search.
    Same complexity but less elegant.
    """
    num_map = {num: i for i, num in enumerate(nums)}
    
    for i, num in enumerate(nums):
        complement = target - num
        if complement in num_map and num_map[complement] != i:
            return [i, num_map[complement]]
    
    return []
```

---

#### Rust
```rust
use std::collections::HashMap;

pub fn two_sum(nums: Vec<i32>, target: i32) -> Vec<i32> {
    let mut seen: HashMap<i32, i32> = HashMap::new();
    
    for (i, &num) in nums.iter().enumerate() {
        let complement = target - num;
        
        if let Some(&comp_idx) = seen.get(&complement) {
            return vec![comp_idx, i as i32];
        }
        
        seen.insert(num, i as i32);
    }
    
    vec![] // No solution
}

// Rust-specific notes:
// - HashMap<K, V> requires K: Eq + Hash
// - Use &num to avoid moving value from vector
// - if let Some(x) is idiomatic for Option handling
// - as i32 casts usize to i32


// Performance optimization with capacity hint
pub fn two_sum_optimized(nums: Vec<i32>, target: i32) -> Vec<i32> {
    let mut seen = HashMap::with_capacity(nums.len());
    
    for (i, &num) in nums.iter().enumerate() {
        let complement = target - num;
        
        if let Some(&comp_idx) = seen.get(&complement) {
            return vec![comp_idx, i as i32];
        }
        
        seen.insert(num, i as i32);
    }
    
    vec![]
}
// with_capacity() pre-allocates space, avoiding rehashing
```

---

#### C++
```cpp
#include <vector>
#include <unordered_map>

class Solution {
public:
    std::vector<int> twoSum(std::vector<int>& nums, int target) {
        std::unordered_map<int, int> seen;
        
        for (int i = 0; i < nums.size(); ++i) {
            int complement = target - nums[i];
            
            // Check if complement exists
            auto it = seen.find(complement);
            if (it != seen.end()) {
                return {it->second, i};
            }
            
            // Store current number
            seen[nums[i]] = i;
        }
        
        return {};
    }
    
    // Alternative: Using count() for existence check
    std::vector<int> twoSum_v2(std::vector<int>& nums, int target) {
        std::unordered_map<int, int> seen;
        
        for (int i = 0; i < nums.size(); ++i) {
            int complement = target - nums[i];
            
            if (seen.count(complement)) {  // count returns 0 or 1
                return {seen[complement], i};
            }
            
            seen[nums[i]] = i;
        }
        
        return {};
    }
};

// Performance notes:
// - find() returns iterator, single lookup
// - count() + access = two lookups (less efficient)
// - reserve() can pre-allocate: seen.reserve(nums.size())
```

---

#### C
```c
#include <stdlib.h>

// Simple hash table for integers
#define TABLE_SIZE 1000

typedef struct Entry {
    int key;
    int value;
    int occupied;
} Entry;

int hash_func(int key) {
    return abs(key) % TABLE_SIZE;
}

int* twoSum(int* nums, int numsSize, int target, int* returnSize) {
    Entry table[TABLE_SIZE] = {0};
    
    for (int i = 0; i < numsSize; i++) {
        int complement = target - nums[i];
        int idx = hash_func(complement);
        
        // Linear probing for collision resolution
        int original_idx = idx;
        while (table[idx].occupied) {
            if (table[idx].key == complement) {
                int* result = (int*)malloc(2 * sizeof(int));
                result[0] = table[idx].value;
                result[1] = i;
                *returnSize = 2;
                return result;
            }
            idx = (idx + 1) % TABLE_SIZE;
            if (idx == original_idx) break; // Table full
        }
        
        // Insert current number
        idx = hash_func(nums[i]);
        while (table[idx].occupied && table[idx].key != nums[i]) {
            idx = (idx + 1) % TABLE_SIZE;
        }
        table[idx].key = nums[i];
        table[idx].value = i;
        table[idx].occupied = 1;
    }
    
    *returnSize = 0;
    return NULL;
}

// Notes:
// - C requires manual hash table implementation
// - Linear probing: if collision, try next slot
// - Must handle hash collisions explicitly
```

---

#### Go
```go
package main

func twoSum(nums []int, target int) []int {
    seen := make(map[int]int)
    
    for i, num := range nums {
        complement := target - num
        
        // Check if complement exists
        if compIdx, exists := seen[complement]; exists {
            return []int{compIdx, i}
        }
        
        seen[num] = i
    }
    
    return []int{}
}

// Go idiom: "comma ok" pattern
// if value, ok := map[key]; ok { ... }

// Pre-sized map optimization
func twoSum_optimized(nums []int, target int) []int {
    seen := make(map[int]int, len(nums))
    
    for i, num := range nums {
        complement := target - num
        
        if compIdx, exists := seen[complement]; exists {
            return []int{compIdx, i}
        }
        
        seen[num] = i
    }
    
    return []int{}
}
```

---

## Problem 2: Contains Duplicate (LeetCode 217)

### Problem Statement
Return true if any value appears at least twice in array.

### Mental Model
```
Question: "Have I seen this number before?"
Answer:   Hash set gives O(1) answer!

Without hash set: O(n¬≤) - check each pair
With hash set:    O(n)  - check each once
```

### Implementation

#### Python
```python
def contains_duplicate_set(nums: List[int]) -> bool:
    """
    Most straightforward: use set.
    
    Time: O(n)
    Space: O(n)
    """
    seen = set()
    for num in nums:
        if num in seen:
            return True
        seen.add(num)
    return False


# Pythonic one-liner
def contains_duplicate_pythonic(nums: List[int]) -> bool:
    """
    Compare list length with set length.
    If different, duplicates exist.
    """
    return len(nums) != len(set(nums))
    # Elegant but builds entire set even if early duplicate


# Space-optimized for sorted input
def contains_duplicate_sorted(nums: List[int]) -> bool:
    """
    If allowed to modify input: sort first.
    
    Time: O(n log n)
    Space: O(1) if in-place sort
    """
    nums.sort()
    for i in range(len(nums) - 1):
        if nums[i] == nums[i + 1]:
            return True
    return False
```

---

#### Rust
```rust
use std::collections::HashSet;

pub fn contains_duplicate(nums: Vec<i32>) -> bool {
    let mut seen: HashSet<i32> = HashSet::new();
    
    for num in nums {
        if !seen.insert(num) {  // insert returns false if key existed
            return true;
        }
    }
    
    false
}

// Rust HashSet advantage: insert() returns bool
// true if inserted (new), false if already present


// Idiomatic functional approach
pub fn contains_duplicate_functional(nums: Vec<i32>) -> bool {
    let mut seen = HashSet::new();
    !nums.into_iter().all(|num| seen.insert(num))
}
// all() returns true only if all elements satisfy predicate
// We negate because we want to find duplicates
```

---

#### C++
```cpp
#include <vector>
#include <unordered_set>

class Solution {
public:
    bool containsDuplicate(std::vector<int>& nums) {
        std::unordered_set<int> seen;
        
        for (int num : nums) {
            if (seen.count(num)) {
                return true;
            }
            seen.insert(num);
        }
        
        return false;
    }
    
    // Alternative using insert return value
    bool containsDuplicate_v2(std::vector<int>& nums) {
        std::unordered_set<int> seen;
        
        for (int num : nums) {
            // insert returns pair<iterator, bool>
            // bool is false if element already existed
            if (!seen.insert(num).second) {
                return true;
            }
        }
        
        return false;
    }
};
```

---

#### Go
```go
func containsDuplicate(nums []int) bool {
    seen := make(map[int]bool)
    
    for _, num := range nums {
        if seen[num] {
            return true
        }
        seen[num] = true
    }
    
    return false
}

// Go idiom: map[int]bool for set behavior
// Alternative: map[int]struct{} uses less memory
func containsDuplicate_struct(nums []int) bool {
    seen := make(map[int]struct{})
    
    for _, num := range nums {
        if _, exists := seen[num]; exists {
            return true
        }
        seen[num] = struct{}{}
    }
    
    return false
}
// struct{} has zero size, saves memory for large sets
```

---

## Problem 3: Intersection of Two Arrays (LeetCode 349)

### Problem Statement
Given two arrays, return their intersection (unique elements present in both).

### Mental Model
```
Approach 1: Hash Set
    1. Convert array1 to set
    2. For each element in array2, check if in set
    3. Collect matches

Approach 2: Two Sets
    1. Convert both to sets
    2. Use set intersection operation (language-dependent)
```

### Implementation

#### Python
```python
def intersection(nums1: List[int], nums2: List[int]) -> List[int]:
    """
    Method 1: Manual checking.
    
    Time: O(n + m)
    Space: O(n)
    """
    set1 = set(nums1)
    result = set()
    
    for num in nums2:
        if num in set1:
            result.add(num)
    
    return list(result)


# Pythonic: Built-in set intersection
def intersection_pythonic(nums1: List[int], nums2: List[int]) -> List[int]:
    """
    Method 2: Use set intersection operator.
    Most elegant Python solution.
    """
    return list(set(nums1) & set(nums2))
    # & operator performs intersection


# If input already sorted (optimization)
def intersection_sorted(nums1: List[int], nums2: List[int]) -> List[int]:
    """
    Two-pointer approach for sorted arrays.
    
    Time: O(n + m)
    Space: O(1) excluding output
    """
    nums1.sort()
    nums2.sort()
    
    i, j = 0, 0
    result = []
    
    while i < len(nums1) and j < len(nums2):
        if nums1[i] < nums2[j]:
            i += 1
        elif nums1[i] > nums2[j]:
            j += 1
        else:
            if not result or result[-1] != nums1[i]:
                result.append(nums1[i])
            i += 1
            j += 1
    
    return result
```

---

#### Rust
```rust
use std::collections::HashSet;

pub fn intersection(nums1: Vec<i32>, nums2: Vec<i32>) -> Vec<i32> {
    let set1: HashSet<i32> = nums1.into_iter().collect();
    let mut result = HashSet::new();
    
    for num in nums2 {
        if set1.contains(&num) {
            result.insert(num);
        }
    }
    
    result.into_iter().collect()
}

// Functional style using iterator chains
pub fn intersection_functional(nums1: Vec<i32>, nums2: Vec<i32>) -> Vec<i32> {
    let set1: HashSet<i32> = nums1.into_iter().collect();
    
    nums2.into_iter()
        .filter(|num| set1.contains(num))
        .collect::<HashSet<_>>()  // Remove duplicates from nums2
        .into_iter()
        .collect()
}
```

---

## Complexity Analysis Summary

| Problem | Hash Approach Time | Hash Space | Alternative Time | Alternative Space |
|---------|-------------------|------------|------------------|-------------------|
| Two Sum | O(n) | O(n) | O(n¬≤) brute | O(1) |
| Contains Dup | O(n) | O(n) | O(n log n) sort | O(1) |
| Intersection | O(n+m) | O(n) | O(n log n + m log m) | O(1) two-ptr |

---

## Key Insights & Patterns

### 1. **The Complement Pattern** (Two Sum)
```
Instead of: "Find two numbers that sum to X"
Think:      "For each number, does (X - number) exist?"
```

### 2. **The Existence Check Pattern**
```
Question: "Is X in collection?"
Hash Set: O(1)
Array:    O(n) linear search
```

### 3. **The Deduplication Pattern**
```
Problem: Remove duplicates
Solution: Add to set (automatic deduplication)
```

### 4. **Space-Time Tradeoff**
```
No extra space: O(n¬≤) or O(n log n)
With O(n) space: O(n) time

Decision: Almost always choose hash solution unless:
    - Space is critically limited
    - Input is already sorted
    - Input is small (n < 100)
```

---

## Common Mistakes & Edge Cases

### 1. **Two Sum: Same Element Twice**
```python
# Wrong: Returns same index twice
if target - num in seen:
    return [seen[target - num], i]  # What if target = 2*num at i?

# Correct: Check after storing
seen[num] = i
```

### 2. **Collision Handling in C**
```c
// Must handle hash collisions explicitly
// Linear probing, chaining, or better hash function
```

### 3. **Integer Overflow**
```cpp
int complement = target - nums[i];  // May overflow!
// Use long long if needed
```

---

## Practice Problems

### Easy
1. **Happy Number** (LeetCode 202) - Detect cycle with set
2. **Intersection of Two Arrays II** (LeetCode 350) - Count duplicates
3. **Single Number** (LeetCode 136) - XOR trick vs hash

### Medium
4. **Four Sum II** (LeetCode 454) - Multiple hash maps
5. **Longest Consecutive Sequence** (LeetCode 128) - Set for O(n)
6. **Subarray Sum Equals K** (LeetCode 560) - Prefix sum + hash

### Hard
7. **Max Points on a Line** (LeetCode 149) - Hash slopes
8. **Contains Duplicate III** (LeetCode 220) - Bucketing technique

---

## Interview Tips

### When interviewer asks: "Can you do better than O(n¬≤)?"
‚Üí Think hash map/set

### When you see: "Find two/three elements that..."
‚Üí Consider hash map for O(1) lookup

### When asked: "What if space is limited?"
‚Üí Discuss sorted two-pointer alternative

### When implementing in interview:
1. State approach clearly
2. Discuss edge cases (empty input, no solution)
3. Mention complexity upfront
4. Write clean, readable code

# Rolling Hash Patterns - Complete Guide

## Foundational Concepts

### What is a Rolling Hash?

A **rolling hash** is a hash function that can be efficiently updated when the input changes by a small amount (e.g., sliding a window).

**Key Terminology:**
- **Hash**: A numerical "fingerprint" of data
- **Rolling**: Updating incrementally, not recalculating
- **Window**: Substring being hashed
- **Base**: Radix for polynomial hash (usually prime, e.g., 31, 256)
- **Modulo**: Keep hash values bounded to prevent overflow

---

## Mathematical Foundation

### Polynomial Rolling Hash

The most common rolling hash treats string as polynomial:

```
String: "ABC"
ASCII values: A=65, B=66, C=67

Hash = (65 * base¬≤) + (66 * base¬π) + (67 * base‚Å∞) mod prime

Example with base=31, prime=10‚Åπ+7:
Hash = (65 * 31¬≤) + (66 * 31) + 67
     = (65 * 961) + (66 * 31) + 67
     = 62465 + 2046 + 67
     = 64578
```

### Rolling Property

When sliding window from "ABC" to "BCD":

```
Old hash: hash("ABC") = (A * base¬≤) + (B * base¬π) + (C * base‚Å∞)

Remove 'A':
    Subtract: A * base¬≤

Shift left (multiply by base):
    (B * base¬π) + (C * base‚Å∞) ‚Üí (B * base¬≤) + (C * base¬π)

Add 'D':
    Add: D * base‚Å∞

New hash: hash("BCD") = (B * base¬≤) + (C * base¬π) + (D * base‚Å∞)

Formula:
    new_hash = (old_hash - first_char * base^(k-1)) * base + new_char
    where k = window length
```

---

## Visualization: Rolling Hash in Action

```
Text: "HELLO"
Pattern: "LL"
Base = 26, treating 'A'=1, 'B'=2, etc.

Step-by-step:

Window 1: "HE"
    H=8, E=5
    hash = 8*26 + 5 = 213

Window 2: "EL"
    Remove H=8 from position 1: 213 - 8*26 = 213 - 208 = 5
    Shift: 5 * 26 = 130
    Add L=12: 130 + 12 = 142

Window 3: "LL"
    Remove E=5: 142 - 5*26 = 142 - 130 = 12
    Shift: 12 * 26 = 312
    Add L=12: 312 + 12 = 324

Pattern hash("LL") = 12*26 + 12 = 324 ‚úì Match!
```

---

## Classic Problem: Rabin-Karp String Matching

### Problem Statement
Find all occurrences of pattern in text.

### Approach Evolution

#### Naive Approach
```
Time: O((n-m+1) * m) where n=text length, m=pattern length
For each position:
    Compare pattern character by character
    
Example: n=10‚Å∂, m=10¬≥ ‚Üí 10‚Åπ operations (too slow!)
```

#### Rabin-Karp with Rolling Hash
```
Time: O(n + m) average case
      O(n * m) worst case (many hash collisions)

Algorithm:
1. Compute hash of pattern
2. Compute hash of first window in text
3. Slide window:
   - Update hash in O(1)
   - If hash matches, verify character-by-character
```

---

### Multi-Language Implementation

#### Python
```python
def rabin_karp(text: str, pattern: str) -> list[int]:
    """
    Find all starting indices where pattern occurs in text.
    
    Time: O(n + m) average, O(n*m) worst
    Space: O(1)
    
    Uses polynomial rolling hash with base=256 (ASCII), prime=10^9+7
    """
    if not pattern or len(pattern) > len(text):
        return []
    
    # Constants
    BASE = 256
    PRIME = 10**9 + 7
    
    n, m = len(text), len(pattern)
    pattern_hash = 0
    window_hash = 0
    base_power = 1  # BASE^(m-1) mod PRIME
    
    # Compute base^(m-1) for removing leftmost character
    for i in range(m - 1):
        base_power = (base_power * BASE) % PRIME
    
    # Compute initial hashes for pattern and first window
    for i in range(m):
        pattern_hash = (pattern_hash * BASE + ord(pattern[i])) % PRIME
        window_hash = (window_hash * BASE + ord(text[i])) % PRIME
    
    result = []
    
    # Slide window through text
    for i in range(n - m + 1):
        # Check if hashes match
        if pattern_hash == window_hash:
            # Verify character by character (avoid false positives)
            if text[i:i+m] == pattern:
                result.append(i)
        
        # Compute hash for next window
        if i < n - m:
            # Remove leftmost character
            window_hash = (window_hash - ord(text[i]) * base_power) % PRIME
            # Shift left and add new character
            window_hash = (window_hash * BASE + ord(text[i + m])) % PRIME
            # Handle negative values
            window_hash = (window_hash + PRIME) % PRIME
    
    return result


# Visualization for text="ABCD", pattern="BC":
#
# Initial:
#   pattern_hash = hash("BC") = (66*256 + 67) % PRIME
#   window_hash = hash("AB") = (65*256 + 66) % PRIME
#   base_power = 256^1 = 256
#
# Window 1: "AB", i=0
#   window_hash != pattern_hash, skip
#
# Slide to Window 2: "BC", i=1
#   Remove 'A': (window_hash - 65*256) % PRIME
#   Shift & add 'C': (result * 256 + 67) % PRIME
#   Now window_hash == pattern_hash, verify and add index 1


# Example usage:
text = "AABAACAADAABAABA"
pattern = "AABA"
indices = rabin_karp(text, pattern)
print(f"Pattern found at indices: {indices}")
# Output: [0, 9, 12]
```

---

#### Rust
```rust
pub fn rabin_karp(text: &str, pattern: &str) -> Vec<usize> {
    if pattern.is_empty() || pattern.len() > text.len() {
        return vec![];
    }
    
    const BASE: u64 = 256;
    const PRIME: u64 = 1_000_000_007;
    
    let text_bytes = text.as_bytes();
    let pattern_bytes = pattern.as_bytes();
    let n = text_bytes.len();
    let m = pattern_bytes.len();
    
    // Compute base^(m-1) mod PRIME
    let mut base_power: u64 = 1;
    for _ in 0..m-1 {
        base_power = (base_power * BASE) % PRIME;
    }
    
    // Compute hashes
    let mut pattern_hash: u64 = 0;
    let mut window_hash: u64 = 0;
    
    for i in 0..m {
        pattern_hash = (pattern_hash * BASE + pattern_bytes[i] as u64) % PRIME;
        window_hash = (window_hash * BASE + text_bytes[i] as u64) % PRIME;
    }
    
    let mut result = Vec::new();
    
    // Slide window
    for i in 0..=n-m {
        if pattern_hash == window_hash {
            // Verify match
            if &text_bytes[i..i+m] == pattern_bytes {
                result.push(i);
            }
        }
        
        if i < n - m {
            // Rolling hash update
            let old_char = text_bytes[i] as u64;
            let new_char = text_bytes[i + m] as u64;
            
            // Remove old character
            window_hash = (window_hash + PRIME - (old_char * base_power) % PRIME) % PRIME;
            // Shift and add new character
            window_hash = (window_hash * BASE + new_char) % PRIME;
        }
    }
    
    result
}

// Rust advantages:
// - No negative modulo issues (we add PRIME before mod)
// - as_bytes() gives efficient byte slice access
// - u64 prevents overflow for reasonable string lengths
```

---

#### C++
```cpp
#include <vector>
#include <string>

class RabinKarp {
private:
    static constexpr long long BASE = 256;
    static constexpr long long PRIME = 1000000007;
    
public:
    std::vector<int> search(const std::string& text, const std::string& pattern) {
        if (pattern.empty() || pattern.length() > text.length()) {
            return {};
        }
        
        int n = text.length();
        int m = pattern.length();
        
        // Compute BASE^(m-1) mod PRIME
        long long base_power = 1;
        for (int i = 0; i < m - 1; ++i) {
            base_power = (base_power * BASE) % PRIME;
        }
        
        // Compute initial hashes
        long long pattern_hash = 0;
        long long window_hash = 0;
        
        for (int i = 0; i < m; ++i) {
            pattern_hash = (pattern_hash * BASE + pattern[i]) % PRIME;
            window_hash = (window_hash * BASE + text[i]) % PRIME;
        }
        
        std::vector<int> result;
        
        // Slide window
        for (int i = 0; i <= n - m; ++i) {
            if (pattern_hash == window_hash) {
                // Verify match
                if (text.substr(i, m) == pattern) {
                    result.push_back(i);
                }
            }
            
            if (i < n - m) {
                // Update hash
                window_hash = (window_hash - text[i] * base_power % PRIME + PRIME) % PRIME;
                window_hash = (window_hash * BASE + text[i + m]) % PRIME;
            }
        }
        
        return result;
    }
};

// Performance note:
// substr() creates string copy - use char comparison in production:
// bool match = true;
// for (int j = 0; j < m; ++j) {
//     if (text[i+j] != pattern[j]) { match = false; break; }
// }
```

---

#### C
```c
#include <string.h>
#include <stdlib.h>

#define BASE 256
#define PRIME 1000000007

int* rabin_karp(const char* text, const char* pattern, int* result_size) {
    int n = strlen(text);
    int m = strlen(pattern);
    
    if (m > n || m == 0) {
        *result_size = 0;
        return NULL;
    }
    
    // Compute base^(m-1)
    long long base_power = 1;
    for (int i = 0; i < m - 1; i++) {
        base_power = (base_power * BASE) % PRIME;
    }
    
    // Compute hashes
    long long pattern_hash = 0;
    long long window_hash = 0;
    
    for (int i = 0; i < m; i++) {
        pattern_hash = (pattern_hash * BASE + pattern[i]) % PRIME;
        window_hash = (window_hash * BASE + text[i]) % PRIME;
    }
    
    // Allocate result array (worst case: all positions match)
    int* result = (int*)malloc((n - m + 1) * sizeof(int));
    int count = 0;
    
    // Slide window
    for (int i = 0; i <= n - m; i++) {
        if (pattern_hash == window_hash) {
            // Verify match
            int match = 1;
            for (int j = 0; j < m; j++) {
                if (text[i + j] != pattern[j]) {
                    match = 0;
                    break;
                }
            }
            if (match) {
                result[count++] = i;
            }
        }
        
        if (i < n - m) {
            // Update hash
            window_hash = (window_hash - (long long)text[i] * base_power % PRIME + PRIME) % PRIME;
            window_hash = (window_hash * BASE + text[i + m]) % PRIME;
        }
    }
    
    *result_size = count;
    return result;
}
```

---

#### Go
```go
package main

const (
    BASE  = 256
    PRIME = 1000000007
)

func rabinKarp(text, pattern string) []int {
    n, m := len(text), len(pattern)
    
    if m == 0 || m > n {
        return []int{}
    }
    
    // Compute base^(m-1) mod PRIME
    basePower := 1
    for i := 0; i < m-1; i++ {
        basePower = (basePower * BASE) % PRIME
    }
    
    // Compute hashes
    patternHash := 0
    windowHash := 0
    
    for i := 0; i < m; i++ {
        patternHash = (patternHash*BASE + int(pattern[i])) % PRIME
        windowHash = (windowHash*BASE + int(text[i])) % PRIME
    }
    
    result := []int{}
    
    // Slide window
    for i := 0; i <= n-m; i++ {
        if patternHash == windowHash {
            // Verify match
            if text[i:i+m] == pattern {
                result = append(result, i)
            }
        }
        
        if i < n-m {
            // Update hash
            windowHash = (windowHash - int(text[i])*basePower%PRIME + PRIME) % PRIME
            windowHash = (windowHash*BASE + int(text[i+m])) % PRIME
        }
    }
    
    return result
}
```

---

## Problem: Longest Duplicate Substring (LeetCode 1044)

### Problem Statement
Find any duplicate substring of maximum length in string S.

### Approach: Binary Search + Rolling Hash

**Key Insight**: If there's a duplicate of length k, there's also duplicate of length k-1.
This monotonic property allows binary search!

```
Algorithm:
1. Binary search on substring length (1 to n-1)
2. For each length, use rolling hash to find duplicates
3. Return longest found

Time: O(n log n)
    - Binary search: O(log n) iterations
    - Each iteration: O(n) with rolling hash
```

#### Python Implementation
```python
def longest_dup_substring(s: str) -> str:
    """
    Binary search on length + rolling hash for detection.
    
    Time: O(n log n)
    Space: O(n)
    """
    def search_duplicate(length: int) -> int:
        """
        Check if duplicate substring of given length exists.
        Returns starting index of duplicate, or -1 if none.
        """
        BASE = 256
        PRIME = 2**63 - 1  # Large prime
        
        # Compute hash for first window
        h = 0
        base_power = 1
        for i in range(length):
            h = (h * BASE + ord(s[i])) % PRIME
            if i < length - 1:
                base_power = (base_power * BASE) % PRIME
        
        # Store seen hashes
        seen = {h: [0]}
        
        # Slide window and check for duplicates
        for start in range(1, len(s) - length + 1):
            # Rolling hash
            h = (h - ord(s[start - 1]) * base_power) % PRIME
            h = (h * BASE + ord(s[start + length - 1])) % PRIME
            h = (h + PRIME) % PRIME
            
            if h in seen:
                # Hash collision or actual match?
                current = s[start:start + length]
                for prev_start in seen[h]:
                    if s[prev_start:prev_start + length] == current:
                        return start
                seen[h].append(start)
            else:
                seen[h] = [start]
        
        return -1
    
    # Binary search on length
    left, right = 1, len(s) - 1
    result_start = -1
    result_length = 0
    
    while left <= right:
        mid = (left + right) // 2
        start = search_duplicate(mid)
        
        if start != -1:
            result_start = start
            result_length = mid
            left = mid + 1  # Try longer
        else:
            right = mid - 1  # Try shorter
    
    return s[result_start:result_start + result_length] if result_start != -1 else ""


# Example:
# s = "banana"
# Binary search: try lengths 3, 2, 1
# Length 3: search_duplicate(3)
#   "ban": hash1
#   "ana": hash2
#   "nan": hash3
#   "ana": hash2 again! ‚Üí duplicate found
# Return "ana"
```

---

## Complexity Analysis

### Rolling Hash Technique

| Operation | Time | Space | Notes |
|-----------|------|-------|-------|
| Initial hash | O(m) | O(1) | m = pattern length |
| Hash update | O(1) | O(1) | Key advantage! |
| Total (n chars) | O(n) | O(1) | vs O(nm) naive |

### Rabin-Karp Algorithm

- **Best case**: O(n + m) - no collisions
- **Average case**: O(n + m) - few collisions
- **Worst case**: O(nm) - many collisions (rare with good hash)

---

## Key Insights & Optimization Techniques

### 1. **Choosing Base and Prime**

```python
# Common choices:
BASE = 256  # For ASCII strings
BASE = 26   # For lowercase English letters
BASE = 10   # For numeric strings

PRIME = 10**9 + 7  # Large prime, avoids overflow in most languages
PRIME = 2**61 - 1  # Mersenne prime, efficient modulo

# Trade-off:
# Larger prime ‚Üí Less collision, more overflow risk
# Smaller prime ‚Üí More collision, safer arithmetic
```

### 2. **Handling Negative Modulo**

```python
# Problem: (a - b) % PRIME can be negative in some languages

# Solution: Add PRIME before modulo
hash_val = (hash_val - contribution + PRIME) % PRIME

# Or ensure non-negative:
hash_val = ((hash_val - contribution) % PRIME + PRIME) % PRIME
```

### 3. **Double Hashing to Reduce Collisions**

```python
def double_hash(s: str) -> tuple:
    """
    Use two different bases/primes for ultra-low collision rate.
    """
    BASE1, PRIME1 = 256, 10**9 + 7
    BASE2, PRIME2 = 31, 10**9 + 9
    
    hash1, hash2 = 0, 0
    for char in s:
        hash1 = (hash1 * BASE1 + ord(char)) % PRIME1
        hash2 = (hash2 * BASE2 + ord(char)) % PRIME2
    
    return (hash1, hash2)

# Probability of collision: 1/(PRIME1 * PRIME2) ‚âà 10^-18
```

---

## Common Mistakes & Pitfalls

### 1. **Integer Overflow**
```python
# Wrong: Can overflow even with modulo
hash_val = hash_val * BASE + char  # Overflow before modulo!

# Correct: Modulo intermediate results
hash_val = (hash_val * BASE) % PRIME
hash_val = (hash_val + char) % PRIME
```

### 2. **Forgetting to Verify Match**
```python
# Wrong: Trust hash equality
if pattern_hash == window_hash:
    return True  # False positive possible!

# Correct: Always verify character-by-character
if pattern_hash == window_hash and text[i:i+m] == pattern:
    return True
```

### 3. **Incorrect Base Power Calculation**
```python
# Wrong:
base_power = BASE ** (m - 1)  # Not modulo'd!

# Correct:
base_power = 1
for _ in range(m - 1):
    base_power = (base_power * BASE) % PRIME
```

---

## Practice Problems

### Easy
1. **Implement strStr()** (LeetCode 28) - Basic Rabin-Karp
2. **Repeated Substring Pattern** (LeetCode 459) - Rolling hash variant

### Medium
3. **Repeated DNA Sequences** (LeetCode 187) - Fixed-length windows
4. **Longest Duplicate Substring** (LeetCode 1044) - Binary search + rolling hash
5. **Find All Anagrams** (LeetCode 438) - Frequency hash variant

### Hard
6. **Longest Happy Prefix** (LeetCode 1392) - KMP or rolling hash
7. **Distinct Echo Substrings** (LeetCode 1316) - Advanced rolling hash

---

## Interview Strategy

### When to Apply Rolling Hash:
‚úì Substring matching with many queries  
‚úì Finding duplicate substrings  
‚úì Comparing many substrings efficiently  
‚úì Sliding window with string comparison  

### When NOT to Apply:
‚úó Single substring search (KMP better)  
‚úó Exact matching without collisions (simple comparison)  
‚úó Very short patterns (overhead not worth it)  

### Key Points to Mention:
1. "Using rolling hash reduces per-window cost from O(m) to O(1)"
2. "Need to handle hash collisions with character verification"
3. "Choice of base and prime affects collision rate"
4. "Can combine with binary search for optimization problems"

# Custom Hash Patterns - Complete Guide

## Philosophy: The Art of Hash Function Design

Custom hash patterns require **creative mathematical thinking**. You must:
1. Identify the "sameness" property
2. Design a function that maps equivalent items to same value
3. Ensure efficient computation

This is problem-solving at its most elegant.

---

## Pattern 5.1: Coordinate/Position Hashing

### Problem: Robot Return to Origin (LeetCode 657)

**Task**: Determine if robot returns to origin after executing moves.

**Insight**: We only care about final position, not path.

#### Mental Model
```
Moves: "UDLR"
Instead of: Tracking every position
Think:      net_x = 0, net_y = 0?

Hash key: (x, y) coordinate tuple
```

#### Implementation

```python
def judge_circle(moves: str) -> bool:
    """
    No hash needed - just track position!
    But demonstrates coordinate hashing concept.
    
    Time: O(n)
    Space: O(1)
    """
    x, y = 0, 0
    
    for move in moves:
        if move == 'U': y += 1
        elif move == 'D': y -= 1
        elif move == 'L': x -= 1
        elif move == 'R': x += 1
    
    return x == 0 and y == 0


# More complex: Track visited positions
def count_unique_positions(moves: str) -> int:
    """
    Count unique positions visited.
    Hash key: (x, y) tuple
    """
    visited = {(0, 0)}  # Start position
    x, y = 0, 0
    
    for move in moves:
        if move == 'U': y += 1
        elif move == 'D': y -= 1
        elif move == 'L': x -= 1
        elif move == 'R': x += 1
        
        visited.add((x, y))
    
    return len(visited)


# Advanced: Convert tuple to single integer (space optimization)
def position_to_int(x: int, y: int, width: int = 10000) -> int:
    """
    Map 2D coordinate to 1D integer.
    
    Assumption: coordinates bounded by width
    x, y in [-width, width]
    
    Formula: (x + width) * (2 * width + 1) + (y + width)
    """
    return (x + width) * (2 * width + 1) + (y + width)

# Example:
# (0, 0) ‚Üí 10000 * 20001 + 10000 = 200020000
# (1, 0) ‚Üí 10001 * 20001 + 10000 = 200030001
# Unique mapping!
```

---

## Pattern 5.2: Sorted/Canonical Form Hashing

### Problem: 4Sum II (LeetCode 454)

**Task**: Count quadruplets (i,j,k,l) where nums1[i] + nums2[j] + nums3[k] + nums4[l] = 0

**Naive**: O(n‚Å¥) - try all combinations

**Optimized with Hash**: O(n¬≤)

#### Expert Thought Process
```
Rearrange equation:
    nums1[i] + nums2[j] + nums3[k] + nums4[l] = 0
    ‚Üì
    nums1[i] + nums2[j] = -(nums3[k] + nums4[l])

Strategy:
1. Compute all sums from nums1 + nums2 ‚Üí Hash map (sum ‚Üí count)
2. For each sum from nums3 + nums4, check if negation exists in map
3. Add count of matching pairs

Hash key: Integer sum
```

#### Implementation

```python
from collections import defaultdict
from typing import List

def four_sum_count(
    nums1: List[int], 
    nums2: List[int], 
    nums3: List[int], 
    nums4: List[int]
) -> int:
    """
    Count quadruplets summing to zero.
    
    Time: O(n¬≤)
    Space: O(n¬≤)
    
    Key: Divide into two O(n¬≤) problems instead of one O(n‚Å¥)
    """
    # Step 1: Count all sums from nums1 + nums2
    sum_count = defaultdict(int)
    for a in nums1:
        for b in nums2:
            sum_count[a + b] += 1
    
    # Step 2: For each sum from nums3 + nums4, check complement
    result = 0
    for c in nums3:
        for d in nums4:
            complement = -(c + d)
            if complement in sum_count:
                result += sum_count[complement]
    
    return result


# Visualization for small example:
# nums1 = [1, 2], nums2 = [-2, -1], nums3 = [-1, 2], nums4 = [0, 2]
#
# sum_count after step 1:
# {
#     1 + (-2) = -1: 1,
#     1 + (-1) =  0: 1,
#     2 + (-2) =  0: 1,
#     2 + (-1) =  1: 1
# }
# ‚Üí {-1: 1, 0: 2, 1: 1}
#
# Step 2:
# c=-1, d=0:  -(‚àí1+0) = 1  ‚Üí sum_count[1] = 1, result += 1
# c=-1, d=2:  -(‚àí1+2) = -1 ‚Üí sum_count[-1] = 1, result += 1
# c=2, d=0:   -(2+0) = -2  ‚Üí not in map
# c=2, d=2:   -(2+2) = -4  ‚Üí not in map
#
# Total: 2
```

---

## Pattern 5.3: Multiset/Frequency Signature Hashing

### Problem: Group Shifted Strings (LeetCode 249)

**Task**: Group strings that can be transformed into each other by shifting.

**Example**: `["abc", "bcd", "xyz"]` ‚Üí `[["abc","bcd"], ["xyz"]]`
- "abc" ‚Üí "bcd" by shifting each char by +1
- But "abc" ‚â† "xyz" (different shift pattern)

#### Mental Model
```
What makes strings "shift-equivalent"?
    ‚Üí Same pattern of differences between adjacent characters!

"abc": differences = [b-a, c-b] = [1, 1]
"bcd": differences = [c-b, d-c] = [1, 1]  ‚Üê Same!
"xyz": differences = [y-x, z-y] = [1, 1]  ‚Üê Also same!

Wait, that's wrong! Need better hash...

Better hash: Shift to start with 'a'
"abc" ‚Üí "abc" (already starts with 'a')
"bcd" ‚Üí "abc" (shift by -1)
"xyz" ‚Üí "abc" (shift by -23)

Even better: Use difference tuple as hash key!
But handle wrap-around: 'za' ‚Üí [25, -25] or [25, 1]?

Canonical form: Normalize differences modulo 26
```

#### Implementation

```python
from collections import defaultdict

def group_shifted_strings(strings: List[str]) -> List[List[str]]:
    """
    Group strings that are shifts of each other.
    
    Time: O(n * m) where n = len(strings), m = max string length
    Space: O(n * m)
    
    Hash key: Tuple of character differences
    """
    def get_hash(s: str) -> tuple:
        """
        Compute hash based on character differences.
        Normalize to handle wrap-around.
        """
        if len(s) == 1:
            return (0,)  # Single char, all map to same
        
        diffs = []
        for i in range(1, len(s)):
            # Difference between consecutive characters
            diff = (ord(s[i]) - ord(s[i-1])) % 26
            diffs.append(diff)
        
        return tuple(diffs)
    
    groups = defaultdict(list)
    
    for s in strings:
        hash_key = get_hash(s)
        groups[hash_key].append(s)
    
    return list(groups.values())


# Example walkthrough:
# "abc": diffs = [(98-97)%26, (99-98)%26] = [1, 1]
# "bcd": diffs = [(99-98)%26, (100-99)%26] = [1, 1]  ‚Üê Same hash!
# "xyz": diffs = [(121-120)%26, (122-121)%26] = [1, 1]  ‚Üê Same hash!
# "az":  diffs = [(122-97)%26] = [25]
# "ba":  diffs = [(97-98)%26] = [25]  ‚Üê Same hash!
#
# Result: [["abc","bcd","xyz"], ["az","ba"]]


# Alternative: Shift to canonical form starting with 'a'
def group_shifted_strings_canonical(strings: List[str]) -> List[List[str]]:
    """
    Alternative: Normalize each string to start with 'a'.
    """
    def normalize(s: str) -> str:
        if not s:
            return s
        
        # Shift so first char becomes 'a'
        shift = ord('a') - ord(s[0])
        
        normalized = []
        for char in s:
            # Shift each character
            new_char = chr((ord(char) - ord('a') + shift) % 26 + ord('a'))
            normalized.append(new_char)
        
        return ''.join(normalized)
    
    groups = defaultdict(list)
    
    for s in strings:
        canonical = normalize(s)
        groups[canonical].append(s)
    
    return list(groups.values())
```

---

## Pattern 5.4: Structural Hashing (Trees/Graphs)

### Problem: Find Duplicate Subtrees (LeetCode 652)

**Task**: Find all duplicate subtrees in binary tree.

**Challenge**: How to hash a tree structure?

#### Mental Model
```
Tree:
      1
     / \
    2   3
   /   / \
  4   2   4
     /
    4

How to identify duplicate subtree?
    ‚Üí Serialize tree structure into string!

Subtree rooted at left '2': "2,4,#,#,#"
Subtree rooted at middle '2': "2,4,#,#,#"  ‚Üê Same! Duplicate found

Hash key: Serialized tree structure
```

#### Implementation

```python
from collections import defaultdict

class TreeNode:
    def __init__(self, val=0, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right


def find_duplicate_subtrees(root: TreeNode) -> List[TreeNode]:
    """
    Find all duplicate subtrees using structural hashing.
    
    Time: O(n¬≤) worst case (serialization cost)
    Space: O(n¬≤) for storing serializations
    
    Key: Serialize subtree structure as hash key
    """
    subtree_count = defaultdict(int)
    result = []
    
    def serialize(node: TreeNode) -> str:
        """
        Serialize subtree structure.
        Format: "value,left_subtree,right_subtree"
        """
        if not node:
            return "#"
        
        # Post-order traversal (left, right, root)
        left_serial = serialize(node.left)
        right_serial = serialize(node.right)
        
        # Create unique structure identifier
        serial = f"{node.val},{left_serial},{right_serial}"
        
        # Track count of this structure
        subtree_count[serial] += 1
        
        # Add to result only on second occurrence
        if subtree_count[serial] == 2:
            result.append(node)
        
        return serial
    
    serialize(root)
    return result


# Visualization:
#       1
#      / \
#     2   3
#    /   / \
#   4   2   4
#      /
#     4
#
# Serialization (post-order):
#
# Node with value 4 (leftmost): "4,#,#"
# Node with value 2 (left):     "2,4,#,#,#"
# Node with value 4 (bottom middle): "4,#,#"
# Node with value 2 (middle):   "2,4,#,#,#"  ‚Üê Duplicate!
# Node with value 4 (rightmost): "4,#,#"  ‚Üê Duplicate!
# Node with value 3: "3,2,4,#,#,#,4,#,#"
# Node with value 1: "1,2,4,#,#,#,3,2,4,#,#,#,4,#,#"


# Optimization: Use integer IDs instead of strings
def find_duplicate_subtrees_optimized(root: TreeNode) -> List[TreeNode]:
    """
    Space-optimized version using integer IDs.
    
    Time: O(n)
    Space: O(n)
    """
    trees = defaultdict(int)  # serialization ‚Üí ID
    count = defaultdict(int)  # ID ‚Üí count
    result = []
    uid = 0
    
    def serialize(node):
        nonlocal uid
        if not node:
            return 0
        
        left_id = serialize(node.left)
        right_id = serialize(node.right)
        
        # Unique identifier for this structure
        serial = (node.val, left_id, right_id)
        
        if serial not in trees:
            uid += 1
            trees[serial] = uid
        
        tree_id = trees[serial]
        count[tree_id] += 1
        
        if count[tree_id] == 2:
            result.append(node)
        
        return tree_id
    
    serialize(root)
    return result
```

---

## Pattern 5.5: Mathematical Hash Functions

### Problem: Max Points on a Line (LeetCode 149)

**Task**: Find maximum points lying on the same straight line.

**Challenge**: How to identify "same line"?

#### Mathematical Insight
```
Line equation: y = mx + b
Two points define a line by slope: m = (y2-y1)/(x2-x1)

Problem: Points with same slope from reference point lie on same line!

Hash key: Slope (as fraction to avoid floating point issues)
```

#### Implementation

```python
from collections import defaultdict
from math import gcd
from typing import List

def max_points(points: List[List[int]]) -> int:
    """
    Find max points on a line using slope hashing.
    
    Time: O(n¬≤)
    Space: O(n)
    
    Key: For each point, count points with same slope
    """
    if len(points) <= 2:
        return len(points)
    
    def get_slope(p1: List[int], p2: List[int]) -> tuple:
        """
        Compute slope as reduced fraction (dy/dx).
        Returns (dy, dx) tuple.
        """
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        
        # Handle vertical line
        if dx == 0:
            return (1, 0)  # Infinite slope
        
        # Handle horizontal line
        if dy == 0:
            return (0, 1)  # Zero slope
        
        # Reduce fraction using GCD
        g = gcd(abs(dy), abs(dx))
        dy, dx = dy // g, dx // g
        
        # Normalize sign (always keep positive dx)
        if dx < 0:
            dy, dx = -dy, -dx
        
        return (dy, dx)
    
    max_count = 0
    
    # Try each point as reference
    for i in range(len(points)):
        slopes = defaultdict(int)
        duplicate = 0
        
        for j in range(i + 1, len(points)):
            # Check for duplicate points
            if points[i] == points[j]:
                duplicate += 1
                continue
            
            # Compute slope and count
            slope = get_slope(points[i], points[j])
            slopes[slope] += 1
        
        # Max for this reference point
        if slopes:
            current_max = max(slopes.values()) + 1 + duplicate
        else:
            current_max = 1 + duplicate
        
        max_count = max(max_count, current_max)
    
    return max_count


# Example:
# points = [[1,1],[2,2],[3,3],[1,3],[2,4]]
#
# Reference point (1,1):
#   To (2,2): slope = (2-1)/(2-1) = 1/1 ‚Üí (1,1)
#   To (3,3): slope = (3-1)/(3-1) = 2/2 ‚Üí (1,1)  ‚Üê Same line!
#   To (1,3): slope = (3-1)/(1-1) = ‚àû ‚Üí (1,0)
#   To (2,4): slope = (4-1)/(2-1) = 3/1 ‚Üí (3,1)
#
# slopes = {(1,1): 2, (1,0): 1, (3,1): 1}
# max for this reference = 2 + 1 = 3
#
# Answer: 3 points on line (1,1)-(2,2)-(3,3)


# Why use GCD?
# Avoid issues like: 2/4 and 1/2 being different keys but same slope
# gcd(2,4) = 2 ‚Üí 2/4 reduces to 1/2
```

---

## Pattern 5.6: Custom Composite Keys

### Problem: LRU Cache (LeetCode 146)

**Challenge**: Need O(1) get AND put operations with eviction policy.

**Solution**: Hash map + doubly linked list

#### Mental Model
```
Hash map: key ‚Üí node (O(1) access)
DLL: maintains access order (O(1) move to front)

Custom structure, not just hash function!

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Hash Map                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ key   ‚îÇ node pointer ‚îÇ          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§          ‚îÇ
‚îÇ   ‚îÇ  "a"  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  "b"  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îê    ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                  ‚îÇ  ‚îÇ                ‚îÇ
‚îÇ   Doubly Linked List (LRU order)    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ   ‚îÇ a  ‚îÇ‚Üê‚Üí‚îÇ b  ‚îÇ‚Üê‚Üí‚îÇ c  ‚îÇ            ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ   ^head           tail^             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Complexity Comparison Table

| Pattern | Hash Key | Time | Space | Best For |
|---------|----------|------|-------|----------|
| Coordinate | (x,y) tuple | O(1) | O(1) | Position tracking |
| Canonical form | Sorted/normalized | O(k) | O(k) | Equivalence classes |
| Frequency | Count tuple | O(k) | O(1) | Anagrams, permutations |
| Structural | Serialization | O(n) | O(n) | Trees, graphs |
| Slope | Fraction (dy,dx) | O(1) | O(1) | Geometric problems |

---

## Design Principles for Custom Hash Functions

### 1. **Uniqueness**
Hash must uniquely identify equivalence class.

### 2. **Efficiency**
Should be computable in O(1) or O(k) where k is small.

### 3. **Determinism**
Same input always produces same hash.

### 4. **Hashability**
Result must be hashable in your language:
- Python: immutable (tuple, string, int)
- Rust: implements Hash trait
- C++: default hashable or custom hash function

### 5. **Collision Handling**
For critical applications, verify actual equality on collision.

---

## Advanced Technique: Zobrist Hashing

### Concept
Pre-assign random numbers to each possible state component, XOR them together.

**Use case**: Board games, state space search

```python
import random

class ZobristHash:
    """
    Fast incremental hashing for board states.
    Used in chess engines, Go programs.
    """
    def __init__(self, board_size=8, num_pieces=6):
        # Pre-generate random numbers for each (position, piece) pair
        random.seed(42)  # For reproducibility
        self.table = {}
        
        for row in range(board_size):
            for col in range(board_size):
                for piece in range(num_pieces):
                    # Random 64-bit integer for each state
                    self.table[(row, col, piece)] = random.getrandbits(64)
    
    def compute_hash(self, board):
        """
        Compute hash of board state.
        board[i][j] = piece type (0 = empty)
        """
        hash_val = 0
        for row in range(len(board)):
            for col in range(len(board[0])):
                piece = board[row][col]
                if piece != 0:  # Not empty
                    # XOR in the random number for this configuration
                    hash_val ^= self.table[(row, col, piece)]
        return hash_val
    
    def update_hash(self, old_hash, move):
        """
        Update hash incrementally in O(1).
        move = (from_row, from_col, to_row, to_col, piece)
        """
        from_row, from_col, to_row, to_col, piece = move
        
        # XOR out old position
        old_hash ^= self.table[(from_row, from_col, piece)]
        # XOR in new position
        old_hash ^= self.table[(to_row, to_col, piece)]
        
        return old_hash

# Property: XOR is its own inverse
# hash ^ x ^ x = hash (cancels out)
# This makes incremental updates possible!
```

---

## Practice Problems by Pattern

### Coordinate Hashing
1. **Robot Room Cleaner** (LeetCode 489)
2. **Spiral Matrix III** (LeetCode 885)

### Canonical Form
3. **Group Anagrams** (LeetCode 49) - Already covered
4. **Group Shifted Strings** (LeetCode 249)

### Structural Hashing
5. **Serialize and Deserialize Binary Tree** (LeetCode 297)
6. **Find Duplicate Subtrees** (LeetCode 652)

### Mathematical Hashing
7. **Max Points on a Line** (LeetCode 149)
8. **Mirror Reflection** (LeetCode 858)

### Composite Data Structures
9. **LRU Cache** (LeetCode 146)
10. **LFU Cache** (LeetCode 460)

---

## Interview Strategy

### Problem Recognition Checklist

**Use custom hash when:**
‚úì Standard hash doesn't capture problem structure  
‚úì Need to group by complex property  
‚úì Geometric or mathematical relationship  
‚úì Tree/graph isomorphism  

### Design Process

1. **Identify invariant**: What stays same for equivalent items?
2. **Choose representation**: Tuple, string, integer?
3. **Implement efficiently**: Avoid expensive operations
4. **Test edge cases**: Empty, single element, duplicates

### Common Interview Questions

**Q**: "How did you choose this hash function?"  
**A**: "I identified that [property] determines equivalence, then designed a function capturing that property uniquely and efficiently."

**Q**: "What if there are hash collisions?"  
**A**: "I handle collisions by [verification step / better hash / separate chaining]."

**Q**: "Can you optimize space/time?"  
**A**: "Currently O(n) space. Could reduce by [specific technique] if constraints allow."

---

## Mental Models Summary

1. **Coordinate**: Position as identity
2. **Canonical**: Normalize to standard form
3. **Frequency**: Count-based signatures
4. **Structural**: Serialize complex structure
5. **Mathematical**: Capture geometric/algebraic property
6. **Incremental**: Update hash without full recomputation

Master these patterns, and you'll handle 95% of hash function problems in interviews and competitions!

# Advanced Hash Topics & Common Pitfalls
## For the Final 5% of Mastery

---

## üö® Common Pitfalls & How to Avoid Them

### Pitfall 1: Floating Point Hash Keys

**Problem**: Floating point numbers are NOT reliable hash keys!

```python
# WRONG: Floating point keys
hash_map = {}
hash_map[0.1 + 0.2] = "value"
print(hash_map[0.3])  # KeyError! Because 0.1 + 0.2 != 0.3 exactly

# Why: 0.1 + 0.2 = 0.30000000000000004 in binary floating point

# CORRECT: Use tuples of integers
def float_to_fraction(f: float, precision: int = 6) -> tuple:
    """Convert float to (numerator, denominator) tuple"""
    multiplier = 10 ** precision
    numerator = int(f * multiplier)
    from math import gcd
    g = gcd(numerator, multiplier)
    return (numerator // g, multiplier // g)

hash_map[(1, 10)]  # Use fractions instead
```

**Rule**: Never use floats as hash keys. Use:
- Integers (scaled)
- Fractions (as tuples)
- Strings (rounded to precision)

---

### Pitfall 2: Mutable Hash Keys

**Problem**: Mutable objects can't be hash keys (Python lists, sets)

```python
# WRONG
key = [1, 2, 3]
hash_map[key] = "value"  # TypeError: unhashable type: 'list'

# CORRECT: Use immutable types
key = (1, 2, 3)  # Tuple
hash_map[key] = "value"  # Works!

# Or convert to string
key = str([1, 2, 3])
hash_map[key] = "value"
```

**Language-Specific**:
- **Python**: Lists, sets, dicts are unhashable ‚Üí Use tuples, frozensets
- **Rust**: Keys must implement `Hash` trait
- **C++**: Provide custom hash function for complex types
- **Go**: Arrays (not slices) can be map keys

---

### Pitfall 3: Not Handling Hash Collisions

**Problem**: Trusting hash equality without verification

```python
# WRONG: False positives possible
if hash(str1) == hash(str2):
    return True  # Might be collision!

# CORRECT: Always verify
if hash(str1) == hash(str2) and str1 == str2:
    return True  # Now certain

# For rolling hash in string matching
if pattern_hash == window_hash:
    # MUST verify character by character
    if text[i:i+m] == pattern:
        return i
```

**Collision Probability**:
```
Good hash function with output space size M:
- Probability of collision ‚âà n¬≤/(2M)
- For M = 10‚Åπ+7, n = 10‚Å∂: P ‚âà 0.00005
- Still not zero! Always verify.
```

---

### Pitfall 4: Integer Overflow in Hash Computation

**Problem**: Hash computation overflows, gives wrong results

```c++
// WRONG: Overflow before modulo
long long hash = 0;
for (char c : str) {
    hash = hash * BASE + c;  // Can overflow!
}
hash %= PRIME;

// CORRECT: Modulo intermediate steps
long long hash = 0;
for (char c : str) {
    hash = (hash * BASE) % PRIME;
    hash = (hash + c) % PRIME;
}
```

**Language Considerations**:
- **C/C++**: Use `long long`, modulo frequently
- **Python**: Arbitrary precision integers, but slower
- **Rust**: Use `u64`, wrapping arithmetic, or checked operations
- **Go**: `int` size is platform-dependent, use `int64`

---

### Pitfall 5: Negative Modulo Issues

**Problem**: Different languages handle negative modulo differently

```python
# C/C++/Java: (-5) % 3 = -2
# Python: (-5) % 3 = 1

# Problem in rolling hash update:
hash = (hash - old_char * base_power) % PRIME
# Can be negative in C++!

# SOLUTION: Add PRIME before modulo
hash = ((hash - old_char * base_power) % PRIME + PRIME) % PRIME

# Or in C++:
hash = (hash - old_char * base_power % PRIME + PRIME) % PRIME;
```

---

### Pitfall 6: Incorrect Base Power Computation

**Problem**: Not using modular exponentiation

```python
# WRONG: Overflow and slow
base_power = BASE ** (length - 1)
base_power %= PRIME

# CORRECT: Iterative with modulo
base_power = 1
for _ in range(length - 1):
    base_power = (base_power * BASE) % PRIME

# Or use built-in power function with modulo
base_power = pow(BASE, length - 1, PRIME)  # Python's efficient modpow
```

---

### Pitfall 7: Not Pre-sizing Hash Tables

**Problem**: Unnecessary rehashing causes performance degradation

```python
# SLOW: Multiple resizings
hash_map = {}
for i in range(1000000):
    hash_map[i] = i

# FAST: Pre-allocate
hash_map = {}
# In C++: hash_map.reserve(1000000)
# In Rust: HashMap::with_capacity(1000000)
```

**Performance Impact**:
```
Without pre-sizing: ~O(n log n) due to multiple rehashes
With pre-sizing: O(n)

For n = 10‚Å∂: 2-3x speedup
```

---

### Pitfall 8: Using Hash When Array Suffices

**Problem**: Over-engineering with hash when simple array works

```python
# WRONG: Hash map for small, dense key space
freq = {}
for char in string:
    freq[char] = freq.get(char, 0) + 1

# CORRECT: Array for 26 letters
freq = [0] * 26
for char in string:
    freq[ord(char) - ord('a')] += 1

# 2-3x faster, cache-friendly
```

**Rule**: If keys are:
- Small range (< 1000)
- Dense (most values used)
- Integer or convertible to integer

‚Üí Use array, not hash map

---

## üé® Advanced Optimization Techniques

### Technique 1: Double Hashing for Ultra-Low Collisions

```python
class DoubleHash:
    """
    Use two independent hash functions.
    Collision probability: 1/(M‚ÇÅ * M‚ÇÇ)
    """
    PRIME1 = 10**9 + 7
    PRIME2 = 10**9 + 9
    BASE1 = 31
    BASE2 = 37
    
    @staticmethod
    def compute(s: str) -> tuple:
        hash1, hash2 = 0, 0
        for char in s:
            hash1 = (hash1 * DoubleHash.BASE1 + ord(char)) % DoubleHash.PRIME1
            hash2 = (hash2 * DoubleHash.BASE2 + ord(char)) % DoubleHash.PRIME2
        return (hash1, hash2)
    
    @staticmethod
    def rolling_update(old_hash: tuple, old_char: str, new_char: str, 
                       length: int, base_powers: tuple) -> tuple:
        """Update both hashes simultaneously"""
        h1, h2 = old_hash
        bp1, bp2 = base_powers
        
        # Update hash1
        h1 = (h1 - ord(old_char) * bp1) % DoubleHash.PRIME1
        h1 = (h1 * DoubleHash.BASE1 + ord(new_char)) % DoubleHash.PRIME1
        h1 = (h1 + DoubleHash.PRIME1) % DoubleHash.PRIME1
        
        # Update hash2
        h2 = (h2 - ord(old_char) * bp2) % DoubleHash.PRIME2
        h2 = (h2 * DoubleHash.BASE2 + ord(new_char)) % DoubleHash.PRIME2
        h2 = (h2 + DoubleHash.PRIME2) % DoubleHash.PRIME2
        
        return (h1, h2)

# Use case: When collision probability must be < 10‚Åª¬π‚Åµ
```

---

### Technique 2: Polynomial Hashing with Precomputed Powers

```python
class FastRollingHash:
    """
    Precompute all powers for O(1) updates without division
    """
    def __init__(self, text: str, base: int = 31, prime: int = 10**9 + 7):
        self.text = text
        self.base = base
        self.prime = prime
        self.n = len(text)
        
        # Precompute base^i mod prime for all i
        self.powers = [1] * (self.n + 1)
        for i in range(1, self.n + 1):
            self.powers[i] = (self.powers[i-1] * base) % prime
        
        # Precompute prefix hashes
        self.hashes = [0] * (self.n + 1)
        for i in range(self.n):
            self.hashes[i+1] = (self.hashes[i] * base + ord(text[i])) % prime
    
    def get_hash(self, left: int, right: int) -> int:
        """
        Get hash of substring [left, right) in O(1)
        """
        result = (self.hashes[right] - self.hashes[left] * self.powers[right-left]) % self.prime
        return (result + self.prime) % self.prime

# Use case: Multiple substring queries on same text
# Preprocessing: O(n)
# Each query: O(1)
```

---

### Technique 3: Bloom Filter for Space Optimization

**When**: Need to check existence with minimal space

```python
import hashlib

class BloomFilter:
    """
    Probabilistic set membership testing
    Space: Much less than hash set
    False positive rate: Configurable
    False negative rate: 0 (never says "not present" if present)
    """
    def __init__(self, size: int, num_hashes: int = 3):
        self.size = size
        self.num_hashes = num_hashes
        self.bits = [False] * size
    
    def _hash(self, item: str, seed: int) -> int:
        """Multiple hash functions via different seeds"""
        h = hashlib.sha256((item + str(seed)).encode()).hexdigest()
        return int(h, 16) % self.size
    
    def add(self, item: str):
        for i in range(self.num_hashes):
            idx = self._hash(item, i)
            self.bits[idx] = True
    
    def contains(self, item: str) -> bool:
        """Returns True if item MIGHT be present"""
        return all(self.bits[self._hash(item, i)] for i in range(self.num_hashes))

# Use case: Spell checkers, web crawlers, caches
# Example: 1 billion words
#   Regular set: ~10GB
#   Bloom filter (1% false positive): ~1.2GB
```

---

### Technique 4: Cuckoo Hashing for Guaranteed O(1) Lookup

```python
class CuckooHashTable:
    """
    Two hash functions, two tables
    Worst-case O(1) lookup (not average!)
    
    If collision: "kick out" existing element to other table
    """
    def __init__(self, size: int):
        self.size = size
        self.table1 = [None] * size
        self.table2 = [None] * size
        self.max_kicks = 100
    
    def _hash1(self, key: int) -> int:
        return key % self.size
    
    def _hash2(self, key: int) -> int:
        return (key // self.size) % self.size
    
    def insert(self, key: int, value: any):
        for _ in range(self.max_kicks):
            idx1 = self._hash1(key)
            if self.table1[idx1] is None:
                self.table1[idx1] = (key, value)
                return True
            
            # Kick out existing
            self.table1[idx1], (key, value) = (key, value), self.table1[idx1]
            
            idx2 = self._hash2(key)
            if self.table2[idx2] is None:
                self.table2[idx2] = (key, value)
                return True
            
            # Kick out from table2
            self.table2[idx2], (key, value) = (key, value), self.table2[idx2]
        
        return False  # Failed after max_kicks, need rehash
    
    def lookup(self, key: int) -> any:
        """O(1) worst-case lookup"""
        idx1 = self._hash1(key)
        if self.table1[idx1] and self.table1[idx1][0] == key:
            return self.table1[idx1][1]
        
        idx2 = self._hash2(key)
        if self.table2[idx2] and self.table2[idx2][0] == key:
            return self.table2[idx2][1]
        
        return None

# Use case: Real-time systems requiring guaranteed performance
```

---

## üî¨ Advanced Hash Function Designs

### Design 1: Zobrist Hashing (Board Games)

```python
import random

class ZobristHash:
    """
    Extremely fast incremental hashing for board states
    Used in chess engines, Go programs
    """
    def __init__(self, board_size: int, num_pieces: int):
        random.seed(42)
        self.table = {}
        
        for row in range(board_size):
            for col in range(board_size):
                for piece in range(num_pieces):
                    self.table[(row, col, piece)] = random.getrandbits(64)
        
        self.turn_hash = random.getrandbits(64)
    
    def compute_hash(self, board: list) -> int:
        h = 0
        for row in range(len(board)):
            for col in range(len(board[0])):
                if board[row][col] != 0:
                    h ^= self.table[(row, col, board[row][col])]
        return h
    
    def update_move(self, old_hash: int, move: tuple) -> int:
        """
        Update hash in O(1) for a move
        move = (from_row, from_col, to_row, to_col, piece)
        """
        from_r, from_c, to_r, to_c, piece = move
        
        # XOR out old position
        old_hash ^= self.table[(from_r, from_c, piece)]
        # XOR in new position
        old_hash ^= self.table[(to_r, to_c, piece)]
        # Toggle turn
        old_hash ^= self.turn_hash
        
        return old_hash

# Property: XOR is self-inverse
# x ^ y ^ y = x
# Makes incremental updates trivial!
```

---

### Design 2: Geometric Hashing (Computer Vision)

```python
from typing import List, Tuple
import math

class GeometricHash:
    """
    Hash for geometric shapes invariant to rotation, scale, translation
    Used in: Image recognition, shape matching
    """
    def __init__(self, precision: int = 3):
        self.precision = precision
    
    def normalize_shape(self, points: List[Tuple[float, float]]) -> tuple:
        """
        Normalize shape to canonical form:
        1. Center at origin (translation invariant)
        2. Scale to unit circle (scale invariant)
        3. Rotate to align first point with x-axis (rotation invariant)
        """
        # Center
        cx = sum(p[0] for p in points) / len(points)
        cy = sum(p[1] for p in points) / len(points)
        centered = [(x - cx, y - cy) for x, y in points]
        
        # Scale
        max_dist = max(math.sqrt(x**2 + y**2) for x, y in centered)
        if max_dist > 0:
            scaled = [(x/max_dist, y/max_dist) for x, y in centered]
        else:
            scaled = centered
        
        # Rotate to align first point
        if scaled:
            angle = math.atan2(scaled[0][1], scaled[0][0])
            cos_a, sin_a = math.cos(-angle), math.sin(-angle)
            rotated = [
                (x * cos_a - y * sin_a, x * sin_a + y * cos_a)
                for x, y in scaled
            ]
        else:
            rotated = scaled
        
        # Round to precision
        rounded = tuple(
            (round(x, self.precision), round(y, self.precision))
            for x, y in rotated
        )
        
        return rounded
    
    def shapes_match(self, shape1: List[Tuple], shape2: List[Tuple]) -> bool:
        """Check if two shapes are similar"""
        return self.normalize_shape(shape1) == self.normalize_shape(shape2)

# Use case: "Find this shape in an image at any position/rotation/size"
```

---

## üéØ Competition Programming Tricks

### Trick 1: Custom Hash for std::pair in C++

```cpp
// Problem: std::pair not hashable by default in unordered_map

// Solution 1: Custom hash function
struct PairHash {
    template <typename T1, typename T2>
    std::size_t operator()(const std::pair<T1, T2>& p) const {
        auto h1 = std::hash<T1>{}(p.first);
        auto h2 = std::hash<T2>{}(p.second);
        return h1 ^ (h2 << 1);  // Combine hashes
    }
};

// Usage:
std::unordered_map<std::pair<int, int>, int, PairHash> map;

// Solution 2: Convert to long long (if possible)
long long pair_to_ll(int x, int y) {
    return ((long long)x << 32) | (unsigned int)y;
}
// Usage:
std::unordered_map<long long, int> map;
map[pair_to_ll(x, y)] = value;
```

---

### Trick 2: Python frozenset for Unordered Collections

```python
# Problem: Need to hash a set of elements
# Wrong:
key = {1, 2, 3}  # set is unhashable

# Correct:
key = frozenset([1, 2, 3])
hash_map[key] = "value"

# Use case: Graph problems where order doesn't matter
# Example: Hash edges as frozenset({u, v})
```

---

### Trick 3: String Pooling for Memory Optimization

```python
import sys

# Problem: Many duplicate strings use lots of memory
strings = ["hello"] * 1000000  # Each is separate object

# Solution: String interning (Python)
strings = [sys.intern("hello")] * 1000000  # All reference same object

# Memory saved: ~40MB for 1M strings
```

---

## üìä Hash Function Performance Benchmarks

### Benchmark Results (1M operations)

```
Test: Insert and lookup 1M random integers
Hardware: 3.5GHz CPU, 16GB RAM

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Structure        ‚îÇ Insert   ‚îÇ Lookup   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ std::unordered_map      ‚îÇ  250ms   ‚îÇ  180ms   ‚îÇ
‚îÇ Python dict             ‚îÇ  180ms   ‚îÇ  150ms   ‚îÇ
‚îÇ Rust HashMap            ‚îÇ  200ms   ‚îÇ  160ms   ‚îÇ
‚îÇ Go map                  ‚îÇ  220ms   ‚îÇ  170ms   ‚îÇ
‚îÇ                         ‚îÇ          ‚îÇ          ‚îÇ
‚îÇ Array (when applicable) ‚îÇ   80ms   ‚îÇ   50ms   ‚îÇ
‚îÇ std::map (ordered)      ‚îÇ  600ms   ‚îÇ  500ms   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Key Insight: Hash table ~3x faster than ordered map
             Array ~3x faster than hash when applicable
```

---

## üß™ Testing Your Hash Functions

### Test Suite Template

```python
def test_hash_function():
    """Comprehensive test for custom hash functions"""
    
    # Test 1: Determinism
    assert hash_func("test") == hash_func("test")
    
    # Test 2: Uniqueness (for known inputs)
    assert hash_func("abc") != hash_func("abd")
    
    # Test 3: Collision rate
    keys = generate_random_keys(10000)
    hashes = [hash_func(k) for k in keys]
    unique = len(set(hashes))
    collision_rate = 1 - unique / len(hashes)
    assert collision_rate < 0.01  # Less than 1%
    
    # Test 4: Distribution uniformity (Chi-squared test)
    buckets = [0] * 100
    for h in hashes:
        buckets[h % 100] += 1
    
    expected = len(hashes) / 100
    chi_squared = sum((b - expected)**2 / expected for b in buckets)
    # Accept if chi-squared < critical value
    
    # Test 5: Edge cases
    assert hash_func("") is not None  # Empty input
    assert hash_func("a" * 10000) is not None  # Very long input
    
    print("All tests passed!")
```

---

## üéì Final Mastery Exercises

### Exercise 1: Implement Perfect Hashing
Design a two-level hash table with zero collisions for static data.

### Exercise 2: Design Minimal Perfect Hash Function
Create hash function for fixed set of n keys producing values 0 to n-1 with no gaps or collisions.

### Exercise 3: Optimize for Cache Locality
Modify hash table to be cache-friendly (linear probing vs chaining).

### Exercise 4: Implement Concurrent Hash Map
Thread-safe hash table with fine-grained locking.

### Exercise 5: Create Custom Hash for Your Domain
Design specialized hash for specific data (genomic sequences, IP addresses, etc.).

---

## üèÜ Beyond Mastery: Research Topics

1. **Consistent Hashing** - Distributed systems
2. **Locality-Sensitive Hashing** - Similarity search
3. **MinHash** - Set similarity estimation
4. **SimHash** - Near-duplicate detection
5. **Cryptographic Hashing** - Security applications

These are PhD-level topics but worth exploring for complete mastery!

---

END OF ADVANCED TOPICS

You now have everything needed for the top 1% of hash function expertise.